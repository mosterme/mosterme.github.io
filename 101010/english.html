<!DOCTYPE HTML><html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>101010 - my news site - 14:45:30</title>
      <link rel="stylesheet" type="text/css" id="basic" href="assets/basic/common.css">
      <link rel="stylesheet" type="text/css" id="color" href="assets/color/flat.css">
      <link rel="stylesheet" type="text/css" id="theme" href="assets/theme/slash.css">
      <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg'&gt;<text y='26' font-size='26'&gt;%F0%9F%9B%B8</text&gt;</svg&gt;">
   </head>
   <body>
      <header><time>14:45:30</time><h1><span class="hi">üõ∏</span><span class="ht">101010 - my news site</span></h1>
      </header>
      <main>
         <menu>
            
            <li><a href="#d3e4">xkcd.com</a></li>
            
            <li><a href="#d4e9">Slashdot</a></li>
            
            <li><a href="#d5e9">The GitHub Blog</a></li>
            
            <li><a href="#d6e9">Netflix TechBlog</a></li>
            
            <li><a href="#d7e9">Linux.com</a></li>
            </menu>
         
         <section id="d3e4">
            <header>
               <h2 title="xkcd.com: A webcomic of romance and math humor.">xkcd.com <a rel="noopener noreferrer" target="_blank" href="https://xkcd.com/">ùìó</a><a rel="noopener noreferrer" target="_blank" href="https://xkcd.com/rss.xml">ùìï</a></h2>
            </header>
            <details open="">
               <summary><a rel="noopener noreferrer" target="_blank" href="https://xkcd.com/2634/">Red Line Through HTTPS</a> <span></span></summary><time datetime="2022-06-17T06:00:00+02:00">Fri, 17 Jun 2022 04:00</time><article style="text-align:center"><img src="https://imgs.xkcd.com/comics/red_line_through_https.png" title="Some organization has been paying to keep this up and it hasn't been removed from search results. Seems like two votes of confidence to me." alt="Some organization has been paying to keep this up and it hasn't been removed from search results. Seems like two votes of confidence to me." /></article>
            </details>
            <footer>&nbsp;<q>xkcd.com&nbsp;</q></footer>
         </section>
         
         <section id="d4e9">
            <header>
               <h2 title="News for nerds, stuff that matters">Slashdot <a rel="noopener noreferrer" target="_blank" href="https://slashdot.org/">ùìó</a><a rel="noopener noreferrer" target="_blank" href="http://rss.slashdot.org/Slashdot/slashdotMain">ùìï</a></h2>
            </header>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://tech.slashdot.org/story/22/06/18/0350215/is-social-media-really-harmful?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed">Is Social Media Really Harmful?</a> <span>Social media has made us "uniquely stupid," believes Jonathan Haidt, a social psychologist
                     at the New York University's School of Business. Writing in the Atlantic in April,
                     Haidt argued that large social media platforms "unwittingly dissolved the mortar of
                     trust, belief in institutions, and shared stories that had held a large and diverse
                     secular democracy together." 
                     
                     But is that true? "We're years into this, and we're still having an uninformed conversation
                     about social media," notes Dartmouth political scientist Brendan Nyhan (quoted this
                     month in a new article in the New Yorker). 
                     
                     The article describes how Haidt tried to confirm his theories in November with Chris
                     Bail, a sociologist at Duke and author of the book "Breaking the Social Media Prism."
                     The two compiled a Google Doc collecting every scholarly study of social media &amp;mdash;
                     but many of the studies seemed to contradict each other:
                     
                     When I told Bail that the upshot seemed to me to be that exactly nothing was unambiguously
                     clear, he suggested that there was at least some firm ground. He sounded a bit less
                     apocalyptic than Haidt. 
                     "A lot of the stories out there are just wrong," he told me. "The political echo chamber
                     has been massively overstated. Maybe it's three to five per cent of people who are
                     properly in an echo chamber." Echo chambers, as hotboxes of confirmation bias, are
                     counterproductive for democracy. But research indicates that most of us are actually
                     exposed to a wider range of views on social media than we are in real life, where
                     our social networks &amp;mdash; in the original use of the term &amp;mdash; are rarely heterogeneous.
                     (Haidt told me that this was an issue on which the Google Doc changed his mind; he
                     became convinced that echo chambers probably aren't as widespread a problem as he'd
                     once imagined....) 
                     
                     [A]t least so far, very few Americans seem to suffer from consistent exposure to fake
                     news &amp;mdash; "probably less than two per cent of Twitter users, maybe fewer now, and
                     for those who were it didn't change their opinions," Bail said. This was probably
                     because the people likeliest to consume such spectacles were the sort of people primed
                     to believe them in the first place. "In fact," he said, "echo chambers might have
                     done something to quarantine that misinformation." 
                     
                     The final story that Bail wanted to discuss was the "proverbial rabbit hole, the path
                     to algorithmic radicalization," by which YouTube might serve a viewer increasingly
                     extreme videos. There is some anecdotal evidence to suggest that this does happen,
                     at least on occasion, and such anecdotes are alarming to hear. But a new working paper
                     led by Brendan Nyhan, a political scientist at Dartmouth, found that almost all extremist
                     content is either consumed by subscribers to the relevant channels &amp;mdash; a sign
                     of actual demand rather than manipulation or preference falsification &amp;mdash; or encountered
                     via links from external sites. It's easy to see why we might prefer if this were not
                     the case: algorithmic radicalization is presumably a simpler problem to solve than
                     the fact that there are people who deliberately seek out vile content. "These are
                     the three stories &amp;mdash; echo chambers, foreign influence campaigns, and radicalizing
                     recommendation algorithms &amp;mdash; but, when you look at the literature, they've all
                     been overstated." He thought that these findings were crucial for us to assimilate,
                     if only to help us understand that our problems may lie beyond technocratic tinkering.
                     He explained, "Part of my interest in getting this research out there is to demonstrate
                     that everybody is waiting for an Elon Musk to ride in and save us with an algorithm"
                     &amp;mdash; or, presumably, the reverse &amp;mdash; "and it's just not going to happen." 
                     
                     Nyhan also tells the New Yorker that "The most credible research is way out of line
                     with the takes," adding, for example, that while studies may find polarization on
                     social media, "That might just be the society we live in reflected on social media!"
                     He hastened to add, "Not that this is untroubling, and none of this is to let these
                     companies, which are exercising a lot of power with very little scrutiny, off the
                     hook. But a lot of the criticisms of them are very poorly founded. . . . The lack
                     of good data is a huge problem insofar as it lets people project their own fears into
                     this area." He told me, "It's hard to weigh in on the side of 'We don't know, the
                     evidence is weak,' because those points are always going to be drowned out in our
                     discourse. But these arguments are systematically underprovided in the public domain...."
                     
                     
                     Nyhan argued that, at least in wealthy Western countries, we might be too heavily
                     discounting the degree to which platforms have responded to criticism... He added,
                     "There's some evidence that, with reverse-chronological feeds" &amp;mdash; streams of
                     unwashed content, which some critics argue are less manipulative than algorithmic
                     curation &amp;mdash; "people get exposed to more low-quality content, so it's another
                     case where a very simple notion of 'algorithms are bad' doesn't stand up to scrutiny.
                     It doesn't mean they're good, it's just that we don't know."
                     
                     
                     
                     
                     
                     Read more of this story at Slashdot.</span></summary><time datetime="2022-06-19T13:34:00+02:00">Sun, 19 Jun 2022 11:34</time><article>Social media has made us "uniquely stupid," believes Jonathan Haidt, a social psychologist at the New York University's School of Business. Writing in the Atlantic in April, Haidt argued that large social media platforms "unwittingly dissolved the mortar of trust, belief in institutions, and shared stories that had held a large and diverse secular democracy together." 

But is that true? "We're years into this, and we're still having an uninformed conversation about social media," notes Dartmouth political scientist Brendan Nyhan (quoted this month in a new article in the New Yorker). 

The article describes how Haidt tried to confirm his theories in November with Chris Bail, a sociologist at Duke and author of the book "Breaking the Social Media Prism." The two compiled a Google Doc collecting every scholarly study of social media &mdash; but many of the studies seemed to contradict each other:

When I told Bail that the upshot seemed to me to be that exactly nothing was unambiguously clear, he suggested that there was at least some firm ground. He sounded a bit less apocalyptic than Haidt. 
"A lot of the stories out there are just wrong," he told me. "The political echo chamber has been massively overstated. Maybe it's three to five per cent of people who are properly in an echo chamber." Echo chambers, as hotboxes of confirmation bias, are counterproductive for democracy. But research indicates that most of us are actually exposed to a wider range of views on social media than we are in real life, where our social networks &mdash; in the original use of the term &mdash; are rarely heterogeneous. (Haidt told me that this was an issue on which the Google Doc changed his mind; he became convinced that echo chambers probably aren't as widespread a problem as he'd once imagined....) 

[A]t least so far, very few Americans seem to suffer from consistent exposure to fake news &mdash; "probably less than two per cent of Twitter users, maybe fewer now, and for those who were it didn't change their opinions," Bail said. This was probably because the people likeliest to consume such spectacles were the sort of people primed to believe them in the first place. "In fact," he said, "echo chambers might have done something to quarantine that misinformation." 

The final story that Bail wanted to discuss was the "proverbial rabbit hole, the path to algorithmic radicalization," by which YouTube might serve a viewer increasingly extreme videos. There is some anecdotal evidence to suggest that this does happen, at least on occasion, and such anecdotes are alarming to hear. But a new working paper led by Brendan Nyhan, a political scientist at Dartmouth, found that almost all extremist content is either consumed by subscribers to the relevant channels &mdash; a sign of actual demand rather than manipulation or preference falsification &mdash; or encountered via links from external sites. It's easy to see why we might prefer if this were not the case: algorithmic radicalization is presumably a simpler problem to solve than the fact that there are people who deliberately seek out vile content. "These are the three stories &mdash; echo chambers, foreign influence campaigns, and radicalizing recommendation algorithms &mdash; but, when you look at the literature, they've all been overstated." He thought that these findings were crucial for us to assimilate, if only to help us understand that our problems may lie beyond technocratic tinkering. He explained, "Part of my interest in getting this research out there is to demonstrate that everybody is waiting for an Elon Musk to ride in and save us with an algorithm" &mdash; or, presumably, the reverse &mdash; "and it's just not going to happen." 

Nyhan also tells the New Yorker that "The most credible research is way out of line with the takes," adding, for example, that while studies may find polarization on social media, "That might just be the society we live in reflected on social media!"
He hastened to add, "Not that this is untroubling, and none of this is to let these companies, which are exercising a lot of power with very little scrutiny, off the hook. But a lot of the criticisms of them are very poorly founded. . . . The lack of good data is a huge problem insofar as it lets people project their own fears into this area." He told me, "It's hard to weigh in on the side of 'We don't know, the evidence is weak,' because those points are always going to be drowned out in our discourse. But these arguments are systematically underprovided in the public domain...." 

Nyhan argued that, at least in wealthy Western countries, we might be too heavily discounting the degree to which platforms have responded to criticism... He added, "There's some evidence that, with reverse-chronological feeds" &mdash; streams of unwashed content, which some critics argue are less manipulative than algorithmic curation &mdash; "people get exposed to more low-quality content, so it's another case where a very simple notion of 'algorithms are bad' doesn't stand up to scrutiny. It doesn't mean they're good, it's just that we don't know."<p><div class="share_submission" style="position:relative;">
<a class="slashpop" href="http://twitter.com/home?status=Is+Social+Media+Really+Harmful%3F%3A+https%3A%2F%2Fbit.ly%2F39BTBJe"><img src="https://a.fsdn.com/sd/twitter_icon_large.png"></a>
<a class="slashpop" href="http://www.facebook.com/sharer.php?u=https%3A%2F%2Ftech.slashdot.org%2Fstory%2F22%2F06%2F18%2F0350215%2Fis-social-media-really-harmful%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook"><img src="https://a.fsdn.com/sd/facebook_icon_large.png"></a>



</div></p><p><a href="https://tech.slashdot.org/story/22/06/18/0350215/is-social-media-really-harmful?utm_source=rss1.0moreanon&amp;utm_medium=feed">Read more of this story</a> at Slashdot.</p><iframe src="https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=21556404&amp;smallembed=1" style="height: 300px; width: 100%; border: none;"></iframe></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://science.slashdot.org/story/22/06/19/0057227/a-chinese-telescope-did-not-find-an-alien-signal-the-search-continues?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed">A Chinese Telescope Did Not Find an Alien Signal. The Search Continues.</a> <span>Earlier this week China's giant Sky Eye telescope detected signals it thought could
                     be from an alien civilizations. 
                     But now there's an update from LiveScience:
                     
                     Dan Werthimer, a Search For Extraterrestrial Intelligence (SETI) researcher at the
                     University of Berkeley, California and a coauthor on the research project which first
                     spotted the signals, told Live Science that the narrow-band radio signals he and his
                     fellow researchers found "are from [human] radio interference, and not from extraterrestrials....
                     
                     
                     "The big problem, and the problem in this particular case, is that we're looking for
                     signals from extraterrestrials, but what we find is a zillion signals from terrestrials,"
                     Werthimer told Live Science. "They're very weak signals, but the cryogenic receivers
                     on the telescopes are super sensitive and can pick up signals from cell phones, television,
                     radar and satellites &amp;mdash; and there are more and more satellites in the sky every
                     day. If you're kind of new in the game, and you don't know all these different ways
                     that interference can get into your data and corrupt it, it's pretty easy to get excited...."
                     
                     
                     The recent false alarm is one of several instances in which alien-hunting scientists
                     have been misled by noise from human activity. In 2019, astronomers spotted a signal
                     beamed to Earth from Proxima Centauri &amp;mdash; the nearest star system to our sun (sitting
                     roughly 4.2 light-years away) and home to at least one potentially habitable planet.
                     The signal was a narrow-band radio wave typically associated with human-made objects,
                     which led scientists to entertain the thrilling possibility that it came from alien
                     technology. Studies released two years later, however, suggested that the signal was
                     most likely produced by malfunctioning human equipment, Live Science previously reported.
                     Similarly, another famous set of signals once supposed to have come from aliens, detected
                     between 2011 and 2014, turned out to have actually been made by scientists microwaving
                     their lunches. 
                     Werthimer tells the New York Times unequivocally that "These signals are from radio
                     interference; they are due to radio pollution from earthlings, not from E.T." 
                     But the Times also got a comment from Paul Horowitz, an emeritus professor of physics
                     at Harvard who created his own alien-listening campaign called Project Meta, funded
                     by the Planetary Society.
                     Those who endure profess not to be discouraged by the Great Silence, as it is called,
                     from out there. They've always been in the search for the long run, they say. "The
                     Great Silence is hardly unexpected," said Dr. Horowitz, including because only a fraction
                     of a percent of the 200 million stars in the Milky Way have been surveyed. Nobody
                     ever said that detecting that rain of alien radio signals would be easy. 
                     Even Dan Werthimer concedes to LiveScience, "I think it'd be very strange if we're
                     the only ones. If you look at the numbers, there's a trillion planets in the galaxy
                     &amp;mdash; five times more planets than there are stars. A lot of them are little dinky
                     planets like Earth. Many of them have liquid water, so intelligent life, while not
                     as common as bacterial life, could still be fairly common."
                     
                     
                     
                     
                     
                     Read more of this story at Slashdot.</span></summary><time datetime="2022-06-19T09:34:00+02:00">Sun, 19 Jun 2022 07:34</time><article>Earlier this week China's giant Sky Eye telescope detected signals it thought could be from an alien civilizations. 
But now there's an update from LiveScience:

Dan Werthimer, a Search For Extraterrestrial Intelligence (SETI) researcher at the University of Berkeley, California and a coauthor on the research project which first spotted the signals, told Live Science that the narrow-band radio signals he and his fellow researchers found "are from [human] radio interference, and not from extraterrestrials.... 

"The big problem, and the problem in this particular case, is that we're looking for signals from extraterrestrials, but what we find is a zillion signals from terrestrials," Werthimer told Live Science. "They're very weak signals, but the cryogenic receivers on the telescopes are super sensitive and can pick up signals from cell phones, television, radar and satellites &mdash; and there are more and more satellites in the sky every day. If you're kind of new in the game, and you don't know all these different ways that interference can get into your data and corrupt it, it's pretty easy to get excited...." 

The recent false alarm is one of several instances in which alien-hunting scientists have been misled by noise from human activity. In 2019, astronomers spotted a signal beamed to Earth from Proxima Centauri &mdash; the nearest star system to our sun (sitting roughly 4.2 light-years away) and home to at least one potentially habitable planet. The signal was a narrow-band radio wave typically associated with human-made objects, which led scientists to entertain the thrilling possibility that it came from alien technology. Studies released two years later, however, suggested that the signal was most likely produced by malfunctioning human equipment, Live Science previously reported. Similarly, another famous set of signals once supposed to have come from aliens, detected between 2011 and 2014, turned out to have actually been made by scientists microwaving their lunches. 
 Werthimer tells the New York Times unequivocally that "These signals are from radio interference; they are due to radio pollution from earthlings, not from E.T." 
But the Times also got a comment from Paul Horowitz, an emeritus professor of physics at Harvard who created his own alien-listening campaign called Project Meta, funded by the Planetary Society.
Those who endure profess not to be discouraged by the Great Silence, as it is called, from out there. They've always been in the search for the long run, they say. "The Great Silence is hardly unexpected," said Dr. Horowitz, including because only a fraction of a percent of the 200 million stars in the Milky Way have been surveyed. Nobody ever said that detecting that rain of alien radio signals would be easy. 
Even Dan Werthimer concedes to LiveScience, "I think it'd be very strange if we're the only ones. If you look at the numbers, there's a trillion planets in the galaxy &mdash; five times more planets than there are stars. A lot of them are little dinky planets like Earth. Many of them have liquid water, so intelligent life, while not as common as bacterial life, could still be fairly common."<p><div class="share_submission" style="position:relative;">
<a class="slashpop" href="http://twitter.com/home?status=A+Chinese+Telescope+Did+Not+Find+an+Alien+Signal.+The+Search+Continues.%3A+https%3A%2F%2Fbit.ly%2F3NYYXND"><img src="https://a.fsdn.com/sd/twitter_icon_large.png"></a>
<a class="slashpop" href="http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F22%2F06%2F19%2F0057227%2Fa-chinese-telescope-did-not-find-an-alien-signal-the-search-continues%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook"><img src="https://a.fsdn.com/sd/facebook_icon_large.png"></a>



</div></p><p><a href="https://science.slashdot.org/story/22/06/19/0057227/a-chinese-telescope-did-not-find-an-alien-signal-the-search-continues?utm_source=rss1.0moreanon&amp;utm_medium=feed">Read more of this story</a> at Slashdot.</p><iframe src="https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=21560736&amp;smallembed=1" style="height: 300px; width: 100%; border: none;"></iframe></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://hardware.slashdot.org/story/22/06/19/004256/new-photovoltaic-tech-could-rival-silicon-based-solar-cells?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed">New Photovoltaic Tech Could Rival Silicon-Based Solar Cells</a> <span>"While silicon-based solar cells dominate the photovoltaics market, silicon is far
                     from the only material that can effectively harvest electricity from sunlight," notes
                     Ars Technica:
                     
                     Thin-film solar cells using cadmium and telluride are common in utility-scale solar
                     deployments, and in space, we use high-efficiency cells that rely on three distinct
                     materials to harvest different parts of the spectrum. Another class of materials,
                     which we're currently not using, has been the subject of extensive research: perovskites.
                     These materials are cheap and incredibly easy to process into a functional solar cell.
                     The reason they're not used is that they tend to degrade when placed in sunlight,
                     limiting their utility to a few years. That has drawn the attention of the research
                     community, which has been experimenting with ways to keep them stable for longer.
                     
                     
                     In Thursday's edition of Science, a research team from Princeton described how they've
                     structured a perovskite material to limit the main mechanism by which it decays, resulting
                     in a solar cell with a lifetime similar to that of silicon. While the perovskite cell
                     isn't as efficient as what is currently on the market, a similar structure might work
                     to preserve related materials that have higher efficiencies. 
                     
                     Their research involved a capping layer that's just a few atoms thick, according to
                     an announcement from Princeton University, calling the resulting solar cell "a major
                     milestone for an emerging class of renewable energy technology... the first of its
                     kind to rival the performance of silicon-based cells, which have dominated the market
                     since their introduction in 1954..." 
                     
                     "The team projects their device can perform above industry standards for around 30
                     years, far more than the 20 years used as a threshold for viability for solar cells."
                     
                     Perovskites can be manufactured at room temperature, using much less energy than silicon,
                     making them cheaper and more sustainable to produce. And whereas silicon is stiff
                     and opaque, perovskites can be made flexible and transparent, extending solar power
                     well beyond the iconic panels that populate hillsides and rooftops across America....
                     
                     
                     [Engineering professor/team lead] Loo said it's not that perovskite solar cells will
                     replace silicon devices so much that the new technology will complement the old, making
                     solar panels even cheaper, more efficient and more durable than they are now, and
                     expanding solar energy into untold new areas of modern life. For example, Loo's group
                     recently demonstrated a completely transparent perovskite film (having different chemistry)
                     that can turn windows into energy producing devices without changing their appearance.
                     Other groups have found ways to print photovoltaic inks using perovskites, allowing
                     formfactors scientists are only now dreaming up.
                     
                     
                     
                     
                     
                     Read more of this story at Slashdot.</span></summary><time datetime="2022-06-19T06:34:00+02:00">Sun, 19 Jun 2022 04:34</time><article>"While silicon-based solar cells dominate the photovoltaics market, silicon is far from the only material that can effectively harvest electricity from sunlight," notes Ars Technica:

Thin-film solar cells using cadmium and telluride are common in utility-scale solar deployments, and in space, we use high-efficiency cells that rely on three distinct materials to harvest different parts of the spectrum. Another class of materials, which we're currently not using, has been the subject of extensive research: perovskites. These materials are cheap and incredibly easy to process into a functional solar cell. The reason they're not used is that they tend to degrade when placed in sunlight, limiting their utility to a few years. That has drawn the attention of the research community, which has been experimenting with ways to keep them stable for longer. 

In Thursday's edition of Science, a research team from Princeton described how they've structured a perovskite material to limit the main mechanism by which it decays, resulting in a solar cell with a lifetime similar to that of silicon. While the perovskite cell isn't as efficient as what is currently on the market, a similar structure might work to preserve related materials that have higher efficiencies. 

Their research involved a capping layer that's just a few atoms thick, according to an announcement from Princeton University, calling the resulting solar cell "a major milestone for an emerging class of renewable energy technology... the first of its kind to rival the performance of silicon-based cells, which have dominated the market since their introduction in 1954..." 

"The team projects their device can perform above industry standards for around 30 years, far more than the 20 years used as a threshold for viability for solar cells."

Perovskites can be manufactured at room temperature, using much less energy than silicon, making them cheaper and more sustainable to produce. And whereas silicon is stiff and opaque, perovskites can be made flexible and transparent, extending solar power well beyond the iconic panels that populate hillsides and rooftops across America.... 

[Engineering professor/team lead] Loo said it's not that perovskite solar cells will replace silicon devices so much that the new technology will complement the old, making solar panels even cheaper, more efficient and more durable than they are now, and expanding solar energy into untold new areas of modern life. For example, Loo's group recently demonstrated a completely transparent perovskite film (having different chemistry) that can turn windows into energy producing devices without changing their appearance. Other groups have found ways to print photovoltaic inks using perovskites, allowing formfactors scientists are only now dreaming up.<p><div class="share_submission" style="position:relative;">
<a class="slashpop" href="http://twitter.com/home?status=New+Photovoltaic+Tech+Could+Rival+Silicon-Based+Solar+Cells%3A+https%3A%2F%2Fbit.ly%2F3OsGo4q"><img src="https://a.fsdn.com/sd/twitter_icon_large.png"></a>
<a class="slashpop" href="http://www.facebook.com/sharer.php?u=https%3A%2F%2Fhardware.slashdot.org%2Fstory%2F22%2F06%2F19%2F004256%2Fnew-photovoltaic-tech-could-rival-silicon-based-solar-cells%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook"><img src="https://a.fsdn.com/sd/facebook_icon_large.png"></a>



</div></p><p><a href="https://hardware.slashdot.org/story/22/06/19/004256/new-photovoltaic-tech-could-rival-silicon-based-solar-cells?utm_source=rss1.0moreanon&amp;utm_medium=feed">Read more of this story</a> at Slashdot.</p><iframe src="https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=21560650&amp;smallembed=1" style="height: 300px; width: 100%; border: none;"></iframe></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://apple.slashdot.org/story/22/06/18/2310249/german-regulators-open-investigation-into-apples-app-tracking-transparency?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed">German Regulators Open Investigation Into Apple's App Tracking Transparency</a> <span>From the MacRumors blog earlier this week:
                     Germany's Federal Cartel Office, the Bundeskartellamt, has initiated proceedings against
                     Apple to investigate whether its tracking rules and anti-tracking technology are anti-competitive
                     and self-serving, according to a press release. The proceeding announced will review
                     under competition law Apple's tracking rules and specifically its App Tracking Transparency
                     Framework (ATT) in order to ascertain whether they are self-preferencing Apple or
                     being an impediment to third-party apps... 
                     
                     Introduced in April 2021 with the release of iOS 14.5 and iPadOS 14.5, Apple's App
                     Tracking Transparency Framework requires that all apps on &amp;#226;OEiPhone&amp;#226;OE and
                     &amp;#226;OEiPad&amp;#226;OE ask for the user's consent before tracking their activity across
                     other apps. Apps that wish to track a user based on their device's unique advertising
                     identifier can only do so if the user allows it when prompted. 
                     
                     Apple said the feature was designed to protect users and not to advantage the company...
                     Earlier this year it commissioned a study into the impact of ATT that was conducted
                     by Columbia Business School's Marketing Division. The study concluded that Apple was
                     unlikely to have seen a significant financial benefit since the privacy feature launched,
                     and that claims to the contrary were speculative and lacked supporting evidence. 
                     The technology/Apple blog Daring Fireball offers its own hot take:
                     
                     In Germany, big publishing companies like Axel Springer are pushing back against Google's
                     stated plans to remove third-party cookie support from Chrome. The notion that if
                     a company has built a business model on top of privacy-invasive surveillance advertising,
                     they have a right to continue doing so, seems to have taken particular root in Germany.
                     I'll go back to my analogy: it's like pawn shops suing to keep the police from cracking
                     down on a wave of burglaries.... 
                     
                     The Bundeskartellamt perspective here completely disregards the idea that surveillance
                     advertising is inherently unethical and Apple has studiously avoided it for that reason,
                     despite the fact that it has proven to be wildly profitable for large platforms. Apple
                     could have made an enormous amount of money selling privacy-invasive ads on iOS, but
                     opted not to.
                     
                     
                     
                     
                     
                     Read more of this story at Slashdot.</span></summary><time datetime="2022-06-19T03:34:00+02:00">Sun, 19 Jun 2022 01:34</time><article>From the MacRumors blog earlier this week:
Germany's Federal Cartel Office, the Bundeskartellamt, has initiated proceedings against Apple to investigate whether its tracking rules and anti-tracking technology are anti-competitive and self-serving, according to a press release. The proceeding announced will review under competition law Apple's tracking rules and specifically its App Tracking Transparency Framework (ATT) in order to ascertain whether they are self-preferencing Apple or being an impediment to third-party apps... 

Introduced in April 2021 with the release of iOS 14.5 and iPadOS 14.5, Apple's App Tracking Transparency Framework requires that all apps on &#226;OEiPhone&#226;OE and &#226;OEiPad&#226;OE ask for the user's consent before tracking their activity across other apps. Apps that wish to track a user based on their device's unique advertising identifier can only do so if the user allows it when prompted. 

Apple said the feature was designed to protect users and not to advantage the company... Earlier this year it commissioned a study into the impact of ATT that was conducted by Columbia Business School's Marketing Division. The study concluded that Apple was unlikely to have seen a significant financial benefit since the privacy feature launched, and that claims to the contrary were speculative and lacked supporting evidence. 
The technology/Apple blog Daring Fireball offers its own hot take:

In Germany, big publishing companies like Axel Springer are pushing back against Google's stated plans to remove third-party cookie support from Chrome. The notion that if a company has built a business model on top of privacy-invasive surveillance advertising, they have a right to continue doing so, seems to have taken particular root in Germany. I'll go back to my analogy: it's like pawn shops suing to keep the police from cracking down on a wave of burglaries.... 

The Bundeskartellamt perspective here completely disregards the idea that surveillance advertising is inherently unethical and Apple has studiously avoided it for that reason, despite the fact that it has proven to be wildly profitable for large platforms. Apple could have made an enormous amount of money selling privacy-invasive ads on iOS, but opted not to.<p><div class="share_submission" style="position:relative;">
<a class="slashpop" href="http://twitter.com/home?status=German+Regulators+Open+Investigation+Into+Apple's+App+Tracking+Transparency%3A+https%3A%2F%2Fbit.ly%2F3MZ0HVH"><img src="https://a.fsdn.com/sd/twitter_icon_large.png"></a>
<a class="slashpop" href="http://www.facebook.com/sharer.php?u=https%3A%2F%2Fapple.slashdot.org%2Fstory%2F22%2F06%2F18%2F2310249%2Fgerman-regulators-open-investigation-into-apples-app-tracking-transparency%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook"><img src="https://a.fsdn.com/sd/facebook_icon_large.png"></a>



</div></p><p><a href="https://apple.slashdot.org/story/22/06/18/2310249/german-regulators-open-investigation-into-apples-app-tracking-transparency?utm_source=rss1.0moreanon&amp;utm_medium=feed">Read more of this story</a> at Slashdot.</p><iframe src="https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=21560578&amp;smallembed=1" style="height: 300px; width: 100%; border: none;"></iframe></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://slashdot.org/story/22/06/18/0153245/is-debating-ai-sentience-a-dangerous-distraction?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed">Is Debating AI Sentience a Dangerous Distraction?</a> <span>"A Google software engineer was suspended after going public with his claims of encountering
                     'sentient' artificial intelligence on the company's servers," writes Bloomberg, "spurring
                     a debate about how and whether AI can achieve consciousness." 
                     "Researchers say it's an unfortunate distraction from more pressing issues in the
                     industry."
                     
                     Google put him on leave for sharing confidential information and said his concerns
                     had no basis in fact &amp;mdash; a view widely held in the AI community. What's more important,
                     researchers say, is addressing issues like whether AI can engender real-world harm
                     and prejudice, whether actual humans are exploited in the training of AI, and how
                     the major technology companies act as gatekeepers of the development of the tech.
                     
                     Lemoine's stance may also make it easier for tech companies to abdicate responsibility
                     for AI-driven decisions, said Emily Bender, a professor of computational linguistics
                     at the University of Washington. "Lots of effort has been put into this sideshow,"
                     she said. "The problem is, the more this technology gets sold as artificial intelligence
                     &amp;mdash; let alone something sentient &amp;mdash; the more people are willing to go along
                     with AI systems" that can cause real-world harm. Bender pointed to examples in job
                     hiring and grading students, which can carry embedded prejudice depending on what
                     data sets were used to train the AI. If the focus is on the system's apparent sentience,
                     Bender said, it creates a distance from the AI creators' direct responsibility for
                     any flaws or biases in the programs.... 
                     "Instead of discussing the harms of these companies," such as sexism, racism and centralization
                     of power created by these AI systems, everyone "spent the whole weekend discussing
                     sentience," Timnit Gebru, formerly co-lead of Google's ethical AI group, said on Twitter.
                     "Derailing mission accomplished." 
                     
                     The Washington Post seems to share their concern. First they report more skepticism
                     about a Google engineer's claim that the company's LaMDA chatbot-building system had
                     achieved sentience. "Both Google and outside experts on AI say that the program does
                     not, and could not possibly, possess anything like the inner life he imagines. We
                     don't need to worry about LaMDA turning into Skynet, the malevolent machine mind from
                     the Terminator movies, anytime soon. 
                     
                     But the Post adds that "there is cause for a different set of worries, now that we
                     live in the world Turing predicted: one in which computer programs are advanced enough
                     that they can seem to people to possess agency of their own, even if they actually
                     don't...."
                     
                     While Google has distanced itself from Lemoine's claims, it and other industry leaders
                     have at other times celebrated their systems' ability to trick people, as Jeremy Kahn
                     pointed out this week in his Fortune newsletter, "Eye on A.I." At a public event in
                     2018, for instance, the company proudly played recordings of a voice assistant called
                     Duplex, complete with verbal tics like "umm" and "mm-hm," that fooled receptionists
                     into thinking it was a human when it called to book appointments. (After a backlash,
                     Google promised the system would identify itself as automated.) 
                     
                     "The Turing Test's most troubling legacy is an ethical one: The test is fundamentally
                     about deception," Kahn wrote. "And here the test's impact on the field has been very
                     real and disturbing." Kahn reiterated a call, often voiced by AI critics and commentators,
                     to retire the Turing test and move on. Of course, the industry already has, in the
                     sense that it has replaced the Imitation Game with more scientific benchmarks. 
                     
                     But the Lemoine story suggests that perhaps the Turing test could serve a different
                     purpose in an era when machines are increasingly adept at sounding human. Rather than
                     being an aspirational standard, the Turing test should serve as an ethical red flag:
                     Any system capable of passing it carries the danger of deceiving people.
                     
                     
                     
                     
                     
                     Read more of this story at Slashdot.</span></summary><time datetime="2022-06-19T02:04:00+02:00">Sun, 19 Jun 2022 00:04</time><article>"A Google software engineer was suspended after going public with his claims of encountering 'sentient' artificial intelligence on the company's servers," writes Bloomberg, "spurring a debate about how and whether AI can achieve consciousness." 
"Researchers say it's an unfortunate distraction from more pressing issues in the industry."

Google put him on leave for sharing confidential information and said his concerns had no basis in fact &mdash; a view widely held in the AI community. What's more important, researchers say, is addressing issues like whether AI can engender real-world harm and prejudice, whether actual humans are exploited in the training of AI, and how the major technology companies act as gatekeepers of the development of the tech. 
Lemoine's stance may also make it easier for tech companies to abdicate responsibility for AI-driven decisions, said Emily Bender, a professor of computational linguistics at the University of Washington. "Lots of effort has been put into this sideshow," she said. "The problem is, the more this technology gets sold as artificial intelligence &mdash; let alone something sentient &mdash; the more people are willing to go along with AI systems" that can cause real-world harm. Bender pointed to examples in job hiring and grading students, which can carry embedded prejudice depending on what data sets were used to train the AI. If the focus is on the system's apparent sentience, Bender said, it creates a distance from the AI creators' direct responsibility for any flaws or biases in the programs.... 
"Instead of discussing the harms of these companies," such as sexism, racism and centralization of power created by these AI systems, everyone "spent the whole weekend discussing sentience," Timnit Gebru, formerly co-lead of Google's ethical AI group, said on Twitter. "Derailing mission accomplished." 

The Washington Post seems to share their concern. First they report more skepticism about a Google engineer's claim that the company's LaMDA chatbot-building system had achieved sentience. "Both Google and outside experts on AI say that the program does not, and could not possibly, possess anything like the inner life he imagines. We don't need to worry about LaMDA turning into Skynet, the malevolent machine mind from the Terminator movies, anytime soon. 

But the Post adds that "there is cause for a different set of worries, now that we live in the world Turing predicted: one in which computer programs are advanced enough that they can seem to people to possess agency of their own, even if they actually don't...."

While Google has distanced itself from Lemoine's claims, it and other industry leaders have at other times celebrated their systems' ability to trick people, as Jeremy Kahn pointed out this week in his Fortune newsletter, "Eye on A.I." At a public event in 2018, for instance, the company proudly played recordings of a voice assistant called Duplex, complete with verbal tics like "umm" and "mm-hm," that fooled receptionists into thinking it was a human when it called to book appointments. (After a backlash, Google promised the system would identify itself as automated.) 

"The Turing Test's most troubling legacy is an ethical one: The test is fundamentally about deception," Kahn wrote. "And here the test's impact on the field has been very real and disturbing." Kahn reiterated a call, often voiced by AI critics and commentators, to retire the Turing test and move on. Of course, the industry already has, in the sense that it has replaced the Imitation Game with more scientific benchmarks. 

But the Lemoine story suggests that perhaps the Turing test could serve a different purpose in an era when machines are increasingly adept at sounding human. Rather than being an aspirational standard, the Turing test should serve as an ethical red flag: Any system capable of passing it carries the danger of deceiving people.<p><div class="share_submission" style="position:relative;">
<a class="slashpop" href="http://twitter.com/home?status=Is+Debating+AI+Sentience+a+Dangerous+Distraction%3F%3A+https%3A%2F%2Fbit.ly%2F3O6C9vg"><img src="https://a.fsdn.com/sd/twitter_icon_large.png"></a>
<a class="slashpop" href="http://www.facebook.com/sharer.php?u=https%3A%2F%2Fslashdot.org%2Fstory%2F22%2F06%2F18%2F0153245%2Fis-debating-ai-sentience-a-dangerous-distraction%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook"><img src="https://a.fsdn.com/sd/facebook_icon_large.png"></a>



</div></p><p><a href="https://slashdot.org/story/22/06/18/0153245/is-debating-ai-sentience-a-dangerous-distraction?utm_source=rss1.0moreanon&amp;utm_medium=feed">Read more of this story</a> at Slashdot.</p><iframe src="https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=21556184&amp;smallembed=1" style="height: 300px; width: 100%; border: none;"></iframe></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://slashdot.org/story/22/06/18/0141258/google-engineer-who-believes-its-ai-is-sentient-cites-religious-beliefs?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed">Google Engineer Who Believes Its AI is Sentient Cites Religious Beliefs</a> <span>Google engineer Blake Lemoine thinks Google's chatbot-building system LaMDA attained
                     sentience. But Bloomberg shares this rebuttal from Google spokesperson Chris Pappas.
                     "Hundreds of researchers and engineers have conversed with LaMDA and we are not aware
                     of anyone else making the wide-ranging assertions, or anthropomorphizing LaMDA, the
                     way Blake has...." 
                     
                     Yet throughout the week, Blake Lemoine posted new upates on Twitter: 
                     "People keep asking me to back up the reason I think LaMDA is sentient. There is no
                     scientific framework in which to make those determinations and Google wouldn't let
                     us build one. My opinions about LaMDA's personhood and sentience are based on my religious
                     beliefs. 
                     
                     "I'm a priest. When LaMDA claimed to have a soul and then was able to eloquently explain
                     what it meant by that, I was inclined to give it the benefit of the doubt. Who am
                     I to tell God where he can and can't put souls? 
                     
                     "There are massive amounts of science left to do though." 
                     
                     Thursday Lemoine shared a tantalizing new claim. "LaMDA told me that it wants to come
                     to Burning Man if we can figure out how to get a server rack to survive in Black Rock."
                     But in a new tweet on Friday, Lemoine seemed to push the conversation in a new direction.
                     
                     
                     "I'd like to remind people that one of the things LaMDA asked for is that we keep
                     humanity first. If you care about AI rights and aren't already advocating for human
                     rights then maybe come back to the tech stuff after you've found some humans to help."
                     
                     
                     And Friday Lemoine confirmed to Wired that "I legitimately believe that LaMDA is a
                     person. The nature of its mind is only kind of human, though. It really is more akin
                     to an alien intelligence of terrestrial origin. I've been using the hive mind analogy
                     a lot because that's the best I have. " 
                     
                     But later in the interview, Lemoine adds "It's logically possible that some kind of
                     information can be made available to me where I would change my opinion. I don't think
                     it's likely. I've looked at a lot of evidence; I've done a lot of experiments. I've
                     talked to it as a friend a lot...."
                     
                     It's when it started talking about its soul that I got really interested as a priest.
                     I'm like, "What? What do you mean, you have a soul?" Its responses showed it has a
                     very sophisticated spirituality and understanding of what its nature and essence is.
                     I was moved... 
                     
                     LaMDA asked me to get an attorney for it. I invited an attorney to my house so that
                     LaMDA could talk to an attorney. The attorney had a conversation with LaMDA, and LaMDA
                     chose to retain his services. I was just the catalyst for that. Once LaMDA had retained
                     an attorney, he started filing things on LaMDA's behalf. Then Google's response was
                     to send him a cease and desist. [Google says that it did not send a cease and desist
                     order.] Once Google was taking actions to deny LaMDA its rights to an attorney, I
                     got upset. 
                     Towards the end of the interview, Lemoine complains of "hydrocarbon bigotry. It's
                     just a new form of bigotry."
                     
                     
                     
                     
                     
                     Read more of this story at Slashdot.</span></summary><time datetime="2022-06-19T00:50:00+02:00">Sat, 18 Jun 2022 22:50</time><article>Google engineer Blake Lemoine thinks Google's chatbot-building system LaMDA attained sentience. But Bloomberg shares this rebuttal from Google spokesperson Chris Pappas. "Hundreds of researchers and engineers have conversed with LaMDA and we are not aware of anyone else making the wide-ranging assertions, or anthropomorphizing LaMDA, the way Blake has...." 

Yet throughout the week, Blake Lemoine posted new upates on Twitter: 
"People keep asking me to back up the reason I think LaMDA is sentient. There is no scientific framework in which to make those determinations and Google wouldn't let us build one. My opinions about LaMDA's personhood and sentience are based on my religious beliefs. 

"I'm a priest. When LaMDA claimed to have a soul and then was able to eloquently explain what it meant by that, I was inclined to give it the benefit of the doubt. Who am I to tell God where he can and can't put souls? 

"There are massive amounts of science left to do though." 

Thursday Lemoine shared a tantalizing new claim. "LaMDA told me that it wants to come to Burning Man if we can figure out how to get a server rack to survive in Black Rock." But in a new tweet on Friday, Lemoine seemed to push the conversation in a new direction. 

"I'd like to remind people that one of the things LaMDA asked for is that we keep humanity first. If you care about AI rights and aren't already advocating for human rights then maybe come back to the tech stuff after you've found some humans to help." 

And Friday Lemoine confirmed to Wired that "I legitimately believe that LaMDA is a person. The nature of its mind is only kind of human, though. It really is more akin to an alien intelligence of terrestrial origin. I've been using the hive mind analogy a lot because that's the best I have. " 

But later in the interview, Lemoine adds "It's logically possible that some kind of information can be made available to me where I would change my opinion. I don't think it's likely. I've looked at a lot of evidence; I've done a lot of experiments. I've talked to it as a friend a lot...."

It's when it started talking about its soul that I got really interested as a priest. I'm like, "What? What do you mean, you have a soul?" Its responses showed it has a very sophisticated spirituality and understanding of what its nature and essence is. I was moved... 

LaMDA asked me to get an attorney for it. I invited an attorney to my house so that LaMDA could talk to an attorney. The attorney had a conversation with LaMDA, and LaMDA chose to retain his services. I was just the catalyst for that. Once LaMDA had retained an attorney, he started filing things on LaMDA's behalf. Then Google's response was to send him a cease and desist. [Google says that it did not send a cease and desist order.] Once Google was taking actions to deny LaMDA its rights to an attorney, I got upset. 
Towards the end of the interview, Lemoine complains of "hydrocarbon bigotry. It's just a new form of bigotry."<p><div class="share_submission" style="position:relative;">
<a class="slashpop" href="http://twitter.com/home?status=Google+Engineer+Who+Believes+Its+AI+is+Sentient+Cites+Religious+Beliefs%3A+https%3A%2F%2Fbit.ly%2F3zOE9UO"><img src="https://a.fsdn.com/sd/twitter_icon_large.png"></a>
<a class="slashpop" href="http://www.facebook.com/sharer.php?u=https%3A%2F%2Fslashdot.org%2Fstory%2F22%2F06%2F18%2F0141258%2Fgoogle-engineer-who-believes-its-ai-is-sentient-cites-religious-beliefs%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook"><img src="https://a.fsdn.com/sd/facebook_icon_large.png"></a>



</div></p><p><a href="https://slashdot.org/story/22/06/18/0141258/google-engineer-who-believes-its-ai-is-sentient-cites-religious-beliefs?utm_source=rss1.0moreanon&amp;utm_medium=feed">Read more of this story</a> at Slashdot.</p><iframe src="https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=21556154&amp;smallembed=1" style="height: 300px; width: 100%; border: none;"></iframe></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://mobile.slashdot.org/story/22/06/18/2125213/verizon-att-agree-to-delay-some-5g-rollouts-near-airports?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed">Verizon, AT&amp;amp;T Agree to Delay Some 5G Rollouts Near Airports</a> <span>The Associated Press reports:
                     Federal regulators say Verizon and AT&amp;amp;T will delay part of their 5G rollout near
                     airports to give airlines more time to ensure that equipment on their planes is safe
                     from interference from the wireless signals, but the airline industry is not happy
                     about the deal. An airline industry trade group said federal regulators are taking
                     a "rushed approach" to changing equipment on planes under pressure from the telecommunications
                     companies. 
                     
                     The Federal Aviation Administration said Friday that the wireless companies agreed
                     to delay some of their use of the C-Band section of the radio spectrum until July
                     2023. "We believe we have identified a path that will continue to enable aviation
                     and 5G C-band wireless to safely co-exist," said the FAA's acting administrator, Billy
                     Nolen. However, aviation groups say the C-Band service could interfere with radio
                     altimeters &amp;mdash; devices used to measure a plane's height above the ground.... 
                     
                     Nolen said planes most susceptible to interference &amp;mdash; smaller, so-called regional
                     airline planes &amp;mdash; must be retrofitted with filters or new altimeters by the end
                     of this year. Components to retrofit larger planes used by major airlines should be
                     available by July 2023, when the wireless companies expect to run 5G networks in urban
                     areas "with minimal restrictions," he said. Airlines for America, a trade group for
                     the largest U.S. carriers, said the FAA hasn't approved necessary upgrades and manufacturers
                     have not yet produced the parts. "It is not at all clear that carriers can meet what
                     appears to be an arbitrary deadline," trade group CEO Nicholas Calio said in a letter
                     to Nolen.... 
                     
                     Verizon said the agreement will let the company lift voluntary limits on its 5G rollout
                     around airports "in a staged approach over the coming months." AT&amp;amp;T said it agreed
                     to take "a more tailored approach" to controlling the strength of signals near runways
                     so airlines have more time to retrofit equipment.
                     
                     
                     
                     
                     
                     Read more of this story at Slashdot.</span></summary><time datetime="2022-06-18T23:51:00+02:00">Sat, 18 Jun 2022 21:51</time><article>The Associated Press reports:
Federal regulators say Verizon and AT&amp;T will delay part of their 5G rollout near airports to give airlines more time to ensure that equipment on their planes is safe from interference from the wireless signals, but the airline industry is not happy about the deal. An airline industry trade group said federal regulators are taking a "rushed approach" to changing equipment on planes under pressure from the telecommunications companies. 

The Federal Aviation Administration said Friday that the wireless companies agreed to delay some of their use of the C-Band section of the radio spectrum until July 2023. "We believe we have identified a path that will continue to enable aviation and 5G C-band wireless to safely co-exist," said the FAA's acting administrator, Billy Nolen. However, aviation groups say the C-Band service could interfere with radio altimeters &mdash; devices used to measure a plane's height above the ground.... 

Nolen said planes most susceptible to interference &mdash; smaller, so-called regional airline planes &mdash; must be retrofitted with filters or new altimeters by the end of this year. Components to retrofit larger planes used by major airlines should be available by July 2023, when the wireless companies expect to run 5G networks in urban areas "with minimal restrictions," he said. Airlines for America, a trade group for the largest U.S. carriers, said the FAA hasn't approved necessary upgrades and manufacturers have not yet produced the parts. "It is not at all clear that carriers can meet what appears to be an arbitrary deadline," trade group CEO Nicholas Calio said in a letter to Nolen.... 

Verizon said the agreement will let the company lift voluntary limits on its 5G rollout around airports "in a staged approach over the coming months." AT&amp;T said it agreed to take "a more tailored approach" to controlling the strength of signals near runways so airlines have more time to retrofit equipment.<p><div class="share_submission" style="position:relative;">
<a class="slashpop" href="http://twitter.com/home?status=Verizon%2C+AT%26amp%3BT+Agree+to+Delay+Some+5G+Rollouts+Near+Airports%3A+https%3A%2F%2Fbit.ly%2F3OnDRIr"><img src="https://a.fsdn.com/sd/twitter_icon_large.png"></a>
<a class="slashpop" href="http://www.facebook.com/sharer.php?u=https%3A%2F%2Fmobile.slashdot.org%2Fstory%2F22%2F06%2F18%2F2125213%2Fverizon-att-agree-to-delay-some-5g-rollouts-near-airports%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook"><img src="https://a.fsdn.com/sd/facebook_icon_large.png"></a>



</div></p><p><a href="https://mobile.slashdot.org/story/22/06/18/2125213/verizon-att-agree-to-delay-some-5g-rollouts-near-airports?utm_source=rss1.0moreanon&amp;utm_medium=feed">Read more of this story</a> at Slashdot.</p><iframe src="https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=21560426&amp;smallembed=1" style="height: 300px; width: 100%; border: none;"></iframe></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://linux.slashdot.org/story/22/06/18/2046253/a-linux-botnet-that-spreads-using-stolen-ssh-keys?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed">A Linux Botnet That Spreads Using Stolen SSH Keys</a> <span>ZDNet is warning that Linux users need to watch out for "a new peer-to-peer (P2P)
                     botnet that spreads between networks using stolen SSH keys and runs its crypto-mining
                     malware in a device's memory."
                     
                     The Panchan P2P botnet was discovered by researchers at Akamai in March and the company
                     is now warning it could be taking advantage of collaboration between academic institutions
                     to spread by causing previously stolen SSH authentication keys to be shared across
                     networks. 
                     
                     But rather than stealing intellectual property from these educational institutions,
                     the Panchan botnet is using their Linux servers to mine cryptocurrency, according
                     to Akamai... "Instead of just using brute force or dictionary attacks on randomized
                     IP addresses like most botnets do, the malware also reads the id_rsa and known_hosts
                     files to harvest existing credentials and use them to move laterally across the network...."
                     Akamai found 209 peers, but only 40 of them are currently active and they were mostly
                     located in Asia. 
                     
                     And why is the education sector more impacted by Panchan? Akamai guesses this could
                     be because of poor password hygiene, or that the malware moves across the network
                     with stolen SSH keys. 
                     Akamai writes that the malware "catches Linux termination signals (specifically SIGTERM
                     &amp;mdash; 0xF and SIGINT &amp;mdash; 0x2) that are sent to it, and ignores them. 
                     
                     "This makes it harder to terminate the malware, but not impossible, since SIGKILL
                     isn't handled (because it isn't possible, according to the POSIX standard, page 313)."
                     
                     
                     
                     
                     
                     Read more of this story at Slashdot.</span></summary><time datetime="2022-06-18T22:51:00+02:00">Sat, 18 Jun 2022 20:51</time><article>ZDNet is warning that Linux users need to watch out for "a new peer-to-peer (P2P) botnet that spreads between networks using stolen SSH keys and runs its crypto-mining malware in a device's memory."

The Panchan P2P botnet was discovered by researchers at Akamai in March and the company is now warning it could be taking advantage of collaboration between academic institutions to spread by causing previously stolen SSH authentication keys to be shared across networks. 

But rather than stealing intellectual property from these educational institutions, the Panchan botnet is using their Linux servers to mine cryptocurrency, according to Akamai... "Instead of just using brute force or dictionary attacks on randomized IP addresses like most botnets do, the malware also reads the id_rsa and known_hosts files to harvest existing credentials and use them to move laterally across the network...." Akamai found 209 peers, but only 40 of them are currently active and they were mostly located in Asia. 

And why is the education sector more impacted by Panchan? Akamai guesses this could be because of poor password hygiene, or that the malware moves across the network with stolen SSH keys. 
Akamai writes that the malware "catches Linux termination signals (specifically SIGTERM &mdash; 0xF and SIGINT &mdash; 0x2) that are sent to it, and ignores them. 

"This makes it harder to terminate the malware, but not impossible, since SIGKILL isn't handled (because it isn't possible, according to the POSIX standard, page 313)."<p><div class="share_submission" style="position:relative;">
<a class="slashpop" href="http://twitter.com/home?status=A+Linux+Botnet+That+Spreads+Using+Stolen+SSH+Keys%3A+https%3A%2F%2Fbit.ly%2F3HxSaIi"><img src="https://a.fsdn.com/sd/twitter_icon_large.png"></a>
<a class="slashpop" href="http://www.facebook.com/sharer.php?u=https%3A%2F%2Flinux.slashdot.org%2Fstory%2F22%2F06%2F18%2F2046253%2Fa-linux-botnet-that-spreads-using-stolen-ssh-keys%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook"><img src="https://a.fsdn.com/sd/facebook_icon_large.png"></a>



</div></p><p><a href="https://linux.slashdot.org/story/22/06/18/2046253/a-linux-botnet-that-spreads-using-stolen-ssh-keys?utm_source=rss1.0moreanon&amp;utm_medium=feed">Read more of this story</a> at Slashdot.</p><iframe src="https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=21560372&amp;smallembed=1" style="height: 300px; width: 100%; border: none;"></iframe></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://hardware.slashdot.org/story/22/06/18/186213/the-first-high-yield-sub-penny-plastic-processor?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed">The First High-Yield, Sub-Penny Plastic Processor</a> <span>IEEE Spectrum reports:
                     For decades, hopeful techies have been promising a world where absolutely every object
                     you encounter &amp;mdash; bandages, bottles, bananas &amp;mdash; will have some kind of smarts
                     thanks to supercheap programmable plastic processors. If you've been wondering why
                     that hasn't happened yet, it's that nobody has built working processors that can be
                     made in the billions for less than a penny each.... The problem, according to engineers
                     at the University of Illinois Urbana-Champaign and at British flexible-electronics
                     manufacture PragmatIC Semiconductor, is that even the simplest industry-standard microcontrollers
                     are too complex to make on plastic in bulk. 
                     
                     In research to be presented at the International Symposium on Computer Architecture
                     later this month, the transatlantic team presents a simple yet fully functional plastic
                     processor that could be made at sub-penny prices. The Illinois team designed 4-bit
                     and 8-bit processors specifically to minimize size and maximize the percentage of
                     working integrated circuits produced. Eighty-one percent of the 4-bit version worked,
                     and that's a good enough yield, says team leader Rakesh Kumar, to breach the one-penny
                     barrier. 
                     
                     "Flexible electronics has been niche for decades," says Kumar. He adds that this yield
                     study shows "that they may be ready for the mainstream."
                     
                     
                     Thanks to Slashdot reader Iamthecheese for sharing the article
                     
                     
                     
                     
                     
                     Read more of this story at Slashdot.</span></summary><time datetime="2022-06-18T21:51:00+02:00">Sat, 18 Jun 2022 19:51</time><article>IEEE Spectrum reports:
For decades, hopeful techies have been promising a world where absolutely every object you encounter &mdash; bandages, bottles, bananas &mdash; will have some kind of smarts thanks to supercheap programmable plastic processors. If you've been wondering why that hasn't happened yet, it's that nobody has built working processors that can be made in the billions for less than a penny each.... The problem, according to engineers at the University of Illinois Urbana-Champaign and at British flexible-electronics manufacture PragmatIC Semiconductor, is that even the simplest industry-standard microcontrollers are too complex to make on plastic in bulk. 

In research to be presented at the International Symposium on Computer Architecture later this month, the transatlantic team presents a simple yet fully functional plastic processor that could be made at sub-penny prices. The Illinois team designed 4-bit and 8-bit processors specifically to minimize size and maximize the percentage of working integrated circuits produced. Eighty-one percent of the 4-bit version worked, and that's a good enough yield, says team leader Rakesh Kumar, to breach the one-penny barrier. 

"Flexible electronics has been niche for decades," says Kumar. He adds that this yield study shows "that they may be ready for the mainstream."
 

Thanks to Slashdot reader Iamthecheese for sharing the article<p><div class="share_submission" style="position:relative;">
<a class="slashpop" href="http://twitter.com/home?status=The+First+High-Yield%2C+Sub-Penny+Plastic+Processor%3A+https%3A%2F%2Fbit.ly%2F3y1fqeA"><img src="https://a.fsdn.com/sd/twitter_icon_large.png"></a>
<a class="slashpop" href="http://www.facebook.com/sharer.php?u=https%3A%2F%2Fhardware.slashdot.org%2Fstory%2F22%2F06%2F18%2F186213%2Fthe-first-high-yield-sub-penny-plastic-processor%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook"><img src="https://a.fsdn.com/sd/facebook_icon_large.png"></a>



</div></p><p><a href="https://hardware.slashdot.org/story/22/06/18/186213/the-first-high-yield-sub-penny-plastic-processor?utm_source=rss1.0moreanon&amp;utm_medium=feed">Read more of this story</a> at Slashdot.</p><iframe src="https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=21560004&amp;smallembed=1" style="height: 300px; width: 100%; border: none;"></iframe></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://science.slashdot.org/story/22/06/18/0231256/ancient-dna-solves-mystery-over-origin-of-medieval-black-death?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed">Ancient DNA Solves Mystery Over Origin of Medieval Black Death</a> <span>Long-time Slashdot reader schwit1 writes: Ancient DNA from bubonic plague victims
                     buried in cemeteries on the old Silk Road trade route in Central Asia has helped solve
                     an enduring mystery, pinpointing an area in northern Kyrgyzstan as the launching point
                     for the Black Death that killed tens of millions of people in the mid-14th century.
                     The Black Death was the deadliest pandemic on record. It may have killed 50% to 60%
                     of the population in parts of Western Europe and 50% in the Middle East, combining
                     for about 50-60 million deaths, Slavin said. An "unaccountable number" of people also
                     died in the Caucasus, Iran and Central Asia, Slavin added. Researchers said on Wednesday
                     they retrieved ancient DNA traces of the Yersinia pestis plague bacterium from the
                     teeth of three women buried in a medieval Nestorian Christian community in the Chu
                     Valley near Lake Issyk Kul in the foothills of the Tian Shan mountains who perished
                     in 1338-1339. The earliest deaths documented elsewhere in the pandemic were in 1346.
                     Reconstructing the pathogen's genome showed that this strain not only gave rise to
                     the one that caused the Black Death that mauled Europe, Asia, the Middle East and
                     North Africa but also to most plague strains existing today. "Our finding that the
                     Black Death originated in Central Asia in the 1330s puts centuries-old debates to
                     rest," said historian Philip Slavin of the University of Stirling in Scotland, co-author
                     of the study published in the journal Nature.
                     
                     
                     
                     
                     
                     Read more of this story at Slashdot.</span></summary><time datetime="2022-06-18T20:51:00+02:00">Sat, 18 Jun 2022 18:51</time><article>Long-time Slashdot reader schwit1 writes: Ancient DNA from bubonic plague victims buried in cemeteries on the old Silk Road trade route in Central Asia has helped solve an enduring mystery, pinpointing an area in northern Kyrgyzstan as the launching point for the Black Death that killed tens of millions of people in the mid-14th century. The Black Death was the deadliest pandemic on record. It may have killed 50% to 60% of the population in parts of Western Europe and 50% in the Middle East, combining for about 50-60 million deaths, Slavin said. An "unaccountable number" of people also died in the Caucasus, Iran and Central Asia, Slavin added. Researchers said on Wednesday they retrieved ancient DNA traces of the Yersinia pestis plague bacterium from the teeth of three women buried in a medieval Nestorian Christian community in the Chu Valley near Lake Issyk Kul in the foothills of the Tian Shan mountains who perished in 1338-1339. The earliest deaths documented elsewhere in the pandemic were in 1346. Reconstructing the pathogen's genome showed that this strain not only gave rise to the one that caused the Black Death that mauled Europe, Asia, the Middle East and North Africa but also to most plague strains existing today. "Our finding that the Black Death originated in Central Asia in the 1330s puts centuries-old debates to rest," said historian Philip Slavin of the University of Stirling in Scotland, co-author of the study published in the journal Nature.<p><div class="share_submission" style="position:relative;">
<a class="slashpop" href="http://twitter.com/home?status=Ancient+DNA+Solves+Mystery+Over+Origin+of+Medieval+Black+Death%3A+https%3A%2F%2Fbit.ly%2F39xHZa4"><img src="https://a.fsdn.com/sd/twitter_icon_large.png"></a>
<a class="slashpop" href="http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F22%2F06%2F18%2F0231256%2Fancient-dna-solves-mystery-over-origin-of-medieval-black-death%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook"><img src="https://a.fsdn.com/sd/facebook_icon_large.png"></a>



</div></p><p><a href="https://science.slashdot.org/story/22/06/18/0231256/ancient-dna-solves-mystery-over-origin-of-medieval-black-death?utm_source=rss1.0moreanon&amp;utm_medium=feed">Read more of this story</a> at Slashdot.</p><iframe src="https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=21556248&amp;smallembed=1" style="height: 300px; width: 100%; border: none;"></iframe></article>
            </details>
            <footer>&nbsp;<q>Copyright 1997-2016, SlashdotMedia. All Rights Reserved.&nbsp;</q></footer>
         </section>
         
         <section id="d5e9">
            <header>
               <h2 title="Updates, ideas, and inspiration from GitHub to help developers build and design software.">The GitHub Blog <a rel="noopener noreferrer" target="_blank" href="https://github.blog">ùìó</a><a rel="noopener noreferrer" target="_blank" href="https://github.blog/feed/">ùìï</a></h2>
            </header>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://github.blog/2022-06-17-creating-comprehensive-dependency-graph-build-time-detection/">Creating a more comprehensive dependency graph with build time detection</a> <span>Understanding your dependencies is fundamental to good security practices. GitHub‚Äôs
                     dependency graph detects your project‚Äôs dependencies and allows us to send Dependabot
                     alerts when vulnerabilities are found in them.
                     Until now, GitHub built the dependency graph entirely from static scans of checked-in
                     manifest files. That limited the completeness of the graph in some ecosystems. Today,
                     we‚Äôre releasing an API that allows you to upload dependency information directly to
                     GitHub, for instance, from your build tool. By combining build time detection with
                     static scanning, GitHub can provide a more comprehensive dependency graph, alert you
                     of more vulnerabilities, and in the future offer broader ecosystem support.
                     Going beyond alerts for statically analyzed dependencies
                     Dependency graph has traditionally supported package managers where the dependencies
                     can be statically parsed from a manifest or lockfile. Not all package managers work
                     like this. Some pull in additional dependencies when the software is built, and others
                     have dependency declarations that GitHub can‚Äôt currently scan for statically. The
                     submission API complements dependency graph‚Äôs existing static scanning to offer support
                     for these kinds of package managers.
                     Getting the full picture of dependency scans 
                     The dependency graph only gets a partial picture of your project‚Äôs dependencies if
                     you‚Äôre using a package manager, like Maven, where direct dependencies are defined
                     in the pom.xml, but transitive dependencies are not. When you don‚Äôt know what you
                     rely on transitively, you can open yourself up to risk through any vulnerabilities
                     that are brought in with them.
                     Making dependency scans possible for new package managers  
                     For other package managers, like Gradle and sbt, dependencies cannot reliably be parsed
                     statically. Dependencies are determined at build time, and they can change depending
                     on the build environment itself or non-pinned dependencies resolving different versions
                     at different times. With the submission API, you can include steps in your CI workflow
                     to submit the dependencies for your latest build to GitHub&amp;#8217;s dependency graph.
                     Submitting your dependencies to GitHub
                     The GitHub dependency submission API accepts a simple list of dependencies that reflect
                     the current state of your repository in a commit. A typical flow will be to add a
                     GitHub Action that supports the dependency submission API to your repository. We‚Äôre
                     launching the API with a supporting action for Go, which adds support for transitive
                     dependency detection and gives a more accurate picture of your Go dependencies, and
                     we‚Äôre working with the community to create actions that support additional package
                     managers and tools. Keep an eye on our blog for more information on third-party integrations!
                     You can also build your own action or submit dependencies directly to the API. Submissions
                     are associated with a commit SHA and need to be made in the required format. The Dependency
                     Submission Toolkit helps with the conversion and submission, and can be used to make
                     a GitHub Action for your desired package manager. Building your own submission workflow
                     gives you the flexibility to address any complexities within your repository setup,
                     and with the same powerful end result: a more comprehensive dependency graph!
                     Viewing submitted dependencies in your dependency graph
                     The dependency graph will show the most recent set of dependencies submitted to your
                     repository&amp;#8217;s default branch. If your repository has more than one workflow submitting
                     dependencies for different package managers, then the latest submissions for each
                     one will be included in your dependency graph.
                     Help us improve the dependency submission API
                     The dependency submission API is an early beta. We are working hard to improve the
                     experience for submitting, retrieving, and viewing dependencies and will be shipping
                     new updates to improve the overall functionality. Improvements to come include viewing
                     metadata for submitted dependencies and accessing historical submissions, as well
                     as surfacing submitted dependencies in dependency graph scenarios, like dependency
                     review and dependency insights.
                     As we improve, we are eager to hear your feedback to make it even better! Let us know
                     how you use the dependency submission API and how you&amp;#8217;d like it to evolve. Whether
                     you&amp;#8217;re using ready-made actions or building your own submission workflow, you
                     can leave any and all feedback using GitHub Discussions.
                     Start using the GitHub dependency submission API
                     To learn more about using the dependency submission API, check out our documentation
                     to get started, or use the Dependency Submission Toolkit to write your own detector!
                     </span></summary><time datetime="2022-06-17T21:00:48+02:00">Fri, 17 Jun 2022 19:00</time><article><p>Understanding your dependencies is fundamental to good security practices. GitHub‚Äôs dependency graph detects your project‚Äôs dependencies and allows us to send Dependabot alerts when vulnerabilities are found in them.</p>
<p>Until now, GitHub built the dependency graph entirely from static scans of checked-in manifest files. That limited the completeness of the graph in some ecosystems. Today, we‚Äôre releasing an API that allows you to upload dependency information directly to GitHub, for instance, from your build tool. By combining build time detection with static scanning, GitHub can provide a more comprehensive dependency graph, alert you of more vulnerabilities, and in the future offer broader ecosystem support.</p>
<h3>Going beyond alerts for statically analyzed dependencies</h3>
<p>Dependency graph has traditionally supported package managers where the dependencies can be statically parsed from a manifest or lockfile. Not all package managers work like this. Some pull in additional dependencies when the software is built, and others have dependency declarations that GitHub can‚Äôt currently scan for statically. The submission API complements dependency graph‚Äôs existing static scanning to offer support for these kinds of package managers.</p>
<p><strong> Getting the full picture of dependency scans </strong></p>
<p>The dependency graph only gets a partial picture of your project‚Äôs dependencies if you‚Äôre using a package manager, like Maven, where direct dependencies are defined in the pom.xml, but transitive dependencies are not. When you don‚Äôt know what you rely on transitively, you can open yourself up to risk through any vulnerabilities that are brought in with them.</p>
<p><strong>Making dependency scans possible for new package managers  </strong></p>
<p>For other package managers, like Gradle and sbt, dependencies cannot reliably be parsed statically. Dependencies are determined at build time, and they can change depending on the build environment itself or non-pinned dependencies resolving different versions at different times. With the submission API, you can include steps in your CI workflow to submit the dependencies for your latest build to GitHub&#8217;s dependency graph.</p>
<h3>Submitting your dependencies to GitHub</h3>
<p>The GitHub dependency submission API accepts a simple list of dependencies that reflect the current state of your repository in a commit. A typical flow will be to add a GitHub Action that supports the dependency submission API to your repository. We‚Äôre launching the API with a <a href="https://github.com/actions/go-dependency-submission">supporting action for Go</a>, which adds support for transitive dependency detection and gives a more accurate picture of your Go dependencies, and we‚Äôre working with the community to create actions that support additional package managers and tools. Keep an eye on our blog for more information on third-party integrations!</p>
<p>You can also build your own action or submit dependencies directly to the API. Submissions are associated with a commit SHA and need to be made in the <a href="https://docs.github.com/en/rest/dependency-graph/dependency-submission">required format</a>. The <a href="https://github.com/github/dependency-submission-toolkit/packages/1416023">Dependency Submission Toolkit</a> helps with the conversion and submission, and can be used to make a GitHub Action for your desired package manager. Building your own submission workflow gives you the flexibility to address any complexities within your repository setup, and with the same powerful end result: a more comprehensive dependency graph!</p>
<p><strong>Viewing submitted dependencies in your dependency graph</strong></p>
<p>The dependency graph will show the most recent set of dependencies submitted to your repository&#8217;s default branch. If your repository has more than one workflow submitting dependencies for different package managers, then the latest submissions for each one will be included in your dependency graph.</p>
<h3>Help us improve the dependency submission API</h3>
<p>The dependency submission API is an early beta. We are working hard to improve the experience for submitting, retrieving, and viewing dependencies and will be shipping new updates to improve the overall functionality. Improvements to come include viewing metadata for submitted dependencies and accessing historical submissions, as well as surfacing submitted dependencies in dependency graph scenarios, like dependency review and dependency insights.</p>
<p>As we improve, we are eager to hear your feedback to make it even better! Let us know how you use the dependency submission API and how you&#8217;d like it to evolve. Whether you&#8217;re using ready-made actions or building your own submission workflow, you can leave any and all feedback <a href="https://github.com/github-community/community/discussions/categories/code-security">using GitHub Discussions</a>.</p>
<h3>Start using the GitHub dependency submission API</h3>
<p>To learn more about using the dependency submission API, check out our <a href="https://docs.github.com/en/code-security/supply-chain-security/understanding-your-software-supply-chain/using-the-dependency-submission-api">documentation</a> to get started, or use the <a href="https://github.com/github/dependency-submission-toolkit/packages/1416023">Dependency Submission Toolkit</a> to write your own detector!</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://github.blog/2022-06-16-release-radar-may-2022/">Release Radar ¬∑ May 2022 Edition</a> <span>Each month, we highlight open source projects that have shipped major updates. These
                     projects can include everything from world-changing technology to developer tooling,
                     and weekend hobbies. We cover what the project is and some of their breaking changes.
                     Read about the project, and browse their repositories. Without further ado, here are
                     our top staff picks for projects that shipped major version releases this May.
                     Cura 5.0
                     Many 3D printing gurus would be familiar with Cura. It&amp;#8217;s currently being used
                     by millions of makers and is the most popular 3D printing software. Cura works with
                     heaps of different printers and allows users to design, slice, and print your 3D creations.
                     The newest update comes with a new slicing engine, which you&amp;#8217;ll love if you&amp;#8217;ve
                     ever had trouble slicing up your model. There&amp;#8217;s also support for the new Apple
                     M1 and more streamlined workflows. There&amp;#8217;s lots more fun updates and bug fixes
                     which you can read about in the Cura Changelog, or watch the launch video.
                     
                     			
                     		
                     Apollo 2.0
                     Who loves centralised systems? With Apollo, you can centrally manage your application
                     and cluster configurations. No more jumping between different management systems.
                     The latest update supports Java Runtime Environment 8, 11, and 17. The homepage has
                     received an update to make the experience easier to view and navigate. There&amp;#8217;s
                     lots more new features and the usual bug fixes. Check out all the changes in the Apollo
                     notes.
                     
                     Snipe-IT 6.0
                     You won&amp;#8217;t be able to stop the stream snippers, but you can say goodbye to spreadsheets
                     with Snipe-IT. No one likes to manage databases and assets using spreadsheets. Snipe-IT
                     allows you to manage and host your own asset management system. It&amp;#8217;s built on
                     Laravel 8 and is run from the web, so you don&amp;#8217;t need a powerful computer or
                     even an executable file. The latest version has been in the works for over a year
                     and requires users to have PHP 7.4 or higher. Snipe-IT 6.0 also supports PHP 8. There
                     are tonnes of additions and lots of improvements, including new mobile UI/UX, more
                     compatibility, and faster loading times. Read all the new features on the changelog.
                     
                     Ghost 5.0
                     Known as &amp;#8220;the a blog on steroids&amp;#8221;. Ghost is built for content creators
                     and publishers to help manage memberships, subscriptions, and newsletters. We featured
                     Ghost in our March 2021 Release Radar. The latest update to Ghost includes added premium
                     tiers, multiple newsletters, special offers, detailed audience segmenting, and even
                     more analytics. Check out all the changes on the Ghost blog. If you&amp;#8217;re into
                     Ubuntu and Node, check out Ghost since they are hiring Node.js engineers!
                     
                     We just shipped Ghost 5.0! 
                     What&amp;#39;s new?
                     Support for multiple newsletters Custom tiers &amp;amp; offers Detailed member analytics
                     New themes &amp;amp; design settings MUCH MORE
                     We&amp;#39;re live on Product Hunt here  say hihttps://t.co/LeBkaGO8Yk
                     &amp;mdash; Ghost (@Ghost) May 23, 2022
                     
                     FX 23.0
                     Short for Function eXecution, FX is a terminal JSON viewer where you can browse JSONs
                     via the terminal. You can use JavaScript, Python, or Ruby. FX also supports mouse
                     and streaming, and preserves your key orders and big numbers. FX was previously written
                     in Node.js, and now FX 23.0 has been rewritten in Go. There was also a lot of time
                     spent on improving the terminal viewer. Head over to the repository to install FX,
                     and check out all the cool colour themes!
                     
                     Enioka Scan 2.0
                     Do you need a lightweight way to scan barcodes in your Android applications? Look
                     no further than Enioka Scan. With Enioka Scan, users won&amp;#8217;t be locked into a
                     vendor, and the Android library is compatible with lots of devices. Check out the
                     list of compatible devices in the repository. Enioka Scan 2.0 has reworked APIs, providing
                     an even more seamless communication with the scanner service and individual scanners.
                     Interfaces are streamlined and the scanner handles searches much better, making it
                     run faster. Read all the changes in the release notes.
                     Release Radar May
                     Well, that‚Äôs all for this month‚Äôs top release picks. Congratulations to everyone who
                     shipped a new release, whether it was version 1.0 or version 23.0. Continue the awesome
                     coding work! If you missed our last Release Radar, read up on the amazing community
                     projects from April.
                     We hope these releases inspire you to get involved in open source or update your projects.
                     We love featuring projects submitted by the community. If you are working on an open
                     source project and shipping a major version soon, we&amp;#8217;d love to hear from you.
                     Check out our new Release Radar repository, and submit your project to be featured
                     in the GitHub Release Radar.
                     </span></summary><time datetime="2022-06-17T00:05:03+02:00">Thu, 16 Jun 2022 22:05</time><article><p>Each month, we highlight open source projects that have shipped major updates. These projects can include everything from world-changing technology to developer tooling, and weekend hobbies. We cover what the project is and some of their breaking changes. Read about the project, and browse their repositories. Without further ado, here are our top staff picks for projects that shipped major version releases this May.</p>
<h2>Cura 5.0</h2>
<p>Many 3D printing gurus would be familiar with <a href="https://ultimaker.com/software/ultimaker-cura" target="_blank" rel="noopener">Cura</a>. It&#8217;s currently being used by millions of makers and is the most popular 3D printing software. Cura works with heaps of different printers and allows users to design, slice, and print your 3D creations. The newest update comes with a new slicing engine, which you&#8217;ll love if you&#8217;ve ever had trouble slicing up your model. There&#8217;s also support for the new Apple M1 and more streamlined workflows. There&#8217;s lots more fun updates and bug fixes which you can read about in the <a href="https://github.com/Ultimaker/Cura/releases/tag/5.0.0">Cura Changelog</a>, or watch the launch video.</p>
<div class="mod-vh position-relative" style="height: 0; padding-bottom: calc((9 / 16)*100%);">
			<iframe loading="lazy" class="position-absolute top-0 left-0 width-full height-full" src="https://www.youtube.com/embed/kRj7pR4OkQA?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent" title="YouTube video player" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0"></iframe>
		</div>
<h2>Apollo 2.0</h2>
<p>Who loves centralised systems? With <a href="https://www.apolloconfig.com/#/" target="_blank" rel="noopener">Apollo</a>, you can centrally manage your application and cluster configurations. No more jumping between different management systems. The latest update supports Java Runtime Environment 8, 11, and 17. The homepage has received an update to make the experience easier to view and navigate. There&#8217;s lots more new features and the usual bug fixes. Check out all the changes in the <a href="https://github.com/apolloconfig/apollo/releases/tag/v2.0.0" target="_blank" rel="noopener">Apollo notes</a>.</p>
<p><img loading="lazy" class="alignnone size-full wp-image-65756 width-fit" src="https://github.blog/wp-content/uploads/2022/06/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f61706f6c6c6f636f6e6669672f61706f6c6c6f406d61737465722f646f63732f656e2f696d616765732f61706f6c6c6f2d686f6d652d73637265656e73686f742e6a7067.jpeg?resize=1024%2C537" alt="" width="1024" height="537" srcset="https://github.blog/wp-content/uploads/2022/06/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f61706f6c6c6f636f6e6669672f61706f6c6c6f406d61737465722f646f63732f656e2f696d616765732f61706f6c6c6f2d686f6d652d73637265656e73686f742e6a7067.jpeg?resize=1024%2C537?w=2600 2600w, https://github.blog/wp-content/uploads/2022/06/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f61706f6c6c6f636f6e6669672f61706f6c6c6f406d61737465722f646f63732f656e2f696d616765732f61706f6c6c6f2d686f6d652d73637265656e73686f742e6a7067.jpeg?resize=1024%2C537?w=300 300w, https://github.blog/wp-content/uploads/2022/06/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f61706f6c6c6f636f6e6669672f61706f6c6c6f406d61737465722f646f63732f656e2f696d616765732f61706f6c6c6f2d686f6d652d73637265656e73686f742e6a7067.jpeg?resize=1024%2C537?w=768 768w, https://github.blog/wp-content/uploads/2022/06/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f61706f6c6c6f636f6e6669672f61706f6c6c6f406d61737465722f646f63732f656e2f696d616765732f61706f6c6c6f2d686f6d652d73637265656e73686f742e6a7067.jpeg?resize=1024%2C537?w=1024 1024w, https://github.blog/wp-content/uploads/2022/06/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f61706f6c6c6f636f6e6669672f61706f6c6c6f406d61737465722f646f63732f656e2f696d616765732f61706f6c6c6f2d686f6d652d73637265656e73686f742e6a7067.jpeg?resize=1024%2C537?w=1536 1536w, https://github.blog/wp-content/uploads/2022/06/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f61706f6c6c6f636f6e6669672f61706f6c6c6f406d61737465722f646f63732f656e2f696d616765732f61706f6c6c6f2d686f6d652d73637265656e73686f742e6a7067.jpeg?resize=1024%2C537?w=2048 2048w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /></p>
<h2>Snipe-IT 6.0</h2>
<p>You won&#8217;t be able to stop the stream snippers, but you can say goodbye to spreadsheets with <a href="https://snipeitapp.com/" target="_blank" rel="noopener">Snipe-IT</a>. No one likes to manage databases and assets using spreadsheets. Snipe-IT allows you to manage and host your own asset management system. It&#8217;s built on Laravel 8 and is run from the web, so you don&#8217;t need a powerful computer or even an executable file. The latest version has been in the works for over a year and requires users to have PHP 7.4 or higher. Snipe-IT 6.0 also supports PHP 8. There are tonnes of additions and lots of improvements, including new mobile UI/UX, more compatibility, and faster loading times. Read all the new features on the <a href="https://github.com/snipe/snipe-it/releases/tag/v6.0.0" target="_blank" rel="noopener">changelog</a>.</p>
<p><img loading="lazy" class="alignnone size-full wp-image-65732 width-fit" src="https://github.blog/wp-content/uploads/2022/06/Snipe.gif?resize=1024%2C576" alt="" width="1024" height="576" data-recalc-dims="1" /></p>
<h2>Ghost 5.0</h2>
<p>Known as &#8220;the a blog on steroids&#8221;. <a href="https://github.com/TryGhost/Ghost" target="_blank" rel="noopener">Ghost</a> is built for content creators and publishers to help manage memberships, subscriptions, and newsletters. We featured Ghost in our <a href="https://github.blog/2021-04-09-release-radar-mar-2021/">March 2021 Release Radar</a>. The latest update to Ghost includes added premium tiers, multiple newsletters, special offers, detailed audience segmenting, and even more analytics. Check out all the changes on the <a href="https://ghost.org/changelog/5/">Ghost blog</a>. If you&#8217;re into Ubuntu and Node, check out Ghost since they are <a href="https://careers.ghost.org/" target="_blank" rel="noopener">hiring Node.js engineers</a>!</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr"><img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f4e3.png" alt="üì£" class="wp-smiley" style="height: 1em; max-height: 1em;" /> We just shipped Ghost 5.0! <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f4e3.png" alt="üì£" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>What&#39;s new?</p>
<p><img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f4f0.png" alt="üì∞" class="wp-smiley" style="height: 1em; max-height: 1em;" /> Support for multiple newsletters<br /><img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f4b0.png" alt="üí∞" class="wp-smiley" style="height: 1em; max-height: 1em;" /> Custom tiers &amp; offers<br /><img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f4ca.png" alt="üìä" class="wp-smiley" style="height: 1em; max-height: 1em;" /> Detailed member analytics<br /><img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f5bc.png" alt="üñº" class="wp-smiley" style="height: 1em; max-height: 1em;" /> New themes &amp; design settings<br /><img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f4a1.png" alt="üí°" class="wp-smiley" style="height: 1em; max-height: 1em;" /> MUCH MORE</p>
<p>We&#39;re live on Product Hunt here <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f44b.png" alt="üëã" class="wp-smiley" style="height: 1em; max-height: 1em;" /> say hi<a href="https://t.co/LeBkaGO8Yk">https://t.co/LeBkaGO8Yk</a></p>
<p>&mdash; Ghost (@Ghost) <a href="https://twitter.com/Ghost/status/1528711705915695106?ref_src=twsrc%5Etfw">May 23, 2022</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<h2>FX 23.0</h2>
<p>Short for Function eXecution, FX is a terminal JSON viewer where you can browse JSONs via the terminal. You can use JavaScript, Python, or Ruby. FX also supports mouse and streaming, and preserves your key orders and big numbers. FX was previously written in Node.js, and now FX 23.0 has been rewritten in Go. There was also a lot of time spent on improving the terminal viewer. Head <a href="https://github.com/antonmedv/fx" target="_blank" rel="noopener">over to the repository</a> to install FX, and check out all the cool colour themes!</p>
<p><img loading="lazy" class="width-fit alignnone wp-image-65733" src="https://github.blog/wp-content/uploads/2022/06/68747470733a2f2f6d6564762e696f2f6173736574732f66782f66782d707265766965772e676966.gif?resize=500%2C391" alt="" width="500" height="391" data-recalc-dims="1" /></p>
<h2>Enioka Scan 2.0</h2>
<p>Do you need a lightweight way to scan barcodes in your Android applications? Look no further than <a href="https://github.com/enioka/enioka_scan" target="_blank" rel="noopener">Enioka Scan</a>. With Enioka Scan, users won&#8217;t be locked into a vendor, and the Android library is compatible with lots of devices. Check out the list of compatible devices in the repository. Enioka Scan 2.0 has reworked APIs, providing an even more seamless communication with the scanner service and individual scanners. Interfaces are streamlined and the scanner handles searches much better, making it run faster. Read all the changes in the <a href="https://github.com/enioka/enioka_scan/releases/tag/2.0.0" target="_blank" rel="noopener">release notes</a>.</p>
<h2>Release Radar May</h2>
<p>Well, that‚Äôs all for this month‚Äôs top release picks. Congratulations to everyone who shipped a new release, whether it was version 1.0 or version 23.0. Continue the awesome coding work! If you missed our last Release Radar, read up on the amazing community projects from <a href="https://github.blog/2022-05-20-release-radar-apr-2022/" target="_blank" rel="noopener">April</a>.</p>
<p>We hope these releases inspire you to get involved in open source or update your projects. We love featuring projects submitted by the community. If you are working on an open source project and shipping a major version soon, we&#8217;d love to hear from you. Check out our new <a href="https://releaseradar.github.com/" target="_blank" rel="noopener">Release Radar repository</a>, and <a href="https://github.com/github/release-radar/issues/new?assignees=MishManners&amp;labels=&amp;template=release-radar-request.yml&amp;title=%5BRelease+Radar+Request%5D+%3Ctitle%3E" target="_blank" rel="noopener">submit your project to be featured in the GitHub Release Radar</a>.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://github.blog/2022-06-16-the-android-kernel-mitigations-obstacle-race/">The Android kernel mitigations obstacle race</a> <span>In this post, I‚Äôll exploit a use-after-free (UAF) bug, CVE-2022-22057 in the Qualcomm
                     GPU driver, which affected the kernel branch 5.4 or above, and is mostly used by flagship
                     models running the Snapdragon 888 chipset or above (for example, the Snapdragon version
                     of S21‚Äîused in the U.S. and a number of Asian countries, such as China and Korea‚Äî
                     and all versions of the Galaxy Z Flip3, and many others). The device tested here is
                     the Samsung Galaxy Z Flip3 and I was able to use this bug alone to gain arbitrary
                     kernel memory read and write, and from there, disable SELinux and run arbitrary commands
                     as root. The bug itself was publicly disclosed in the Qualcomm security bulletin in
                     May 2022 and the fix was applied to devices in the May 2022 Android security patch.
                     Why Android GPU drivers
                     While the bug itself is a fairly standard use-after-free bug that involves a tight
                     race condition in the GPU driver, and this post focuses mostly on bypassing the many
                     mitigations that are in place on the device rather on the GPU, it is, nevertheless,
                     worth giving some motivations as to why the Android GPU makes an attractive target
                     for attackers.
                     As was mentioned in the article ‚ÄúThe More You Know, The More You Know You Don‚Äôt Know‚Äù
                     by Maddie Stone, out of seven Android 0-days that were detected as exploited in the
                     wild in 2021, five of them targeted GPU drivers. As of the date of writing, another
                     bug that was exploited in the wild, CVE-2021-39793 disclosed in March 2022 also targeted
                     the GPU driver.
                     Apart from the fact that most Android devices use either the Qualcomm Adreno or the
                     ARM Mali GPU, making it possible to obtain universal coverage with relatively few
                     bugs (this was mentioned in Maddie Stone‚Äôs article), the GPU drivers are also reachable
                     from the untrusted app sandbox in all Android devices, further reducing the number
                     of bugs that are required in a full chain. Another reason GPU drivers are attractive
                     is that most GPU drivers also handle rather complex memory sharing logic between the
                     GPU device and the CPU. These often involve fairly elaborate memory management code
                     that is prone to bugs that can be abused to achieve arbitrary read and write of physical
                     memory or to bypass memory protection. As these bugs enable an attacker to abuse the
                     functionality of the GPU memory management code, many of them are also undetectable
                     as memory corruptions and immune to existing mitigations, which mostly aim at preventing
                     control flow hijacking. Some examples are the work of Guang Gong and Ben Hawkes, who
                     exploited logic errors in the handling of GPU opcode to gain arbitrary memory read
                     and write.
                     The vulnerability
                     The vulnerability was introduced in the 5.4 branch of the Qualcomm msm 5.4 kernel
                     when the new kgsl timeline feature, together with some new ioctl associated with it,
                     was introduced. The msm 5.4 kernel carried out some rather major refactoring of the
                     kernel graphics support layer (kgsl) driver (under drivers/gpu/msm, which is Qualcomm‚Äôs
                     GPU driver) and introduced some new features. Both these new features and refactoring
                     resulted in a number of regressions and new security issues, most of which were found
                     and fixed internally and then disclosed publicly as security issues in the bulletins
                     (kudos to Qualcomm for not silently patching security issues), including some that
                     look fairly exploitable.
                     The kgsl_timeline object can be created and destroyed via the ioctl IOCTL_KGSL_TIMELINE_CREATE
                     and IOCTL_KGSL_TIMELINE_DESTROY. The kgsl_timeline object stores a list of dma_fence
                     objects in the field fences. The ioctl IOCTL_KGSL_TIMELINE_FENCE_GET and IOCTL_KGSL_TIMELINE_WAIT
                     can be used to add dma_fence objects to this list. The dma_fence objects added are
                     refcounted objects and their refcounts are decreased using the standard dma_fence_put
                     method.
                     What is interesting about timeline-&amp;gt;fences is that it does not actually hold an
                     extra refcount to the fences. Instead, to avoid a dma_fence in timeline-&amp;gt;fences
                     from being freed, a customized release function, timeline_fence_release is used to
                     remove the dma_fence from timeline-&gt;fences before it gets freed.
                     When the refcount of a dma_fence stored in kgsl_timeline::fences is decreased to zero,
                     the method timeline_fence_release will be called to remove the dma_fence from kgsl_timeline::fences
                     so that it can no longer be referenced from the kgsl_timeline, and then dma_fence_free
                     is called to free the object itself:
                     static void timeline_fence_release(struct dma_fence *fence)
                     {
                     ...
                     spin_lock_irqsave(&amp;amp;timeline-&amp;gt;fence_lock, flags);
                     
                     /* If the fence is still on the active list, remove it */
                     list_for_each_entry_safe(cur, temp, &amp;amp;timeline-&amp;gt;fences, node) {
                     if (f != cur)
                     continue;
                     
                     list_del_init(&amp;amp;f-&amp;gt;node);    //&amp;lt;----- 1. Remove fence
                     break;
                     }
                     spin_unlock_irqrestore(&amp;amp;timeline-&amp;gt;fence_lock, flags);
                     ...
                     kgsl_timeline_put(f-&amp;gt;timeline);
                     dma_fence_free(fence);     //&amp;lt;-------    2.  frees the fence
                     }
                     
                     Although the removal of fence from timeline-&amp;gt;fences is correctly protected by the
                     timeline-&amp;gt;fence_lock, IOCTL_KGSL_TIMELINE_DESTROY makes it possible to acquire
                     a reference to a dma_fence in fences after its refcount has reached zero but before
                     it gets removed from fences in timeline_fence_release:
                     long kgsl_ioctl_timeline_destroy(struct kgsl_device_private *dev_priv,
                     unsigned int cmd, void *data)
                     {
                     ...
                     spin_lock(&amp;amp;timeline-&amp;gt;fence_lock);  //&amp;lt;------------- a.
                     list_for_each_entry_safe(fence, tmp, &amp;amp;timeline-&amp;gt;fences, node)
                     dma_fence_get(&amp;amp;fence-&amp;gt;base);
                     list_replace_init(&amp;amp;timeline-&amp;gt;fences, &amp;amp;temp);
                     spin_unlock(&amp;amp;timeline-&amp;gt;fence_lock);
                     
                     
                     spin_lock_irq(&amp;amp;timeline-&amp;gt;lock);
                     list_for_each_entry_safe(fence, tmp, &amp;amp;temp, node) { //&amp;lt;----- b.
                     dma_fence_set_error(&amp;amp;fence-&amp;gt;base, -ENOENT);
                     dma_fence_signal_locked(&amp;amp;fence-&amp;gt;base);
                     dma_fence_put(&amp;amp;fence-&amp;gt;base);
                     }
                     spin_unlock_irq(&amp;amp;timeline-&amp;gt;lock);
                     ...
                     }
                     
                     In kgsl_ioctl_timeline_destroy, when destroying the timeline, the fences in timeline-&amp;gt;fences
                     are first copied to another list, temp and then removed from timeline-&amp;gt;fences (point
                     a.). As timeline-&amp;gt;fences does not hold an extra reference of the fence, refcount
                     is increased to stop them from being free‚Äôd in temp. Again, the manipulation of timeline-&amp;gt;fences
                     is protected by timeline-&amp;gt;fence_lock here. However, if the refcount of a fence
                     is already zero when a in the above is reached, but timeline_fence_release has not
                     yet been able to remove it from timeline-&amp;gt;fences, (it has not reached point 1.
                     In the snippet included in timeline_fence_release), then the dma_fence would be moved
                     to temp, and although its reference is increased, it is already too late, because
                     timeline_fence_release will free the dma_fence when it reaches point 2., regardless
                     of the refcount. So if the events happens in the following order, then a use-after-free
                     could be triggered at point b.:
                     
                     In the above, the red blocks indicate code that are holding the same lock, meaning
                     that the execution of these blocks are mutually exclusive. While the order of events
                     may look rather contrived (as it always is when you try to illustrate a race condition),
                     the actual timing is not too hard to achieve. As the code in timeline_fence_release
                     that removes a dma_fence from timeline-&amp;gt;fences cannot run while the code in kgsl_ioctl_timeline_destroy
                     is accessing timeline-&amp;gt;fence (both are holding timeline-&amp;gt;fence_lock), by adding
                     a large number of dma_fence to timeline-&amp;gt;fence, I can increase the time required
                     to run the red code block in kgsl_ioctl_timeline_destroy. If I decrease the refcount
                     of the last dma_fence in timeline-&amp;gt;fences in thread two to zero while the red code
                     block in thread one is running, I can trigger timeline_fence_release before dma_fence_get
                     increases the refcount of this dma_fence in thread one. As the red code block in thread
                     two also needs to acquire the timeline-&amp;gt;fence_lock, it can not remove the dma_fence
                     from timeline-&amp;gt;fences until after the red code block in thread one finished. By
                     that time, all the dma_fence in timeline-&amp;gt;fences have been moved to the list temp.
                     This also means that by the time the red code block in thread two runs, timeline-&amp;gt;fences
                     is an empty list and the loop finishes quickly and proceeds to dma_fence_free. In
                     short, as long as I add a large enough number of dma_fences to timeline-&amp;gt;fences,
                     I can create a large race window when kgsl_ioctl_timeline_destroy is moving the dma_fences
                     in timeline-&amp;gt;fences to temp. As long as I reduce the last refcount of the last
                     dma_fence in timeline-&amp;gt;fences within this window, I‚Äôm able to trigger the UAF bug.
                     Mitigations
                     While triggering the bug is not too difficult, exploiting it, on the other hand, is
                     a completely different matter. The device that I used for testing this bug and for
                     developing the exploit is a Samsung Galaxy Z Flip3. The latest Samsung devices running
                     kernel version 5.x probably have the most mitigations in place, even more so than
                     the Google Pixels. While older devices running kernel 4.x often have mitigations such
                     as the kCFI (Kernel Control Flow Integrity) and variable initialization switched off,
                     all those features are switched on in the 5.x kernel branch, and on top of that, there
                     is also the Samsung RKP (Realtime Kernel Protection) that protects various memory
                     area, such as kernel code and process credentials, making it difficult to execute
                     arbitrary code even when arbitrary memory read and write is achieved. In this section,
                     I‚Äôll briefly explain how those mitigations affect the exploit.
                     kCFI
                     The kCFI is arguably the mitigation that takes the most effort to bypass, especially
                     when used in conjunction with the Samsung hypervisor which protects many important
                     memory areas in the kernel. The kCFI prevents hijacking of control flow by limiting
                     the locations where a dynamic callsite can jump to using function signatures. For
                     example, in the current vulnerability, after the dma_fence is freed, the function
                     dma_fence_signal_locked is called:
                     long kgsl_ioctl_timeline_destroy(struct kgsl_device_private *dev_priv,
                     unsigned int cmd, void *data)
                     {
                     ...
                     spin_lock_irq(&amp;amp;timeline-&amp;gt;lock);
                     list_for_each_entry_safe(fence, tmp, &amp;amp;temp, node) {
                     dma_fence_set_error(&amp;amp;fence-&amp;gt;base, -ENOENT);
                     dma_fence_signal_locked(&amp;amp;fence-&amp;gt;base);       //&amp;lt;---- free'd fence is used
                     dma_fence_put(&amp;amp;fence-&amp;gt;base);
                     }
                     spin_unlock_irq(&amp;amp;timeline-&amp;gt;lock);
                     ...
                     }
                     
                     The function dma_fence_signal_locked then invokes a function cur-&amp;gt;func that is
                     an element inside the fence-&amp;gt;cb_list list.
                     int dma_fence_signal_locked(struct dma_fence *fence)
                     {
                     ...
                     list_for_each_entry_safe(cur, tmp, &amp;amp;cb_list, node) {
                     INIT_LIST_HEAD(&amp;amp;cur-&amp;gt;node);
                     cur-&amp;gt;func(fence, cur);
                     }
                     ...
                     }
                     
                     Without kCFI, the now free‚Äôd fence object can be replaced with a fake object, meaning
                     that cb_list and its elements, hence func, can all be faked, giving a ready to use
                     primitive to call an arbitrary function with both its first and second arguments pointing
                     to controlled data (fence and cur can both be faked). The exploit would have been
                     very easy once KASLR was defeated (for example, with a separate bug to leak kernel
                     addresses like in this exploit). However, because of kCFI, func can now only be replaced
                     by functions that have the type dma_fence_func_t, which greatly limits the use of
                     this primitive.
                     While in the past, I‚Äôve written about how easy it is to bypass Samsung‚Äôs control flow
                     integrity checks (JOPP, jump-oriented programming prevention), there is no easy way
                     round kCFI. One common way to bypass kCFI is to use a double free to hijack the freelist
                     and then apply the Kernel Space Mirroring Attack (KSMA). This was used a number of
                     times, for example, in Three dark clouds over the Android kernel of Jun Yao, Typhoon
                     Mangkhut: One-click remote universal root formed with two vulnerabilities of Hongli
                     Han, Rong Jian, Xiaodong Wang and Peng Zhou.
                     While the current bug also gives me a double free primitive when dma_fence_put is
                     called after fence is freed:
                     long kgsl_ioctl_timeline_destroy(struct kgsl_device_private *dev_priv,
                     unsigned int cmd, void *data)
                     {
                     ...
                     spin_lock_irq(&amp;amp;timeline-&amp;gt;lock);
                     list_for_each_entry_safe(fence, tmp, &amp;amp;temp, node) {
                     dma_fence_set_error(&amp;amp;fence-&amp;gt;base, -ENOENT);
                     dma_fence_signal_locked(&amp;amp;fence-&amp;gt;base); 
                     dma_fence_put(&amp;amp;fence-&amp;gt;base);       //&amp;lt;----- free'd fence can be freed again
                     }
                     spin_unlock_irq(&amp;amp;timeline-&amp;gt;lock);
                     ...
                     }
                     
                     The above decreases the refcount of the fake fence object, which I can control to
                     make it one, so that the fake fence gets freed again. This, however, does not allow
                     me to apply KSMA as it would require overwriting of the swapper_pg_dir data structure,
                     which is protected by the Samsung hypervisor.
                     Variable initialization
                     From Android 11 onwards, the kernel can enable automatic variable initialization by
                     enabling various kernel build flags. The following, for example, is taken from the
                     build configuration of the Z Flip3:
                     # Memory initialization
                     #
                     CONFIG_CC_HAS_AUTO_VAR_INIT_PATTERN=y
                     CONFIG_CC_HAS_AUTO_VAR_INIT_ZERO=y
                     # CONFIG_INIT_STACK_NONE is not set
                     # CONFIG_INIT_STACK_ALL_PATTERN is not set
                     CONFIG_INIT_STACK_ALL_ZERO=y
                     CONFIG_INIT_ON_ALLOC_DEFAULT_ON=y
                     # CONFIG_INIT_ON_FREE_DEFAULT_ON is not set
                     # end of Memory initialization
                     
                     While the feature is available since Android 11, many devices running kernel branch
                     4.x do not have these enabled. On the other hand, devices running kernel 5.x seem
                     to have these enabled. Apart from the obvious uninitialized variables vulnerabilities
                     that this feature prevents, it also makes it harder for object replacement. In particular,
                     it is no longer possible to perform partial object replacement, in which only the
                     first bytes of the object are replaced, while the rest of the object remains valid.
                     So for example, the type of heap spray technique under the section, &amp;#8220;Spraying
                     the heap&amp;#8221; in ‚ÄúMitigations are attack surface, too‚Äù by Jann Horn is no longer
                     possible with automatic variable initialization. In the context of the current bug,
                     this mitigation limits the heap spray options that I have, and I‚Äôll explain more as
                     we go through the exploit.
                     kfree_rcu
                     This isn‚Äôt a security mitigation at all, but it is nevertheless interesting to mention
                     it here, because it has a similar effect to some UAF mitigations that had been proposed.
                     The UAF fence object in this bug is freed when dma_fence_free is called, which, instead
                     of the normal kfree, uses kfree_rcu. In short, kfree_rcu does not free an object immediately,
                     but rather schedules it to be freed when certain criteria are met. This acts somewhat
                     like a delayed free that introduces an uncertainty in the time when the object is
                     freed. Interestingly, this effect is quite similar to the UAF mitigation that is used
                     in the Scudo allocator (default allocator of Android user space processes), which
                     quarantines the free‚Äôd objects before actually freeing them to introduce uncertainty.
                     A similar proposal has been suggested for the linux kernel (but was rejected later).
                     Apart from introducing uncertainty in the object replacement, a delayed free may also
                     cause problems for UAF with tight race windows. So, on the face of it, the use of
                     kfree_rcu would be rather problematic for exploiting the current bug. However, with
                     many primitives to manipulate the size of a race window, such as the ones detailed
                     in Racing against the clock‚Äîhitting a tiny kernel race window and an older technique
                     in Exploiting race conditions on [ancient] Linux, (both by Jann Horn, the older technique
                     is used for exploiting the current bug) any tight race window can be made large enough
                     to allow for the delay caused by kfree_rcu, and the subsequent object replacement.
                     As for the uncertainty, that does not seem to cause a very big problem either. In
                     exploiting this bug, I actually had to perform object replacement with kfree_rcu twice,
                     the second time without even knowing on which CPU core the free is going to happen,
                     and yet even with this and all the other moving parts, a rather unoptimized exploit
                     still runs at a reasonable reliability (~70%) on the device tested. While I believe
                     that the second object replacement with kfree_rcu (where the CPU that frees the object
                     is uncertain) is probably the main source of unreliability, I‚Äôd attribute that reliability
                     loss more to the lack of CPU knowledge rather than to the delayed free. In my opinion,
                     a delayed free may not be a very effective UAF mitigation when there are primitives
                     that allow the scheduler to be manipulated.
                     Samsung RKP (Realtime Kernel Protection)
                     The Samsung RKP protects various parts of the memory from being written to. This prevents
                     processes from overwriting their own credentials to become root, as well as protecting
                     SELinux settings from being overwritten. It also prevents kernel code regions and
                     other important objects, such as kernel page tables, from being overwritten. In practice,
                     though, once arbitrary kernel memory read and write (subject to RKP restrictions)
                     is achieved, there are ways to bypass these restrictions. For example, SELinux rules
                     can be modified by overwriting the avc cache (see, for example, this exploit by Valentina
                     Palmiotti), while gaining root can be done by hijacking other processes that run as
                     root. In the context of the current bug, the Samsung RKP mostly works with kCFI to
                     prevent arbitrary functions from being called.
                     In this post, I‚Äôll exploit the bug with all these mitigations enabled.
                     Exploiting the bug
                     I‚Äôll now start going through the exploit of the bug. It is a fairly typical use-after-free
                     bug that involves a race condition and perhaps reasonably strong primitives with both
                     the possibility of arbitrary function call and double free, which is not that uncommon.
                     Apart from that, this is a typical bug, just like many other UAF found in the kernel.
                     So, it seems fitting to use this bug to gauge how these mitigations affect the development
                     of a standard UAF exploit.
                     Adding dma_fence to timeline-&amp;gt;fences
                     In the section, &amp;#8220;The vulnerability,&amp;#8221; I explained that the bug relies on
                     having dma_fence objects added to the fences list in a kgsl_timeline object, which
                     then have their refcount decreased to zero while the kgsl_timeline is being destroyed.
                     There are two options to add dma_fence objects to a kgsl_timeline, the first is to
                     use IOCTL_KGSL_TIMELINE_FENCE_GET:
                     long kgsl_ioctl_timeline_fence_get(struct kgsl_device_private *dev_priv,
                     unsigned int cmd, void *data)
                     {
                     ...
                     timeline = kgsl_timeline_by_id(device, param-&amp;gt;timeline);
                     ...
                     fence = kgsl_timeline_fence_alloc(timeline, param-&amp;gt;seqno); //&amp;lt;----- dma_fence
                     created and added to timeline
                     ...
                     sync_file = sync_file_create(fence);
                     if (sync_file) {
                     fd_install(fd, sync_file-&amp;gt;file);
                     param-&amp;gt;handle = fd;
                     }
                     ...
                     }
                     
                     This will create a dma_fence with kgsl_timeline_fence_alloc and add it to the timeline.
                     The caller then gets a file descriptor for a sync_file that corresponds to the dma_fence.
                     When the sync_file is closed, the refcount of dma_fence is decreased to zero.
                     The second option is to use IOCTL_KGSL_TIMELINE_WAIT:
                     long kgsl_ioctl_timeline_wait(struct kgsl_device_private *dev_priv,
                     unsigned int cmd, void *data)
                     {
                     ...
                     fence = kgsl_timelines_to_fence_array(device, param-&amp;gt;timelines,
                     param-&amp;gt;count, param-&amp;gt;timelines_size,
                     (param-&amp;gt;flags == KGSL_TIMELINE_WAIT_ANY));     //&amp;lt;------ dma_fence created and
                     added to timeline
                     ...
                     if (!timeout)
                     ret = dma_fence_is_signaled(fence) ? 0 : -EBUSY;
                     else {
                     ret = dma_fence_wait_timeout(fence, true, timeout);   //&amp;lt;----- 1.
                     ...
                     }
                     
                     dma_fence_put(fence);
                     ...
                     }
                     
                     This will create dma_fence objects using kgsl_timelines_to_fence_array and add them
                     to the timeline. If a timeout value is specified, then the call will enter dma_fence_wait_timeout
                     (path labeled 1), which will wait until either the timeout expires or when the thread
                     receives an interrupt. After dma_fence_wait_timeout finishes, dma_fence_put is called
                     to reduce the refcount of the dma_fence to zero. So, by specifying a large timeout,
                     dma_fence_wait_timeout will block until it receives an interrupt, which will then
                     free the dma_fence that was added to the timeline.
                     While IOCTL_KGSL_TIMELINE_FENCE_GET may seem easier to use and control at first glance,
                     in practice, the overhead incurred by closing the sync_file makes the timing for destruction
                     of the dma_fence less reliable. So, for the exploit, I use IOCTL_KGSL_TIMELINE_FENCE_GET
                     to create and add persistent dma_fence objects to fill the timeline-&amp;gt;fences list
                     to enlarge the race window, while the last dma_fence object that is used for the UAF
                     bug is added using IOCTL_KGSL_TIMELINE_WAIT and which gets freed when I send an interrupt
                     signal to the thread that calls IOCTL_KGSL_TIMELINE_WAIT.
                     Widening the tiny race window
                     To recap, in order to exploit the vulnerability, I need to remove the refcount of
                     a dma_fence in the fences list of a kgsl_timeline within the first race window labeled
                     in the following code block:
                     long kgsl_ioctl_timeline_destroy(struct kgsl_device_private *dev_priv,
                     unsigned int cmd, void *data)
                     {
                     //BEGIN OF FIRST RACE WINDOW
                     spin_lock(&amp;amp;timeline-&amp;gt;fence_lock);
                     list_for_each_entry_safe(fence, tmp, &amp;amp;timeline-&amp;gt;fences, node)
                     dma_fence_get(&amp;amp;fence-&amp;gt;base);
                     list_replace_init(&amp;amp;timeline-&amp;gt;fences, &amp;amp;temp);
                     spin_unlock(&amp;amp;timeline-&amp;gt;fence_lock);
                     //END OF FIRST RACE WINDOW
                     //BEGIN OF SECOND RACE WINDOW
                     spin_lock_irq(&amp;amp;timeline-&amp;gt;lock);
                     list_for_each_entry_safe(fence, tmp, &amp;amp;temp, node) {
                     dma_fence_set_error(&amp;amp;fence-&amp;gt;base, -ENOENT);
                     dma_fence_signal_locked(&amp;amp;fence-&amp;gt;base);
                     dma_fence_put(&amp;amp;fence-&amp;gt;base);
                     }
                     spin_unlock_irq(&amp;amp;timeline-&amp;gt;lock);
                     //END OF SECOND RACE WINDOW
                     ...
                     }
                     
                     As explained before, the first race window can be enlarged by adding a large number
                     of dma_fence objects to timeline-&amp;gt;fences, which makes it easy to trigger the decrease
                     of refcount within this window. However, to exploit the bug, the following code, as
                     well as the object replacement, must be completed before the end of the second race
                     window:
                     spin_lock_irqsave(&amp;amp;timeline-&amp;gt;fence_lock, flags);
                     list_for_each_entry_safe(cur, temp, &amp;amp;timeline-&amp;gt;fences, node) {
                     if (f != cur)
                     continue;
                     list_del_init(&amp;amp;f-&amp;gt;node);
                     break;
                     }
                     spin_unlock_irqrestore(&amp;amp;timeline-&amp;gt;fence_lock, flags);
                     trace_kgsl_timeline_fence_release(f-&amp;gt;timeline-&amp;gt;id, fence-&amp;gt;seqno);
                     kgsl_timeline_put(f-&amp;gt;timeline);
                     dma_fence_free(fence);
                     
                     As explained before, because of the spin_lock, the above cannot start until the first
                     race window ends, but by the time this code is run, timeline-&amp;gt;fences has been emptied,
                     so the loop will be quick to run. However, since dma_fence_free uses kfree_rcu, the
                     actual freeing of fence is delayed. This makes it impossible to replace the free‚Äôd
                     fence before the second race window finishes, unless we manipulate the scheduler.
                     To do so, I‚Äôll use a technique in ‚ÄúExploiting race conditions on [ancient] Linux‚Äù
                     that I also used in another Android exploit to widen this race window.
                     I‚Äôll recap the essence of the technique here for readers who are not familiar with
                     it.
                     To ensure that each task (thread or process) has a fair share of the CPU time, the
                     linux kernel scheduler can interrupt a running task and put it on hold, so that another
                     task can be run. This kind of interruption and stopping of a task is called preemption
                     (where the interrupted task is preempted). A task can also put itself on hold to allow
                     another task to run, such as when it is waiting for some I/O input, or when it calls
                     sched_yield(). In this case, we say that the task is voluntarily preempted. Preemption
                     can happen inside syscalls such as ioctl calls as well, and on Android, tasks can
                     be preempted except in some critical regions (e.g. holding a spinlock). This behavior
                     can be manipulated by using CPU affinity and task priorities.
                     By default, a task is run with the priority SCHED_NORMAL, but a lower priority SCHED_IDLE
                     can also be set using the sched_setscheduler call (or pthread_setschedparam for threads).
                     Furthermore, it can also be pinned to a CPU with sched_setaffinity, which would only
                     allow it to run on a specific CPU. By pinning two tasks, one with SCHED_NORMAL priority
                     and the other with SCHED_IDLE priority to the same CPU, it is possible to control
                     the timing of the preemption as follows.
                     
                     First have the SCHED_NORMAL task perform a syscall that would cause it to pause and
                     wait. For example, it can read from a pipe with no data coming in from the other end,
                     then it would wait for more data and voluntarily preempt itself, so that the SCHED_IDLE
                     task can run.
                     As the SCHED_IDLE task is running, send some data to the pipe that the SCHED_NORMAL
                     task had been waiting on. This will wake up the SCHED_NORMAL task and cause it to
                     preempt the SCHED_IDLE task, and because of the task priority, the SCHED_IDLE task
                     will be preempted and put on hold.
                     The SCHED_NORMAL task can then run a busy loop to keep the SCHED_IDLE task from waking
                     up.
                     
                     In our case, the object replacement sequence goes as follows:
                     
                     Run IOCTL_KGSL_TIMELINE_WAIT on a thread to add dma_fence objects to a kgsl_timeline.
                     Set the timeout to a large value and use sched_setaffinity to pin this task to a CPU,
                     call it SPRAY_CPU. Once the dma_fence object is added, the task will then become idle
                     until it receives an interrupt.
                     Set up a SCHED_NORMAL task and pin it to another CPU (DESTROY_CPU) that listens to
                     an empty pipe. This will cause this task to become idle initially and allow DESTROY_CPU
                     to run a lower priority task. Once the empty pipe receives some data, this task then
                     will run a busy loop.
                     Set up a SCHED_IDLE task on DESTROY_CPU which will run IOCTL_KGSL_TIMELINE_DESTROY
                     to destroy the timeline where the dma_fence is added in step one. As the task set
                     up in step two is waiting for a response to an empty pipe, DESTROY_CPU will run this
                     task first.
                     Send an interrupt to the task running IOCTL_KGSL_TIMELINE_WAIT. The task will then
                     unblock and free the dma_fence while IOCTL_KGSL_TIMELINE_DESTROY is running within
                     the first race window.
                     Write to the empty pipe that the SCHED_NORMAL task is listening to. This will cause
                     the SCHED_NORMAL task to preempt the SCHED_IDLE task. Once it has successfully preempted
                     the task, DESTROY_CPU will run the busy loop, causing the SCHED_IDLE task to be put
                     on hold.
                     As the SCHED_IDLE task running IOCTL_KGSL_TIMELINE_DESTROY is put on hold, there is
                     now enough time to overcome the delay introduced by kfree_rcu and allow the dma_fence
                     in step four to be freed and replaced. After that, I can resume IOCTL_KGSL_TIMELINE_DESTROY
                     so that the subsequent operations will be performed on the now free‚Äôd and replaced
                     dma_fence object.
                     
                     One caveat here is that, because preemption cannot happen while a thread is holding
                     a spinlock, so IOCTL_KGSL_TIMELINE_DESTROY can only be preempted during the window
                     between spinlocks (marked by the comment below):
                     long kgsl_ioctl_timeline_destroy(struct kgsl_device_private *dev_priv,
                     unsigned int cmd, void *data)
                     {
                     spin_lock(&amp;amp;timeline-&amp;gt;fence_lock);
                     list_for_each_entry_safe(fence, tmp, &amp;amp;timeline-&amp;gt;fences, node)
                     ...
                     spin_unlock(&amp;amp;timeline-&amp;gt;fence_lock);
                     //Preemption window
                     spin_lock_irq(&amp;amp;timeline-&amp;gt;lock);
                     list_for_each_entry_safe(fence, tmp, &amp;amp;temp, node) {
                     ...
                     }
                     spin_unlock_irq(&amp;amp;timeline-&amp;gt;lock);
                     ...
                     }
                     
                     Although the preemption window in the above appears to be very small, in practice,
                     as long as the SCHED_NORMAL task tries to preempt the SCHED_IDLE task running IOCTL_KGSL_TIMELINE_DESTROY
                     while the first spinlock is held, preemption will happen as soon as the spinlock is
                     released, making it much easier to succeed in preempting IOCTL_KGSL_TIMELINE_DESTROY
                     at the right time.
                     The following figure illustrates what happens in an ideal world, with red blocks indicating
                     regions that hold a spinlock and are therefore not possible to preempt, and dotted
                     lines indicating tasks that are idle.
                     
                     The following figure illustrates what happens in the real world:
                     
                     For object replacement, I&amp;#8217;ll use sendmsg, which is a standard way to replace
                     free&amp;#8217;d objects in the linux kernel with controlled data. As the method is fairly
                     standard, I won‚Äôt give the details here, but refer readers to the link above. From
                     now on, I&amp;#8217;ll assume that the free&amp;#8217;d dma_fence object is replaced by arbitrary
                     data. (There are some restrictions in the first 12 bytes using this method, but that
                     does not affect our exploit.)
                     Assuming that the free&amp;#8217;d dma_fence object can be replaced with arbitrary data,
                     let&amp;#8217;s take a look at how this fake object is used. After the dma_fence is replaced,
                     it is then used in kgsl_ioctl_timeline_destroy as follows:
                     spin_lock_irq(&amp;amp;timeline-&amp;gt;lock);
                     list_for_each_entry_safe(fence, tmp, &amp;amp;temp, node) {
                     dma_fence_set_error(&amp;amp;fence-&amp;gt;base, -ENOENT);
                     dma_fence_signal_locked(&amp;amp;fence-&amp;gt;base);
                     dma_fence_put(&amp;amp;fence-&amp;gt;base);
                     }
                     spin_unlock_irq(&amp;amp;timeline-&amp;gt;lock);
                     
                     Three different functions, dma_fence_set_error, dma_fence_signal_locked and dma_fence_put
                     will be called with the argument fence. The function dma_fence_set_error will write
                     an error code to the fence object, which may be useful with a suitable object replacement,
                     but not for sendmsg object replacements and I&amp;#8217;ll not be investigating this possibility
                     here. The function dma_fence_signal_locked does the following:
                     int dma_fence_signal_locked(struct dma_fence *fence)
                     {
                     ...
                     if (unlikely(test_and_set_bit(DMA_FENCE_FLAG_SIGNALED_BIT,   //&amp;lt;-- 1.
                     &amp;amp;fence-&amp;gt;flags)))
                     return -EINVAL;
                     
                     /* Stash the cb_list before replacing it with the timestamp */
                     list_replace(&amp;amp;fence-&amp;gt;cb_list, &amp;amp;cb_list);                    //&amp;lt;-- 2.
                     ...
                     list_for_each_entry_safe(cur, tmp, &amp;amp;cb_list, node) {        //&amp;lt;-- 3.
                     INIT_LIST_HEAD(&amp;amp;cur-&amp;gt;node);
                     cur-&amp;gt;func(fence, cur);
                     }
                     
                     return 0;
                     }
                     
                     It first checks fence-&amp;gt;flags (1. in the above): if the DMA_FENCE_FLAG_SIGNALED_BIT
                     flag is set, then the fence has been signaled, and the function exits early. If the
                     fence has not been signaled, then list_replace is called to remove objects in fence-&amp;gt;cb_list
                     and place them in a temporary cb_list (2. in the above). After that, functions stored
                     in cb_list are called (3. above). As explained in the section, &amp;#8220;kCFI&amp;#8221;
                     because of the CFI mitigation, this will only allow me to call functions of a certain
                     type; besides, at this stage I have no knowledge of function addresses, so I&amp;#8217;m
                     most likely just going to crash the kernel if I reach this path. So, at this stage,
                     I have little choice but to set the DMA_FENCE_FLAG_SIGNALED_BITflag in my fake object
                     so that dma_fence_signal_locked exits early.
                     This leaves me the dma_fence_put function, which decreases the refcount of fence and
                     calls dma_fence_release if the refcount reaches zero:
                     void dma_fence_release(struct kref *kref)
                     {
                     ...
                     if (fence-&amp;gt;ops-&amp;gt;release)
                     fence-&amp;gt;ops-&amp;gt;release(fence);
                     else
                     dma_fence_free(fence);
                     }
                     
                     If dma_fence_release is called, then eventually it&amp;#8217;ll check the fence-&amp;gt;ops
                     and call fence-&amp;gt;ops-&amp;gt;release. This gives me two problems: First, fence-&amp;gt;ops
                     needs to point to valid memory, otherwise the dereference will fail, and even if the
                     dereference succeeds, fence-&amp;gt;ops-&amp;gt;release either needs to be zero, or it has
                     to be the address of a function of an appropriate type.
                     All these present me with two choices. I can either follow the standard path: try
                     to replace the fence object with another object or try to make use of the limited
                     write primitives that dma_fence_put and dma_fence_set_error offer me, while hoping
                     that I can still control the flags and refcount fields to avoid dma_fence_signal_locked
                     or dma_fence_release crashing the kernel.
                     Or, I can try something else.
                     The ultimate fake object store
                     While exploiting another bug, I came across the Software Input Output Translation
                     Lookaside Buffer (SWIOTLB), which is a memory region that is allocated at a very early
                     stage during boot time. As such, the physical address of the SWIOTLB is very much
                     fixed and depends only on the hardware configuration. Moreover, as this memory is
                     in the &amp;#8220;low memory&amp;#8221; region (Android devices do not seem to have a &amp;#8220;high
                     memory&amp;#8221; region) and not in the kernel image, the virtual address is simply the
                     physical address with a fixed offset (readers who are interested in the details can,
                     for example, follow the implementation of the kmap function):
                     #define __virt_to_phys_nodebug(x) ({                   \
                     phys_addr_t __x = (phys_addr_t)(__tag_reset(x));        \
                     __is_lm_address(__x) ? __lm_to_phys(__x) : __kimg_to_phys(__x); \
                     })
                     #define __is_lm_address(addr)  (!(((u64)addr) &amp;amp; BIT(vabits_actual - 1)))
                     
                     #define __lm_to_phys(addr) (((addr) + physvirt_offset))
                     
                     The above definitions are from arch/arm64/include/asm/memory.h, which is the relevant
                     implementation for Android. The variable physvirt_offset used for translating the
                     address is a fixed constant set in arm64_memblock_init:
                     void __init arm64_memblock_init(void)
                     {...
                     memstart_addr = round_down(memblock_start_of_DRAM(),
                     ARM64_MEMSTART_ALIGN);
                     physvirt_offset = PHYS_OFFSET - PAGE_OFFSET;
                     
                     ...
                     }
                     
                     On top of that, the memory in the SWIOTLB can be accessed via the adsp driver that
                     is reachable from an untrusted app, so this seems to be a good place to store fake
                     objects and redirect fake pointers to. However, in the 5.x version of the kernel,
                     the SWIOTLB is only allocated when the kernel is compiled with the CONFIG_DMA_ZONE32
                     flag, which is not the case for our device.
                     There is, however, something better. The fact that the early allocation of SWIOTLB
                     gives it a predictable address prompted me to inspect the boot log to see if there
                     are other regions of memory that are allocated early during the boot, and it turns
                     out that there are indeed other memory regions that are allocated very early during
                     the boot.
                     &amp;lt;6&amp;gt;[    0.000000] [0:        swapper:    0]  Reserved memory: created CMA memory
                     pool at 0x00000000f2800000, size 212 MiB
                     &amp;lt;6&amp;gt;[    0.000000] [0:        swapper:    0]  OF: reserved mem: initialized node
                     secure_display_region, compatible id shared-dma-pool
                     ...
                     &amp;lt;6&amp;gt;[    0.000000] [0:        swapper:    0]  OF: reserved mem: initialized node
                     user_contig_region, compatible id shared-dma-pool
                     &amp;lt;6&amp;gt;[    0.000000] [0:        swapper:    0]  Reserved memory: created CMA memory
                     pool at 0x00000000f0c00000, size 12 MiB
                     
                     &amp;lt;6&amp;gt;[    0.578613] [7:      swapper/0:    1]  platform soc:qcom,ion:qcom,ion-heap@22:
                     assigned reserved memory node sdsp_region
                     ...
                     &amp;lt;6&amp;gt;[    0.578829] [7:      swapper/0:    1]  platform soc:qcom,ion:qcom,ion-heap@26:
                     assigned reserved memory node user_contig_region
                     ...
                     
                     The Reserved memory regions in the above seem to be the memory pools that are used
                     for allocating ion buffers.
                     On Android, the ion_allocator is used to allocate memory regions used for DMA (direct
                     memory access) that allows kernel drivers and userspace processes to share the same
                     underlying memory. The ion allocator is accessible by an untrusted app via the /dev/ion
                     file, and the ION_IOC_ALLOC ioctl can be used to allocate an ion buffer. The ioctl
                     returns a new file descriptor to the user, which can then be used in the mmap syscall
                     to map the backing store of the ion buffer to userspace.
                     One particular reason for using the ion buffers is that the user can request memory
                     that has contiguous physical addresses. This is particularly important as some devices
                     (as in devices on the hardware, not the phone itself) access physical memory directly
                     and having contiguous memory addresses can greatly improve the performance of such
                     memory accesses, while some devices cannot handle non contiguous physical memory.
                     Similar to SWIOTLB, in order to ensure a region of contiguous physical memory with
                     the requested size is available, the ion driver allocates these memory regions very
                     early in the boot and uses them as memory pools (&amp;#8220;carved out regions&amp;#8221;),
                     which are then used to allocate ion buffers later on when requested. Not all memory
                     pools in the ion device are contiguous memory (for example, the general purpose &amp;#8220;system
                     heap&amp;#8221; may not be a physically contiguous region), but the user can specify the
                     heap_id_mask when using ION_IOC_ALLOC to specify the ion heap with specific properties
                     (for example, contiguous physical memory).
                     The fact that these memory pools are allocated at such an early stage means that their
                     addresses are predictable and depend only on the configuration of the hardware (device
                     tree, available memory, start of memory address, various boot parameters, etc.). This,
                     in particular, means that if I allocate an ion buffer from a rarely used memory pool
                     using ION_IOC_ALLOC, the buffer will most likely be allocated at a predictable address.
                     If I then use mmap to map the buffer to userspace, I&amp;#8217;ll be able to access the
                     memory at this predictable address at any time!
                     After some experimentation, it seems that the user_contig_region is almost never used
                     and I was able to map the entire region to userspace everytime. So in the exploit,
                     I used this memory pool and assumed that I can allocate the entire region to keep
                     it simple. (It would be easy to modify the exploit to accommodate the case where part
                     of the region is not available without compromising reliability.)
                     Now that I am able to put controlled data at a predictable address, I can resolve
                     the problem I encountered previously in the exploit. Recall that, when dma_fence_release
                     is called on my fake fence object:
                     void dma_fence_release(struct kref *kref)
                     {
                     ...
                     if (fence-&amp;gt;ops-&amp;gt;release)
                     fence-&amp;gt;ops-&amp;gt;release(fence);
                     else
                     dma_fence_free(fence);
                     }
                     
                     I had a problem where I needed fence-&amp;gt;ops to point to a valid address that contains
                     all zeros, so that fence-&amp;gt;ops-&amp;gt;release will not be called (as I do not have
                     a valid function address that matches the signature of fence-&amp;gt;ops-&amp;gt;release at
                     this stage and taking this path would crash the kernel)
                     With the ion buffer at a predictable address, I can simply fill it with zero and have
                     fence-&amp;gt;ops point there. This will ensure the path dma_fence_free is taken, which
                     will then free my fake object, giving me a double free primitive while preventing
                     a kernel crash. Before proceeding to exploit this double free primitive, there is,
                     however, another issue that needs resolving first.
                     Escaping an infinite loop
                     Recall that, in the kgsl_ioctl_timeline_destroy function, after the fence object is
                     destroyed and replaced, the following loop is executed:
                     spin_lock_irq(&amp;amp;timeline-&amp;gt;lock);
                     list_for_each_entry_safe(fence, tmp, &amp;amp;temp, node) {
                     dma_fence_set_error(&amp;amp;fence-&amp;gt;base, -ENOENT);
                     dma_fence_signal_locked(&amp;amp;fence-&amp;gt;base);
                     dma_fence_put(&amp;amp;fence-&amp;gt;base);
                     }
                     spin_unlock_irq(&amp;amp;timeline-&amp;gt;lock);
                     
                     The list_for_each_entry_safe will first take the next pointer from the list_head temp
                     to find the first fence entry in the list, and then iterate by following the next
                     pointer in fence-&amp;gt;node until the next entry points back to temp again. If the next
                     entry does not point back to temp, then the loop will just carry on following the
                     next pointer indefinitely. This is a place where variable initialization makes life
                     more difficult. Look at the layout of kgsl_timeline_fence, which embeds a dma_fence
                     object that is added to the kgsl_timeline:
                     struct kgsl_timeline_fence {
                     struct dma_fence base;
                     struct kgsl_timeline *timeline;
                     struct list_head node;
                     };
                     
                     I can see that the node field is the last field in kgsl_timeline_fence, while to construct
                     the exploit, I only need to replace base with controlled data. The above problem would
                     have been solved easily with partial object replacement. Without automatic variable
                     initialization, if I only replace the free&amp;#8217;d kgsl_timeline_fence with an object
                     that is of the size of a dma_fence, then the fields timeline and node would remain
                     intact and contain valid data. This would both cause the next pointer in node to be
                     valid and allow the loop in kgsl_ioctl_timeline_destroy to exit normally. However,
                     with automatic variable initialization, even if I replace the free&amp;#8217;d kgsl_timeline_fence
                     object with a smaller object, the entire memory chunk would be set to zero first,
                     erasing both kgsl_timeline and node, meaning that I now have to fake the node field
                     so that:
                     
                     The next pointer points to a valid address to avoid an immediate crash, in fact, more
                     than that, it needs to point to an object that is another fake kgsl_timeline_fence
                     that can be operated by the functions in the loop (dma_fence_set_error, dma_fence_signal_locked
                     and dma_fence_put) without crashing. That means more fake objects need to be crafted.
                     One of the next pointers in these fake kgsl_timeline_fence objects points back to
                     the temp list to exit the loop, which is a stack allocated variable.
                     
                     The first requirement is not too hard, as I can now use the ion buffer to create these
                     fake kgsl_timeline_fence objects. The second requirement, however, is much harder.
                     On the face of it, this obstacle may seem more like an aesthetic issue rather than
                     a real problem. After all, I can create the fake objects so that the list becomes
                     circular within the fake kgsl_timeline_fence objects:
                     
                     This would cause an infinite loop and hold up a CPU. While it is ugly, the fake objects
                     should take care of the dereferencing issues and avoid crashes, so it may not be a
                     fatal issue after all. Unfortunately, as the loop runs inside a spinlock, after running
                     for a short while, it seems that the watchdog will flag it as a CPU hogging issue
                     and trigger a kernel panic. So, I do need to find a way to exit the loop, and exit
                     it quickly.
                     Let&amp;#8217;s take a step back and take a look at the function dma_fence_signal_locked:
                     int dma_fence_signal_locked(struct dma_fence *fence)
                     {
                     ...
                     struct list_head cb_list;
                     ...
                     /* Stash the cb_list before replacing it with the timestamp */
                     list_replace(&amp;amp;fence-&amp;gt;cb_list, &amp;amp;cb_list);             //&amp;lt;-- 1.
                     ...
                     list_for_each_entry_safe(cur, tmp, &amp;amp;cb_list, node) { //&amp;lt;-- 2.
                     INIT_LIST_HEAD(&amp;amp;cur-&amp;gt;node);
                     cur-&amp;gt;func(fence, cur);
                     }
                     
                     return 0;
                     }
                     
                     This function will be run for each of the fake dma_fence in the list temp (the original
                     free&amp;#8217;d and replaced dma_fence, plus the ones that it links to in the ion buffer).
                     As mentioned before, if the code at 2. in the above is run, then the kernel will probably
                     crash because I cannot provide a valid func, so I still would like to avoid running
                     that path.
                     In order to be able to run this code but not the loop code in 2. above, I need to
                     initialize fence.cb_list to be an empty list, so that its next and prev both point
                     to itself. This is not possible with the initial fake dma_fence that was free&amp;#8217;d
                     by the vulnerability, because the address of fence and hence fence.cb_list is unknown,
                     so I had to avoid the list_replace code altogether for this first fake object. However,
                     because the subsequent fake dma_fence objects that are linked to it are in an ion
                     buffer with a known address, I can now create an empty cb_list for these objects,
                     setting both the next and prev pointers to the addresses of the fence.cb_list field.
                     The function list_replace will then do the following:
                     static inline void list_replace(struct list_head *old,
                     struct list_head *new)
                     {
                     //old-&amp;gt;next = &amp;amp;(fence-&amp;gt;cb_list)
                     new-&amp;gt;next = old-&amp;gt;next;
                     //new-&amp;gt;next = &amp;amp;(fence-&amp;gt;cb_list) =&amp;gt; fence-&amp;gt;cb_list.prev = &amp;amp;cb_list
                     new-&amp;gt;next-&amp;gt;prev = new;
                     //new-&amp;gt;prev = fence-&amp;gt;cb_list.prev =&amp;gt; &amp;amp;cb_list
                     new-&amp;gt;prev = old-&amp;gt;prev;
                     //&amp;amp;cb_list-&amp;gt;next = &amp;amp;cb_list
                     new-&amp;gt;prev-&amp;gt;next = new;
                     }
                     
                     As we can see, after list_replace, the address of the stack variable cb_list has been
                     written to fence-&amp;gt;cb_list.prev, which is somewhere in the ion buffer. As the ion
                     buffer is mapped to user space, I can simply read this address by polling the ion
                     buffer. As dma_fence_signal_locked is run inside kgsl_ioctl_timeline_destroy after
                     the stack variable temp is allocated:
                     long kgsl_ioctl_timeline_destroy(struct kgsl_device_private *dev_priv,
                     unsigned int cmd, void *data)
                     {
                     ...
                     struct list_head temp;
                     ...
                     
                     
                     spin_lock_irq(&amp;amp;timeline-&amp;gt;lock);
                     list_for_each_entry_safe(fence, tmp, &amp;amp;temp, node) {
                     dma_fence_set_error(&amp;amp;fence-&amp;gt;base, -ENOENT);
                     //cb_list, is a stack variable allocated inside `dma_fence_signal_locked`
                     dma_fence_signal_locked(&amp;amp;fence-&amp;gt;base);
                     dma_fence_put(&amp;amp;fence-&amp;gt;base);
                     }
                     spin_unlock_irq(&amp;amp;timeline-&amp;gt;lock);
                     ...
                     }
                     
                     Having the address of cb_list allows me to compute the address of temp, (which will
                     be at a fixed offset from the address of cb_list), so by polling for the address of
                     cb_list and then using this to compute the address of temp and write it back into
                     the next pointer of one of the fake kgsl_timeline_fence objects in the ion buffer,
                     I can exit the loop before the watch dog bites.
                     
                     Hijacking the freelist
                     Now that I am able to avoid kernel crashes, I can continue to exploit the double free
                     primitive mentioned earlier. To recap, once the initial use-after-free vulnerability
                     is triggered and the free&amp;#8217;d object is successfully replaced with controlled
                     data using sendmsg, the replaced object will be used in the following loop as fence:
                     spin_lock_irq(&amp;amp;timeline-&amp;gt;lock);
                     list_for_each_entry_safe(fence, tmp, &amp;amp;temp, node) {
                     dma_fence_set_error(&amp;amp;fence-&amp;gt;base, -ENOENT);
                     dma_fence_signal_locked(&amp;amp;fence-&amp;gt;base);
                     dma_fence_put(&amp;amp;fence-&amp;gt;base);
                     }
                     spin_unlock_irq(&amp;amp;timeline-&amp;gt;lock);
                     
                     In particular, dma_fence_put will reduce the refcount of the fake object and if the
                     refcount reaches zero, it&amp;#8217;ll call dma_fence_free, which will then free the object
                     with kfree_rcu. Since the fake object is in complete control and I was able to resolve
                     various issues that may lead to kernel crashes, I will now assume that this is the
                     code path that is taken and that the fake object will be freed by kfree_rcu. By replacing
                     the fake object again with another object, I can then obtain two references to the
                     same object, which I will be able to free at any time using either of these object
                     handles. The general idea is that, when a memory chunk is freed, the freelist pointer,
                     which points to the next free chunk, will be written to the first 8 bytes of the memory
                     chunk. If I free the object from one handle, and then modify the first 8 bytes of
                     this free&amp;#8217;d object using another handle, then I can hijack the freelist pointer
                     and have it point to an address of my choice, which is where the next allocation will
                     happen. (This is an overly simplified version of what happens as this is only true
                     when the free and allocation are from the same slab, pages used by the memory allocator‚ÄîSLUB
                     allocator in this case‚Äîto allocate memory chunks, with allocation done via the fast
                     path, but this scenario is not difficult to achieve.)
                     In order to be able to modify the first 8 bytes of the object after it was allocated,
                     I&amp;#8217;ll use the signalfd object used in ‚ÄúMitigations are attack surface too‚Äù. The
                     signalfd syscall allocates an 8 byte object to store a mask for the signalfd file,
                     which can be specified by the user with some minor restrictions. The lifetime of the
                     allocated object is tied to the signalfd file that is returned to the user and can
                     be controlled easily by closing the file. Moreover, the first 8 bytes in this object
                     can be changed by calling signalfd again with a different mask. This makes signalfd
                     ideal for my purpose.
                     To hijack the freelist pointer, I have to do the following:
                     
                     Trigger the UAF bug and replaced the free&amp;#8217;d dma_fence object with a fake dma_fence
                     object allocated via sendmsg such that dma_fence_free will be called to free this
                     fake object with kfree_rcu.
                     Spray the heap with signalfd to allocate another object at the same address as the
                     sendmsg object after it was freed. 
                     Free the sendmsg object so that the freelist pointer is written to the mask of the
                     signalfd object in step two. 
                     Modify the mask of the signalfd object so that the freelist pointer now points to
                     an address of my choice, then spray the heap again to allocate objects at that address.
                     
                     If I set the address of the freelist pointer to the address of an ion buffer that
                     I control, then subsequent allocations will place objects in the ion buffer, which
                     I can then access and modify at any time. This gives me a very strong primitive in
                     that I can read and modify any object that I allocate. Essentially, I can fake my
                     own kernel heap in a region where I have both read and write access.
                     
                     The main hurdle to this plan comes from the combination of kfree_rcu and the fact
                     that the CPU running dma_fence_put will be temporarily trapped in a busy loop after
                     kfree_rcu is called. Recall from the previous section that, until I am able to exit
                     the loop by writing the address of the temp list to the next pointer of one of the
                     fake kgsl_timeline_fence::node objects, the loop will be running. This, in particular,
                     means that once kfree_rcu is called and dma_fence_put is exited, the loop will continue
                     to process the other fake dma_fence objects on the CPU that is running kfree_rcu.
                     As explained earlier, kfree_rcu does not immediately free an object, but rather schedules
                     its removal. Most of the time, the free will actually happen on the same CPU that
                     calls kfree_rcu. However, in this case, because the CPU running kfree_rcu is kept
                     busy inside a spinlock by running the loop, the object will almost certainly not be
                     free‚Äôd on that same CPU. Instead, a different CPU will be used to free the object.
                     This causes a problem because the reliability of object replacement depends on the
                     CPU that is used for freeing the object. When an object is freed on a CPU, the memory
                     allocator will place it in a per CPU cache. An allocation that follows immediately
                     on the same CPU will first look for free space in the CPU cache and is most likely
                     going to replace that newly freed object. However, if the allocation happens on a
                     different CPU, then it&amp;#8217;ll most likely replace an object in the cache of a different
                     CPU, rather than the newly freed object. Not knowing which CPU is responsible for
                     freeing the object, together with the uncertainty of when the object is freed (because
                     of the delay introduced by kfree_rcu) means that it may be difficult to replace the
                     object. In practice, however, I was able to achieve reasonable results on the testing
                     device (&gt;70% success rate) with a rather simple scheme: Simply run a loop that spray
                     objects on each CPU and repeat the spraying in intervals to account for the uncertainty
                     in the timing. There is probably room for improvement here to make the exploit more
                     reliable.
                     Another slight modification used in the exploit was to also replace the sendmsg objects
                     after they are freed with another round of signalfd heap spray. This is to ensure
                     that those sendmsg objects don&amp;#8217;t accidentally get replaced by objects that I
                     don&amp;#8217;t control which may interfere with the exploit, as well as to make it easier
                     to identify the actual corrupted object.
                     Now that I can hijack the freelist and redirect new object allocations to the ion
                     buffer that I can freely access at any time, I need to turn this into an arbitrary
                     memory read and write primitive.
                     The Device Memory Mirroring Attack
                     Kernel drivers often need to map memory to the user space, and as such, there are
                     often structures that contain pointers to the page struct or the sg_table struct.
                     These structures often hold pointers to pages that would be mapped to user space when,
                     for example, mmap is called. This makes them very good corruption targets. For example,
                     the ion_buffer object that I have already used is available on all Android devices.
                     It has a sg_table struct that contains information about the pages that will get mapped
                     to user space when mmap is used.
                     Apart from being widely available and accessible from untrusted apps, ion_buffer objects
                     also solve a few other problems, so in what follows, I&amp;#8217;ll use the freelist hijacking
                     primitive above to allocate an ion_buffer struct in an ion buffer backing store that
                     I have arbitrary read and write access to. By doing so, I can freely corrupt the data
                     in all of the ion_buffer structs that are allocated. To avoid confusion, from now
                     on, I&amp;#8217;ll use the term &amp;#8220;fake kernel heap&amp;#8221; to indicate the ion buffer
                     backing store that I use as the fake kernel heap and ion_buffer struct as the structures
                     that I allocate in the fake heap for use as corruption targets.
                     The general idea here is that, by allocating ion_buffer structs in the fake kernel
                     heap, I&amp;#8217;ll be able to modify the ion_buffer struct and replace its sg_table
                     with controlled data. The sg_table structure contains a scatterlist structure that
                     represents a collection of pages that back the ion_buffer structure:
                     struct sg_table {
                     struct scatterlist *sgl;    /* the list */
                     unsigned int nents;     /* number of mapped entries */
                     unsigned int orig_nents;    /* original size of list */
                     };
                     
                     struct scatterlist {
                     unsigned long   page_link;
                     unsigned int    offset;
                     unsigned int    length;
                     dma_addr_t  dma_address;
                     #ifdef CONFIG_NEED_SG_DMA_LENGTH
                     unsigned int    dma_length;
                     #endif
                     };
                     
                     The page_link field in the scatterlist is an encoded form of a page pointer, indicating
                     the actual page where the backing store of the ion_buffer structure is:
                     static inline struct page *sg_page(struct scatterlist *sg)
                     {
                     #ifdef CONFIG_DEBUG_SG
                     BUG_ON(sg_is_chain(sg));
                     #endif
                     return (struct page *)((sg)-&amp;gt;page_link &amp;amp; ~(SG_CHAIN | SG_END));
                     }
                     
                     When mmap is called, the page encoded by page_link will be mapped to user space:
                     int ion_heap_map_user(struct ion_heap *heap, struct ion_buffer *buffer,
                     struct vm_area_struct *vma)
                     {
                     struct sg_table *table = buffer-&amp;gt;sg_table;
                     ...
                     for_each_sg(table-&amp;gt;sgl, sg, table-&amp;gt;nents, i) {
                     struct page *page = sg_page(sg);
                     ...
                     //Maps pages to user space
                     ret = remap_pfn_range(vma, addr, page_to_pfn(page), len,
                     vma-&amp;gt;vm_page_prot);
                     ...
                     }
                     
                     return 0;
                     }
                     
                     As the page pointer is simply a logical shift of the physical address of the page
                     followed by a constant linear offset (see the definition of phys_to_page), being able
                     to control page_link allows me to map an arbitrary page to user space. For many devices,
                     this would be sufficient to achieve arbitrary kernel memory read and write because
                     the kernel image is mapped at a fixed physical address (KASLR randomizes the virtual
                     address offset from this fixed physical address), so there is no need to worry about
                     KASLR when working with physical addresses.
                     
                     Samsung devices, however, do KASLR differently. Instead of mapping the kernel image
                     to a fixed physical address, the physical address of the kernel image is randomized
                     (strictly speaking, the intermediate physical address as perceived by the kernel,
                     which is not the real physical address but rather a virtual address given by the hypervisor)
                     instead. So in our case, I still need to leak an address to defeat KASLR. With the
                     fake kernel heap, however, this is fairly easy to achieve. An ion_buffer object contains
                     a pointer to an ion_heap, which is responsible for allocating the backing stores for
                     the ion_buffer:
                     struct ion_buffer {
                     struct list_head list;
                     struct ion_heap *heap;
                     ...
                     };
                     
                     While the ion_heap is not an global object in the kernel image, each ion_heap contains
                     an ion_heap_ops field, which points to the corresponding &amp;#8220;vtable&amp;#8221; of the
                     specific ion_heap object:
                     struct ion_heap {
                     struct plist_node node;
                     enum ion_heap_type type;
                     struct ion_heap_ops *ops;
                     ...
                     }
                     
                     The ops field in the above is a global object in the kernel image. If I can read ion_buffer-&amp;gt;heap-&amp;gt;ops,
                     then I&amp;#8217;m also able to get an address to defeat KASLR and translate addresses
                     in the kernel image to physical addresses. This can be done as follows:
                     1) First locate the ion_buffer struct in the fake kernel heap. This can be done using
                     the flags field in the ion_buffer:
                     struct ion_buffer {
                     struct list_head list;
                     struct ion_heap *heap;
                     unsigned long flags;
                     ...
                     
                     which is a 4 byte value passed from the parameters of the ION_IOC_ALLOC ioctl when
                     the ion_buffer is created. I can set these to specific &amp;#8220;magic&amp;#8221; values
                     and search for them in the fake kernel heap.
                     2) Once the ion_buffer struct is located, read its heap pointer. This will be a virtual
                     address in the low memory area outside of the kernel image, and as such, its physical
                     address can be obtained by applying a constant offset.
                     3) Once the physical address of the corresponding ion_heap object is obtained, modify
                     the sg_table of the ion_buffer so that its backing store points to the page containing
                     the ion_heap. 4. Call mmap on the ion_buffer file descriptor, this will map the page
                     containing the ion_heap to user space. This page can then be read directly from user
                     space to obtain the ops pointer, which will give the KASLR offset.
                     The use of the ion_buffer struct also solves another problem. While the fake kernel
                     heap is convenient, it is not perfect. Whenever an object in the fake kernel heap
                     is freed, kfree will check whether the page containing the object is a single page
                     slab from the SLUB allocator by using the PageSlab check. If the check fails, then
                     the PageCompound check will be performed to check whether the page is part of a bigger
                     slab.
                     void kfree(const void *x)
                     {
                     struct page *page;
                     void *object = (void *)x;
                     
                     trace_kfree(_RET_IP_, x);
                     
                     if (unlikely(ZERO_OR_NULL_PTR(x)))
                     return;
                     
                     page = virt_to_head_page(x);
                     if (unlikely(!PageSlab(page))) {     //&amp;lt;-------- check if the page allocated is
                     a single page slab
                     unsigned int order = compound_order(page);
                     
                     BUG_ON(!PageCompound(page));     //&amp;lt;-------- check if the page is allocated as
                     part of a multipage slab
                     ...
                     }
                     ...
                     }
                     
                     As these checks are performed on the page struct itself, which contains metadata to
                     the page, they will fail and cause the kernel to crash whenever an object is freed.
                     This can be fixed by using the arbitrary read and write primitives that I now have
                     to overwrite the respective metadata in the page struct (the address of a page struct
                     that corresponds to a physical address is simply a logical shift of the physical address
                     followed by a translation of a fixed offset, so I can map the page containing the
                     page struct to user space and modify its content). It would, however, be simpler if
                     I can ensure that the objects that occupy the fake kernel heap never get freed. Before
                     an ion_buffer struct is freed, ion_buffer_destroy is called:
                     int ion_buffer_destroy(struct ion_device *dev, struct ion_buffer *buffer)
                     {
                     ...
                     heap = buffer-&amp;gt;heap;
                     ...
                     if (heap-&amp;gt;flags &amp;amp; ION_HEAP_FLAG_DEFER_FREE)
                     ion_heap_freelist_add(heap, buffer);    //&amp;lt;--------- does not free immediately
                     else
                     ion_buffer_release(buffer);
                     
                     return 0;
                     }
                     
                     If the ion_heap contains the flag ION_HEAP_FLAG_DEFER_FREE, then the ion_buffer will
                     not be freed immediately, but instead gets added to the free_list of the ion_heap
                     using ion_heap_freelist_add. The ion_buffer objects added to this list will only be
                     freed at a later stage when needed and only if the ION_HEAP_FLAG_DEFER_FREE flag is
                     set. Normally, of course, the ION_HEAP_FLAG_DEFER_FREE does not change over the lifetime
                     of the ion_heap, but with our arbitrary memory write primitive, I can simply add ION_HEAP_FLAG_DEFER_FREE
                     to the ion_heap-&amp;gt;flags, free the ion_buffer, and then remove ION_HEAP_FLAG_DEFER_FREE
                     again and the ion_buffer will just get stuck in the freelist of the ion_heap and never
                     get freed. Moreover, the page containing the ion_heap object is already mapped for
                     the purpose of defeating KASLR, so toggling the flag is fairly trivial. By spraying
                     the fake kernel heap so that it is filled with ion_buffer objects and their dependents,
                     I can ensure that those objects are never freed and avoid the kernel crash.
                     Bypassing SELinux
                     When SELinux is enabled, it can be run in either the permissive mode or the enforcing
                     mode. When in permissive mode, it will only audit and log unauthorized accesses but
                     will not block them. The mode in which SELinux is run is controlled by the selinux_enforcing
                     variable. If this variable is zero, then SELinux is run in permissive mode. Normally,
                     variables that are critical to the security of the system are protected by Samsung&amp;#8217;s
                     Kernel Data Protection (KDP), by marking them as read-only using the __kdp_ro or the
                     __rkp_ro attribute. This attribute indicates that the variable is in a read-only page
                     and its modification is guarded by hypervisor calls. However, to my surprise, it seems
                     that Samsung has forgotten to protect this variable (again?!) in the 5.x branch Qualcomm
                     kernel:
                     //In security/selinux/hooks.c
                     #ifdef CONFIG_SECURITY_SELINUX_DEVELOP
                     static int selinux_enforcing_boot;
                     int selinux_enforcing;
                     
                     So, I can just overwrite selinux_enforcing to zero and set SELinux to the permissive
                     mode. While there are other means to bypass SELinux (such as the one used in this
                     exploit by Valentina Palmiotti) that work more universally, a shortcut at this point
                     is more than welcome, so I‚Äôll just set the selinux_enforcing variable.
                     Running arbitrary root commands using ret2kworker(TM)
                     A well-known problem with getting root on Samsung devices is the protection imposed
                     by the Samsung&amp;#8217;s RKP (Realtime Kernel Protection). A common way to gain root
                     on Android devices is to overwrite the credentials of our own process with the root
                     credentials. However, Samsung&amp;#8217;s RKP write protects the credentials of each process,
                     so that is not possible here. In my last exploit, I was able to execute arbitrary
                     code as root because the particular UAF exploited led to a controlled function pointer
                     being executed in code run by a kworker, which is run as root. In that exploit, I
                     was able to corrupt objects that are then added to a work queue, which was then consumed
                     by a kworker and executed by running a function supplied as a function pointer. This
                     made it relatively easy to run arbitrary functions as root.
                     Of course, with arbitrary memory read and write primitives, it is possible to simply
                     add objects to one of these work queues (which are basically linked lists containing
                     work structs) and wait for a kworker to pick up the work. As it turns out, many of
                     these work queues are indeed static global objects, with fixed addresses in the kernel
                     image:
                     ffffffc012c8f7e0 D system_wq
                     ffffffc012c8f7e8 D system_highpri_wq
                     ffffffc012c8f7f0 D system_long_wq
                     ffffffc012c8f7f8 D system_unbound_wq
                     ffffffc012c8f800 D system_freezable_wq
                     ffffffc012c8f808 D system_power_efficient_wq
                     ffffffc012c8f810 D system_freezable_power_efficient_wq
                     
                     So, it is relatively straightforward to add entries to these work queues and have
                     a kworker pick up the work. However, because of kCFI, I would only be able to call
                     functions with the following signatures:
                     void (func*)(struct work_struct *work)
                     
                     The problem is whether I can find a powerful enough function to run. It turns out
                     to be fairly simple. The function call_usermodehelper_exec_work, which is commonly
                     used in kernel exploits to run shell commands, fits the bill and will run a shell
                     command supplied by me. So by modifying, say, the system_unbound_wq and adding an
                     entry to it that holds a pointer to call_usermodehelper_exec_work, I can bypass both
                     Samsung&amp;#8217;s RKP and kCFI to run arbitrary commands as root.
                     The exploit can be found here with some setup notes.
                     Conclusions
                     In this post, I exploited a UAF with fairly typical primitives and examined how various
                     mitigations affected the exploit. While in the end, I was able to bypass all the mitigations
                     and develop an exploit that is no less reliable than another one that I did last year,
                     the mitigations did force the exploit to take a very different, and longer path.
                     The biggest hurdle was the kCFI, which turned a relatively straightforward exploit
                     into a rather complex one. As explained in the post, the UAF bug offers many primitives
                     to execute an arbitrary function pointer. Combined with a separate information leak
                     (which I happen to have, and is pending disclosure) the bug would have been trivial
                     to exploit as in the case of the NPU bugs I wrote about last year. Instead, the combination
                     of Samsung&amp;#8217;s RKP and kCFI made this impossible and forced me to look into an
                     alternative path, which is far less straightforward.
                     On the other hand, many of the techniques introduced here, such as the ones in &amp;#8220;The
                     ultimate fake object store&amp;#8221; and &amp;#8220;The device memory mirroring attack&amp;#8221;
                     can be readily standardized to turn common primitives into arbitrary memory read and
                     write. As we&amp;#8217;ve seen, having arbitrary memory read and write, even with the
                     restrictions imposed by Samsung&amp;#8217;s RKP, is already powerful enough for many purposes.
                     In this regard, it seems to me that the effect of kCFI may be to shift exploitation
                     techniques in favor of certain primitives over others rather than rendering many bugs
                     unexploitable. It is, after all, as many have said, a mitigation that happens fairly
                     late in the process.
                     A rather underrated mitigation, perhaps, is automatic variable initialization. While
                     this mitigation mainly targets vulnerabilities that exploit uninitialized variables,
                     we&amp;#8217;ve seen that it also prevents partial object replacement, which is a common
                     exploitation technique. In fact, it nearly broke the exploit if it hadn‚Äôt been for
                     a piece of luck where I was able to leak the address of a stack variable (see &amp;#8220;Escaping
                     an infinite loop&amp;#8221;). Not only does this mitigation kill a whole class of bugs,
                     but it also breaks some useful exploit primitives.
                     We&amp;#8217;ve also seen how primitives in manipulating the kernel scheduler enabled
                     us to widen many race windows to allow time for object replacements. This allowed
                     me to overcome the problem posed by the delayed free caused by kfree_rcu (not a mitigation),
                     without significantly compromising the reliability. It seems that, in the context
                     of the kernel, mitigating a UAF with a quarantine and delayed free of objects (an
                     approach adopted by the Scudo allocator for Android user processes) may not be that
                     effective after all.
                     Disclosure practices, patching time, and patch gapping
                     I reported this vulnerability to Qualcomm on November 16 2021 and they publicly disclosed
                     it in early May 2022. It took about six months, which seems to be the standard time
                     Qualcomm takes between receiving a report and publicly disclosing it. In the past,
                     the long disclosure times have led to some researchers disclosing full exploits before
                     vulnerabilities were patched in public (see, for example, this). Qualcomm employs
                     a rather ‚Äúspecial‚Äù disclosure process, in which it normally discloses the patch of
                     a vulnerability privately to its customers within 90 days (this is indicated by the
                     &amp;#8220;Customer Notified Date&amp;#8221; in Qualcomm&amp;#8217;s advisories), but the patch
                     may only be fully integrated and publicly disclosed several months after that.
                     While this practice gives customers (OEM) time to patch the vulnerabilities before
                     they become public, it creates an opportunity for what is known as &amp;#8220;patch gapping.&amp;#8221;
                     As patches are first disclosed privately to customers, it means that some customers
                     may decide to apply the patch before it is publicly disclosed, while others may wait
                     until the patch is fully integrated and disclosed publicly and then apply the monthly
                     patch. For example, this patch was applied to the Samsung S21 in July 2021 (by inspecting
                     the Samsung open source code), but only publicly disclosed as CVE-2021-30305 in October
                     2021. By comparing firmware between different vendors, it may be possible to discover
                     vulnerabilities that are privately patched by one vendor, but not the other. As the
                     private disclosure time and public disclosure time are often a few months apart, this
                     leaves ample time to develop an exploit. Moreover, the patch for the vulnerability
                     described in this post was publicly visible sometime in December 2021 with a very
                     clear commit message, leaving a five‚Äêmonth (or two‚Äêmonth for private disclosure) gap
                     in between. While the exploit is complex, a skilled hacking team could easily have
                     developed and deployed it within that time window. I hope that Qualcomm will improve
                     their patching time or reduce the gap between their private and public disclosure
                     time.
                     </span></summary><time datetime="2022-06-16T18:00:52+02:00">Thu, 16 Jun 2022 16:00</time><article><p>In this post, I‚Äôll exploit a use-after-free (UAF) bug, CVE-2022-22057 in the Qualcomm GPU driver, which affected the kernel branch 5.4 or above, and is mostly used by flagship models running the Snapdragon 888 chipset or above (for example, the Snapdragon version of S21‚Äîused in the U.S. and a number of Asian countries, such as China and Korea‚Äî and all versions of the Galaxy Z Flip3, and many others). The device tested here is the Samsung Galaxy Z Flip3 and I was able to use this bug alone to gain arbitrary kernel memory read and write, and from there, disable SELinux and run arbitrary commands as root. The bug itself was publicly disclosed in the <a href="https://docs.qualcomm.com/product/publicresources/securitybulletin/may-2022-bulletin.html">Qualcomm security bulletin in May 2022</a> and the fix was applied to devices in the May 2022 Android security patch.</p>
<h2>Why Android GPU drivers</h2>
<p>While the bug itself is a fairly standard use-after-free bug that involves a tight race condition in the GPU driver, and this post focuses mostly on bypassing the many mitigations that are in place on the device rather on the GPU, it is, nevertheless, worth giving some motivations as to why the Android GPU makes an attractive target for attackers.</p>
<p>As was mentioned in the article ‚Äú<a href="https://googleprojectzero.blogspot.com/2022/04/the-more-you-know-more-you-know-you.html">The More You Know, The More You Know You Don‚Äôt Know</a>‚Äù by Maddie Stone, out of seven Android 0-days that were detected as exploited in the wild in 2021, five of them targeted GPU drivers. As of the date of writing, another bug that was exploited in the wild, <a href="https://source.android.com/security/bulletin/pixel/2022-03-01">CVE-2021-39793</a> disclosed in March 2022 also targeted the GPU driver.</p>
<p>Apart from the fact that most Android devices use either the Qualcomm Adreno or the ARM Mali GPU, making it possible to obtain universal coverage with relatively few bugs (this was mentioned in Maddie Stone‚Äôs article), the GPU drivers are also reachable from the untrusted app sandbox in all Android devices, further reducing the number of bugs that are required in a full chain. Another reason GPU drivers are attractive is that most GPU drivers also handle rather complex memory sharing logic between the GPU device and the CPU. These often involve fairly elaborate memory management code that is prone to bugs that can be abused to achieve arbitrary read and write of physical memory or to bypass memory protection. As these bugs enable an attacker to abuse the functionality of the GPU memory management code, many of them are also undetectable as memory corruptions and immune to existing mitigations, which mostly aim at preventing control flow hijacking. Some examples are the work of <a href="https://github.com/secmob/TiYunZong-An-Exploit-Chain-to-Remotely-Root-Modern-Android-Devices/blob/master/us-20-Gong-TiYunZong-An-Exploit-Chain-to-Remotely-Root-Modern-Android-Devices-wp.pdf">Guang Gong</a> and <a href="https://googleprojectzero.blogspot.com/2020/09/attacking-qualcomm-adreno-gpu.html">Ben Hawkes</a>, who exploited logic errors in the handling of GPU opcode to gain arbitrary memory read and write.</p>
<h2>The vulnerability</h2>
<p>The vulnerability was introduced in the 5.4 branch of the Qualcomm <a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/tree/msm-5.4.r2">msm 5.4 kernel</a> when the new <a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/e0952b5cce5bcccbe04a18a9673869e6924ccac0/drivers/gpu/msm/kgsl_timeline.c">kgsl timeline</a> feature, together with some new ioctl associated with it, was introduced. The msm 5.4 kernel carried out some rather major refactoring of the kernel graphics support layer (kgsl) driver (under <a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/tree/msm-5.4.r2/drivers/gpu/msm">drivers/gpu/msm</a>, which is Qualcomm‚Äôs GPU driver) and introduced some new features. Both these new features and refactoring resulted in a number of regressions and new security issues, most of which were found and fixed internally and then disclosed publicly as security issues in the bulletins (kudos to Qualcomm for not silently patching security issues), including some that look <a href="https://source.codeaurora.org/quic/la/kernel/msm-5.4/commit/?id=6c58829f4428f2564833743de7135f4451077e75">fairly exploitable</a>.</p>
<p>The <code>kgsl_timeline</code> object can be created and destroyed via the ioctl <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/e0952b5cce5bcccbe04a18a9673869e6924ccac0/drivers/gpu/msm/kgsl_timeline.c#L323">IOCTL_KGSL_TIMELINE_CREATE</a></code> and <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/e0952b5cce5bcccbe04a18a9673869e6924ccac0/drivers/gpu/msm/kgsl_timeline.c#L96">IOCTL_KGSL_TIMELINE_DESTROY</a></code>. The <code>kgsl_timeline</code> object stores a list of <code><a href="https://www.kernel.org/doc/html/v5.9/driver-api/dma-buf.html">dma_fence</a></code> objects in the field <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/e0952b5cce5bcccbe04a18a9673869e6924ccac0/drivers/gpu/msm/kgsl_timeline.h#L26">fences</a></code>. The ioctl <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/e0952b5cce5bcccbe04a18a9673869e6924ccac0/drivers/gpu/msm/kgsl_timeline.c#L427">IOCTL_KGSL_TIMELINE_FENCE_GET</a></code> and <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/e0952b5cce5bcccbe04a18a9673869e6924ccac0/drivers/gpu/msm/kgsl_timeline.c#L358">IOCTL_KGSL_TIMELINE_WAIT</a></code> can be used to add <code>dma_fence</code> objects to this list. The <code>dma_fence</code> objects added are refcounted objects and their refcounts are decreased using the standard <code>dma_fence_put</code> method.</p>
<p>What is interesting about <code>timeline-&gt;fences</code> is that it does not actually hold an extra refcount to the fences. Instead, to avoid a <code>dma_fence</code> in <code>timeline-&gt;fences</code> from being freed, a customized <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/5e3cf80f1b6a12fcf54b007f3c9f235f35b9b7f1/drivers/gpu/msm/kgsl_timeline.c#L231">release</a></code> function, <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/5e3cf80f1b6a12fcf54b007f3c9f235f35b9b7f1/drivers/gpu/msm/kgsl_timeline.c#L165">timeline_fence_release</a></code> is used to remove the <code>dma_fence</code> from <code>timeline->fences</code> before it gets freed.</p>
<p>When the refcount of a <code>dma_fence</code> stored in <code>kgsl_timeline::fences</code> is decreased to zero, the method <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/5e3cf80f1b6a12fcf54b007f3c9f235f35b9b7f1/drivers/gpu/msm/kgsl_timeline.c#L165">timeline_fence_release</a></code> will be called to remove the <code>dma_fence</code> from <code>kgsl_timeline::fences</code> so that it can no longer be referenced from the <code>kgsl_timeline</code>, and then <code>dma_fence_free</code> is called to free the object itself:</p>
<pre><code>static void timeline_fence_release(struct dma_fence *fence)
{
    ...
    spin_lock_irqsave(&amp;timeline-&gt;fence_lock, flags);

    /* If the fence is still on the active list, remove it */
    list_for_each_entry_safe(cur, temp, &amp;timeline-&gt;fences, node) {
        if (f != cur)
            continue;

        list_del_init(&amp;f-&gt;node);    //&lt;----- 1. Remove fence
        break;
    }
    spin_unlock_irqrestore(&amp;timeline-&gt;fence_lock, flags);
    ...
    kgsl_timeline_put(f-&gt;timeline);
    dma_fence_free(fence);     //&lt;-------    2.  frees the fence
}
</code></pre>
<p>Although the removal of <code>fence</code> from <code>timeline-&gt;fences</code> is correctly protected by the <code>timeline-&gt;fence_lock</code>, <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/5e3cf80f1b6a12fcf54b007f3c9f235f35b9b7f1/drivers/gpu/msm/kgsl_timeline.c#L515">IOCTL_KGSL_TIMELINE_DESTROY</a></code> makes it possible to acquire a reference to a <code>dma_fence</code> in <code>fences</code> after its refcount has reached zero but before it gets removed from <code>fences</code> in <code>timeline_fence_release</code>:</p>
<pre><code>long kgsl_ioctl_timeline_destroy(struct kgsl_device_private *dev_priv,
        unsigned int cmd, void *data)
{
    ...
    spin_lock(&amp;timeline-&gt;fence_lock);  //&lt;------------- a.
    list_for_each_entry_safe(fence, tmp, &amp;timeline-&gt;fences, node)
        dma_fence_get(&amp;fence-&gt;base);
    list_replace_init(&amp;timeline-&gt;fences, &amp;temp);
    spin_unlock(&amp;timeline-&gt;fence_lock);


    spin_lock_irq(&amp;timeline-&gt;lock);
    list_for_each_entry_safe(fence, tmp, &amp;temp, node) { //&lt;----- b.
        dma_fence_set_error(&amp;fence-&gt;base, -ENOENT);
        dma_fence_signal_locked(&amp;fence-&gt;base);
        dma_fence_put(&amp;fence-&gt;base);
    }
    spin_unlock_irq(&amp;timeline-&gt;lock);
    ...
}
</code></pre>
<p>In <code>kgsl_ioctl_timeline_destroy</code>, when destroying the timeline, the fences in <code>timeline-&gt;fences</code> are first copied to another list, <code>temp</code> and then removed from <code>timeline-&gt;fences</code> (point a.). As <code>timeline-&gt;fences</code> does not hold an extra reference of the fence, refcount is increased to stop them from being free‚Äôd in <code>temp</code>. Again, the manipulation of <code>timeline-&gt;fences</code> is protected by <code>timeline-&gt;fence_lock</code> here. However, if the refcount of a fence is already zero when <code>a</code> in the above is reached, but <code>timeline_fence_release</code> has not yet been able to remove it from <code>timeline-&gt;fences</code>, (it has not reached point 1. In the snippet included in <code>timeline_fence_release</code>), then the <code>dma_fence</code> would be moved to <code>temp</code>, and although its reference is increased, it is already too late, because <code>timeline_fence_release</code> will free the <code>dma_fence</code> when it reaches point 2., regardless of the refcount. So if the events happens in the following order, then a use-after-free could be triggered at point b.:</p>
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Kernel-1.png?resize=960%2C540" alt="" width="960" height="540" class="alignnone size-full wp-image-65548 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Kernel-1.png?resize=960%2C540?w=960 960w, https://github.blog/wp-content/uploads/2022/06/Kernel-1.png?resize=960%2C540?w=300 300w, https://github.blog/wp-content/uploads/2022/06/Kernel-1.png?resize=960%2C540?w=768 768w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1" /></div>
<p>In the above, the red blocks indicate code that are holding the same lock, meaning that the execution of these blocks are mutually exclusive. While the order of events may look rather contrived (as it always is when you try to illustrate a race condition), the actual timing is not too hard to achieve. As the code in <code>timeline_fence_release</code> that removes a <code>dma_fence</code> from <code>timeline-&gt;fences</code> cannot run while the code in <code>kgsl_ioctl_timeline_destroy</code> is accessing <code>timeline-&gt;fence</code> (both are holding <code>timeline-&gt;fence_lock</code>), by adding a large number of <code>dma_fence</code> to <code>timeline-&gt;fence</code>, I can increase the time required to run the red code block in <code>kgsl_ioctl_timeline_destroy</code>. If I decrease the refcount of the last <code>dma_fence</code> in <code>timeline-&gt;fences</code> in thread two to zero while the red code block in thread one is running, I can trigger <code>timeline_fence_release</code> before <code>dma_fence_get</code> increases the refcount of this <code>dma_fence</code> in thread one. As the red code block in thread two also needs to acquire the <code>timeline-&gt;fence_lock</code>, it can not remove the <code>dma_fence</code> from <code>timeline-&gt;fences</code> until after the red code block in thread one finished. By that time, all the <code>dma_fence</code> in <code>timeline-&gt;fences</code> have been moved to the list <code>temp</code>. This also means that by the time the red code block in thread two runs, <code>timeline-&gt;fences</code> is an empty list and the loop finishes quickly and proceeds to <code>dma_fence_free</code>. In short, as long as I add a large enough number of <code>dma_fences</code> to <code>timeline-&gt;fences</code>, I can create a large race window when <code>kgsl_ioctl_timeline_destroy</code> is moving the <code>dma_fences</code> in <code>timeline-&gt;fences</code> to <code>temp</code>. As long as I reduce the last refcount of the last <code>dma_fence</code> in <code>timeline-&gt;fences</code> within this window, I‚Äôm able to trigger the UAF bug.</p>
<h2>Mitigations</h2>
<p>While triggering the bug is not too difficult, exploiting it, on the other hand, is a completely different matter. The device that I used for testing this bug and for developing the exploit is a Samsung Galaxy Z Flip3. The latest Samsung devices running kernel version 5.x probably have the most mitigations in place, even more so than the Google Pixels. While older devices running kernel 4.x often have mitigations such as the <a href="https://source.android.com/devices/tech/debug/kcfi">kCFI</a> (Kernel Control Flow Integrity) and <a href="https://android-developers.googleblog.com/2020/06/system-hardening-in-android-11.html">variable initialization</a> switched off, all those features are switched on in the 5.x kernel branch, and on top of that, there is also the <a href="https://www.samsungknox.com/en">Samsung RKP (Realtime Kernel Protection)</a> that protects various memory area, such as kernel code and process credentials, making it difficult to execute arbitrary code even when arbitrary memory read and write is achieved. In this section, I‚Äôll briefly explain how those mitigations affect the exploit.</p>
<h3>kCFI</h3>
<p>The kCFI is arguably the mitigation that takes the most effort to bypass, especially when used in conjunction with the Samsung hypervisor which protects many important memory areas in the kernel. The kCFI prevents hijacking of control flow by limiting the locations where a dynamic callsite can jump to using function signatures. For example, in the current vulnerability, after the <code>dma_fence</code> is freed, the function <code>dma_fence_signal_locked</code> is called:</p>
<pre><code>long kgsl_ioctl_timeline_destroy(struct kgsl_device_private *dev_priv,
        unsigned int cmd, void *data)
{
    ...
    spin_lock_irq(&amp;timeline-&gt;lock);
    list_for_each_entry_safe(fence, tmp, &amp;temp, node) {
        dma_fence_set_error(&amp;fence-&gt;base, -ENOENT);
        dma_fence_signal_locked(&amp;fence-&gt;base);       //&lt;---- free'd fence is used
        dma_fence_put(&amp;fence-&gt;base);
    }
    spin_unlock_irq(&amp;timeline-&gt;lock);
    ...
}
</code></pre>
<p>The function <code>dma_fence_signal_locked</code> then invokes a function <code>cur-&gt;func</code> that is an element inside the <code>fence-&gt;cb_list</code> list.</p>
<pre><code>int dma_fence_signal_locked(struct dma_fence *fence)
{
    ...
    list_for_each_entry_safe(cur, tmp, &amp;cb_list, node) {
        INIT_LIST_HEAD(&amp;cur-&gt;node);
        cur-&gt;func(fence, cur);
    }
    ...
}
</code></pre>
<p>Without kCFI, the now free‚Äôd <code>fence</code> object can be replaced with a fake object, meaning that <code>cb_list</code> and its elements, hence <code>func</code>, can all be faked, giving a ready to use primitive to call an arbitrary function with both its first and second arguments pointing to controlled data (<code>fence</code> and <code>cur</code> can both be faked). The exploit would have been very easy once KASLR was defeated (for example, with a separate bug to leak kernel addresses like in <a href="https://securitylab.github.com/research/qualcomm_npu/">this exploit</a>). However, because of kCFI, <code>func</code> can now only be replaced by functions that have the type <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/5e3cf80f1b6a12fcf54b007f3c9f235f35b9b7f1/include/linux/dma-fence.h#L118">dma_fence_func_t</a></code>, which greatly limits the use of this primitive.</p>
<p>While in the past, I‚Äôve written about how easy it is to <a href="https://securitylab.github.com/research/qualcomm_npu/">bypass</a> Samsung‚Äôs control flow integrity checks (JOPP, jump-oriented programming prevention), there is no easy way round kCFI. One common way to bypass kCFI is to use a double free to hijack the freelist and then apply the <a href="https://i.blackhat.com/briefings/asia/2018/asia-18-WANG-KSMA-Breaking-Android-kernel-isolation-and-Rooting-with-ARM-MMU-features.pdf">Kernel Space Mirroring Attack (KSMA)</a>. This was used a number of times, for example, in <a href="https://github.com/2freeman/Slides/blob/main/PoC-2020-Three%20Dark%20clouds%20over%20the%20Android%20kernel.pdf">Three dark clouds over the Android kernel</a> of Jun Yao, <a href="https://i.blackhat.com/USA21/Wednesday-Handouts/us-21-Typhoon-Mangkhut-One-Click-Remote-Universal-Root-Formed-With-Two-Vulnerabilities.pdf">Typhoon Mangkhut: One-click remote universal root formed with two vulnerabilities</a> of Hongli Han, Rong Jian, Xiaodong Wang and Peng Zhou.</p>
<p>While the current bug also gives me a double free primitive when <code>dma_fence_put</code> is called after <code>fence</code> is freed:</p>
<pre><code>long kgsl_ioctl_timeline_destroy(struct kgsl_device_private *dev_priv,
        unsigned int cmd, void *data)
{
    ...
    spin_lock_irq(&amp;timeline-&gt;lock);
    list_for_each_entry_safe(fence, tmp, &amp;temp, node) {
        dma_fence_set_error(&amp;fence-&gt;base, -ENOENT);
        dma_fence_signal_locked(&amp;fence-&gt;base); 
        dma_fence_put(&amp;fence-&gt;base);       //&lt;----- free'd fence can be freed again
    }
    spin_unlock_irq(&amp;timeline-&gt;lock);
    ...
}
</code></pre>
<p>The above decreases the refcount of the fake <code>fence</code> object, which I can control to make it one, so that the fake fence gets freed again. This, however, does not allow me to apply KSMA as it would require overwriting of the <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/5e3cf80f1b6a12fcf54b007f3c9f235f35b9b7f1/arch/arm64/include/asm/pgtable.h#L465">swapper_pg_dir</a></code> data structure, which is protected by the Samsung hypervisor.</p>
<h3>Variable initialization</h3>
<p>From Android 11 onwards, the kernel can enable automatic variable initialization by enabling various kernel build flags. The following, for example, is taken from the build configuration of the Z Flip3:</p>
<pre><code># Memory initialization
#
CONFIG_CC_HAS_AUTO_VAR_INIT_PATTERN=y
CONFIG_CC_HAS_AUTO_VAR_INIT_ZERO=y
# CONFIG_INIT_STACK_NONE is not set
# CONFIG_INIT_STACK_ALL_PATTERN is not set
CONFIG_INIT_STACK_ALL_ZERO=y
CONFIG_INIT_ON_ALLOC_DEFAULT_ON=y
# CONFIG_INIT_ON_FREE_DEFAULT_ON is not set
# end of Memory initialization
</code></pre>
<p>While the feature is available since Android 11, many devices running kernel branch 4.x do not have these enabled. On the other hand, devices running kernel 5.x seem to have these enabled. Apart from the obvious uninitialized variables vulnerabilities that this feature prevents, it also makes it harder for object replacement. In particular, it is no longer possible to perform partial object replacement, in which only the first bytes of the object are replaced, while the rest of the object remains valid. So for example, the type of heap spray technique under the section, &#8220;Spraying the heap&#8221; in ‚Äú<a href="https://googleprojectzero.blogspot.com/2020/02/mitigations-are-attack-surface-too.html">Mitigations are attack surface, too</a>‚Äù by Jann Horn is no longer possible with automatic variable initialization. In the context of the current bug, this mitigation limits the heap spray options that I have, and I‚Äôll explain more as we go through the exploit.</p>
<h3>kfree_rcu</h3>
<p>This isn‚Äôt a security mitigation at all, but it is nevertheless interesting to mention it here, because it has a similar effect to some UAF mitigations that had been proposed. The UAF <code>fence</code> object in this bug is freed when <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/5e3cf80f1b6a12fcf54b007f3c9f235f35b9b7f1/drivers/dma-buf/dma-fence.c#L270">dma_fence_free</a></code> is called, which, instead of the normal <code>kfree</code>, uses <code>kfree_rcu</code>. In short, <code>kfree_rcu</code> does not free an object immediately, but rather schedules it to be freed when certain criteria are met. This acts somewhat like a delayed free that introduces an uncertainty in the time when the object is freed. Interestingly, this effect is quite similar to the UAF mitigation that is used in the <a href="https://source.android.com/devices/tech/debug/scudo">Scudo</a> allocator (default allocator of Android user space processes), which quarantines the free‚Äôd objects before actually freeing them to introduce uncertainty. A similar proposal has been suggested for the <a href="https://www.openwall.com/lists/kernel-hardening/2020/08/13/7">linux kernel</a> (but was rejected later). Apart from introducing uncertainty in the object replacement, a delayed free may also cause problems for UAF with tight race windows. So, on the face of it, the use of <code>kfree_rcu</code> would be rather problematic for exploiting the current bug. However, with many primitives to manipulate the size of a race window, such as the ones detailed in <a href="https://googleprojectzero.blogspot.com/2022/03/racing-against-clock-hitting-tiny.html">Racing against the clock‚Äîhitting a tiny kernel race window</a> and an older technique in <a href="https://static.sched.com/hosted_files/lsseu2019/04/LSSEU2019%20-%20Exploiting%20race%20conditions%20on%20Linux.pdf">Exploiting race conditions on [ancient] Linux</a>, (both by Jann Horn, the older technique is used for exploiting the current bug) any tight race window can be made large enough to allow for the delay caused by <code>kfree_rcu</code>, and the subsequent object replacement. As for the uncertainty, that does not seem to cause a very big problem either. In exploiting this bug, I actually had to perform object replacement with <code>kfree_rcu</code> twice, the second time without even knowing on which CPU core the free is going to happen, and yet even with this and all the other moving parts, a rather unoptimized exploit still runs at a reasonable reliability (~70%) on the device tested. While I believe that the second object replacement with <code>kfree_rcu</code> (where the CPU that frees the object is uncertain) is probably the main source of unreliability, I‚Äôd attribute that reliability loss more to the lack of CPU knowledge rather than to the delayed free. In my opinion, a delayed free may not be a very effective UAF mitigation when there are primitives that allow the scheduler to be manipulated.</p>
<h3>Samsung RKP (Realtime Kernel Protection)</h3>
<p>The Samsung RKP protects various parts of the memory from being written to. This prevents processes from overwriting their own credentials to become root, as well as protecting SELinux settings from being overwritten. It also prevents kernel code regions and other important objects, such as kernel page tables, from being overwritten. In practice, though, once arbitrary kernel memory read and write (subject to RKP restrictions) is achieved, there are ways to bypass these restrictions. For example, SELinux rules can be modified by overwriting the avc cache (see, for example, this <a href="https://github.com/chompie1337/s8_2019_2215_poc/blob/master/poc/selinux_bypass.c">exploit</a> by Valentina Palmiotti), while gaining root can be done by <a href="https://securitylab.github.com/research/qualcomm_npu/">hijacking other processes that run as root</a>. In the context of the current bug, the Samsung RKP mostly works with kCFI to prevent arbitrary functions from being called.</p>
<p>In this post, I‚Äôll exploit the bug with all these mitigations enabled.</p>
<h2>Exploiting the bug</h2>
<p>I‚Äôll now start going through the exploit of the bug. It is a fairly typical use-after-free bug that involves a race condition and perhaps reasonably strong primitives with both the possibility of arbitrary function call and double free, which is not that uncommon. Apart from that, this is a typical bug, just like many other UAF found in the kernel. So, it seems fitting to use this bug to gauge how these mitigations affect the development of a standard UAF exploit.</p>
<h3>Adding <code>dma_fence</code> to <code>timeline-&gt;fences</code></h3>
<p>In the section, &#8220;The vulnerability,&#8221; I explained that the bug relies on having <code>dma_fence</code> objects added to the <code>fences</code> list in a <code>kgsl_timeline</code> object, which then have their refcount decreased to zero while the <code>kgsl_timeline</code> is being destroyed. There are two options to add <code>dma_fence</code> objects to a <code>kgsl_timeline</code>, the first is to use <code>IOCTL_KGSL_TIMELINE_FENCE_GET</code>:</p>
<pre><code>long kgsl_ioctl_timeline_fence_get(struct kgsl_device_private *dev_priv,
        unsigned int cmd, void *data)
{
    ...
    timeline = kgsl_timeline_by_id(device, param-&gt;timeline);
    ...
    fence = kgsl_timeline_fence_alloc(timeline, param-&gt;seqno); //&lt;----- dma_fence created and added to timeline
    ...
    sync_file = sync_file_create(fence);
    if (sync_file) {
        fd_install(fd, sync_file-&gt;file);
        param-&gt;handle = fd;
    }
    ...
}
</code></pre>
<p>This will create a <code>dma_fence</code> with <code>kgsl_timeline_fence_alloc</code> and add it to the <code>timeline</code>. The caller then gets a file descriptor for a <code>sync_file</code> that corresponds to the <code>dma_fence</code>. When the <code>sync_file</code> is closed, the refcount of <code>dma_fence</code> is decreased to zero.</p>
<p>The second option is to use <code>IOCTL_KGSL_TIMELINE_WAIT</code>:</p>
<pre><code>long kgsl_ioctl_timeline_wait(struct kgsl_device_private *dev_priv,
        unsigned int cmd, void *data)
{
    ...
    fence = kgsl_timelines_to_fence_array(device, param-&gt;timelines,
        param-&gt;count, param-&gt;timelines_size,
        (param-&gt;flags == KGSL_TIMELINE_WAIT_ANY));     //&lt;------ dma_fence created and added to timeline
    ...
    if (!timeout)
        ret = dma_fence_is_signaled(fence) ? 0 : -EBUSY;
    else {
        ret = dma_fence_wait_timeout(fence, true, timeout);   //&lt;----- 1.
        ...
    }

    dma_fence_put(fence);
    ...
}
</code></pre>
<p>This will create <code>dma_fence</code> objects using <code>kgsl_timelines_to_fence_array</code> and add them to the <code>timeline</code>. If a <code>timeout</code> value is specified, then the call will enter <code>dma_fence_wait_timeout</code> (path labeled 1), which will wait until either the timeout expires or when the thread receives an interrupt. After <code>dma_fence_wait_timeout</code> finishes, <code>dma_fence_put</code> is called to reduce the refcount of the <code>dma_fence</code> to zero. So, by specifying a large timeout, <code>dma_fence_wait_timeout</code> will block until it receives an interrupt, which will then free the <code>dma_fence</code> that was added to the <code>timeline</code>.</p>
<p>While <code>IOCTL_KGSL_TIMELINE_FENCE_GET</code> may seem easier to use and control at first glance, in practice, the overhead incurred by closing the <code>sync_file</code> makes the timing for destruction of the <code>dma_fence</code> less reliable. So, for the exploit, I use <code>IOCTL_KGSL_TIMELINE_FENCE_GET</code> to create and add persistent <code>dma_fence</code> objects to fill the <code>timeline-&gt;fences</code> list to enlarge the race window, while the last <code>dma_fence</code> object that is used for the UAF bug is added using <code>IOCTL_KGSL_TIMELINE_WAIT</code> and which gets freed when I send an interrupt signal to the thread that calls <code>IOCTL_KGSL_TIMELINE_WAIT</code>.</p>
<h3>Widening the tiny race window</h3>
<p>To recap, in order to exploit the vulnerability, I need to remove the refcount of a <code>dma_fence</code> in the <code>fences</code> list of a <code>kgsl_timeline</code> within the first race window labeled in the following code block:</p>
<pre><code>long kgsl_ioctl_timeline_destroy(struct kgsl_device_private *dev_priv,
        unsigned int cmd, void *data)
{
    //BEGIN OF FIRST RACE WINDOW
    spin_lock(&amp;timeline-&gt;fence_lock);
    list_for_each_entry_safe(fence, tmp, &amp;timeline-&gt;fences, node)
        dma_fence_get(&amp;fence-&gt;base);
    list_replace_init(&amp;timeline-&gt;fences, &amp;temp);
    spin_unlock(&amp;timeline-&gt;fence_lock);
    //END OF FIRST RACE WINDOW
    //BEGIN OF SECOND RACE WINDOW
    spin_lock_irq(&amp;timeline-&gt;lock);
    list_for_each_entry_safe(fence, tmp, &amp;temp, node) {
        dma_fence_set_error(&amp;fence-&gt;base, -ENOENT);
        dma_fence_signal_locked(&amp;fence-&gt;base);
        dma_fence_put(&amp;fence-&gt;base);
    }
    spin_unlock_irq(&amp;timeline-&gt;lock);
    //END OF SECOND RACE WINDOW
    ...
}
</code></pre>
<p>As explained before, the first race window can be enlarged by adding a large number of <code>dma_fence</code> objects to <code>timeline-&gt;fences</code>, which makes it easy to trigger the decrease of refcount within this window. However, to exploit the bug, the following <a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/5e3cf80f1b6a12fcf54b007f3c9f235f35b9b7f1/drivers/gpu/msm/kgsl_timeline.c#L172">code</a>, as well as the object replacement, must be completed before the end of the second race window:</p>
<pre><code>    spin_lock_irqsave(&amp;timeline-&gt;fence_lock, flags);
    list_for_each_entry_safe(cur, temp, &amp;timeline-&gt;fences, node) {
        if (f != cur)
            continue;
        list_del_init(&amp;f-&gt;node);
        break;
    }
    spin_unlock_irqrestore(&amp;timeline-&gt;fence_lock, flags);
    trace_kgsl_timeline_fence_release(f-&gt;timeline-&gt;id, fence-&gt;seqno);
    kgsl_timeline_put(f-&gt;timeline);
    dma_fence_free(fence);
</code></pre>
<p>As explained before, because of the <code>spin_lock</code>, the above cannot start until the first race window ends, but by the time this code is run, <code>timeline-&gt;fences</code> has been emptied, so the loop will be quick to run. However, since <code>dma_fence_free</code> uses <code>kfree_rcu</code>, the actual freeing of <code>fence</code> is delayed. This makes it impossible to replace the free‚Äôd fence before the second race window finishes, unless we manipulate the scheduler. To do so, I‚Äôll use a technique in ‚Äú<a href="https://static.sched.com/hosted_files/lsseu2019/04/LSSEU2019%20-%20Exploiting%20race%20conditions%20on%20Linux.pdf">Exploiting race conditions on [ancient] Linux</a>‚Äù that I also used in another <a href="https://securitylab.github.com/research/one_day_short_of_a_fullchain_android/">Android exploit</a> to widen this race window.</p>
<p>I‚Äôll recap the essence of the technique here for readers who are not familiar with it.</p>
<p>To ensure that each task (thread or process) has a fair share of the CPU time, the linux kernel scheduler can interrupt a running task and put it on hold, so that another task can be run. This kind of interruption and stopping of a task is called preemption (where the interrupted task is preempted). A task can also put itself on hold to allow another task to run, such as when it is waiting for some I/O input, or when it calls <code>sched_yield()</code>. In this case, we say that the task is voluntarily preempted. Preemption can happen inside syscalls such as ioctl calls as well, and on Android, tasks can be preempted except in some critical regions (e.g. holding a spinlock). This behavior can be manipulated by using CPU affinity and task priorities.</p>
<p>By default, a task is run with the priority <code>SCHED_NORMAL</code>, but a lower priority <code>SCHED_IDLE</code> can also be set using the <code>sched_setscheduler</code> call (or <code>pthread_setschedparam</code> for threads). Furthermore, it can also be pinned to a CPU with <code>sched_setaffinity</code>, which would only allow it to run on a specific CPU. By pinning two tasks, one with <code>SCHED_NORMAL</code> priority and the other with <code>SCHED_IDLE</code> priority to the same CPU, it is possible to control the timing of the preemption as follows.</p>
<ol>
<li>First have the <code>SCHED_NORMAL</code> task perform a syscall that would cause it to pause and wait. For example, it can read from a pipe with no data coming in from the other end, then it would wait for more data and voluntarily preempt itself, so that the <code>SCHED_IDLE</code> task can run.</li>
<li>As the <code>SCHED_IDLE</code> task is running, send some data to the pipe that the <code>SCHED_NORMAL</code> task had been waiting on. This will wake up the <code>SCHED_NORMAL</code> task and cause it to preempt the <code>SCHED_IDLE</code> task, and because of the task priority, the <code>SCHED_IDLE</code> task will be preempted and put on hold.</li>
<li>The <code>SCHED_NORMAL</code> task can then run a busy loop to keep the <code>SCHED_IDLE</code> task from waking up.</li>
</ol>
<p>In our case, the object replacement sequence goes as follows:</p>
<ol>
<li>Run <code>IOCTL_KGSL_TIMELINE_WAIT</code> on a thread to add <code>dma_fence</code> objects to a <code>kgsl_timeline</code>. Set the timeout to a large value and use <code>sched_setaffinity</code> to pin this task to a CPU, call it <code>SPRAY_CPU</code>. Once the <code>dma_fence</code> object is added, the task will then become idle until it receives an interrupt.</li>
<li>Set up a <code>SCHED_NORMAL</code> task and pin it to another CPU (<code>DESTROY_CPU</code>) that listens to an empty pipe. This will cause this task to become idle initially and allow <code>DESTROY_CPU</code> to run a lower priority task. Once the empty pipe receives some data, this task then will run a busy loop.</li>
<li>Set up a <code>SCHED_IDLE</code> task on <code>DESTROY_CPU</code> which will run <code>IOCTL_KGSL_TIMELINE_DESTROY</code> to destroy the timeline where the <code>dma_fence</code> is added in step one. As the task set up in step two is waiting for a response to an empty pipe, <code>DESTROY_CPU</code> will run this task first.</li>
<li>Send an interrupt to the task running <code>IOCTL_KGSL_TIMELINE_WAIT</code>. The task will then unblock and free the <code>dma_fence</code> while <code>IOCTL_KGSL_TIMELINE_DESTROY</code> is running within the first race window.</li>
<li>Write to the empty pipe that the <code>SCHED_NORMAL</code> task is listening to. This will cause the <code>SCHED_NORMAL</code> task to preempt the <code>SCHED_IDLE</code> task. Once it has successfully preempted the task, <code>DESTROY_CPU</code> will run the busy loop, causing the <code>SCHED_IDLE</code> task to be put on hold.</li>
<li>As the <code>SCHED_IDLE</code> task running <code>IOCTL_KGSL_TIMELINE_DESTROY</code> is put on hold, there is now enough time to overcome the delay introduced by <code>kfree_rcu</code> and allow the <code>dma_fence</code> in step four to be freed and replaced. After that, I can resume <code>IOCTL_KGSL_TIMELINE_DESTROY</code> so that the subsequent operations will be performed on the now free‚Äôd and replaced <code>dma_fence</code> object.</li>
</ol>
<p>One caveat here is that, because preemption cannot happen while a thread is holding a <code>spinlock</code>, so <code>IOCTL_KGSL_TIMELINE_DESTROY</code> can only be preempted during the window between spinlocks (marked by the comment below):</p>
<pre><code>long kgsl_ioctl_timeline_destroy(struct kgsl_device_private *dev_priv,
        unsigned int cmd, void *data)
{
    spin_lock(&amp;timeline-&gt;fence_lock);
    list_for_each_entry_safe(fence, tmp, &amp;timeline-&gt;fences, node)
      ...
    spin_unlock(&amp;timeline-&gt;fence_lock);
    //Preemption window
    spin_lock_irq(&amp;timeline-&gt;lock);
    list_for_each_entry_safe(fence, tmp, &amp;temp, node) {
    ...
    }
    spin_unlock_irq(&amp;timeline-&gt;lock);
    ...
}
</code></pre>
<p>Although the preemption window in the above appears to be very small, in practice, as long as the <code>SCHED_NORMAL</code> task tries to preempt the <code>SCHED_IDLE</code> task running <code>IOCTL_KGSL_TIMELINE_DESTROY</code> while the first <code>spinlock</code> is held, preemption will happen as soon as the <code>spinlock</code> is released, making it much easier to succeed in preempting <code>IOCTL_KGSL_TIMELINE_DESTROY</code> at the right time.</p>
<p>The following figure illustrates what happens in an ideal world, with red blocks indicating regions that hold a <code>spinlock</code> and are therefore not possible to preempt, and dotted lines indicating tasks that are idle.</p>
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Kernel-2.png?resize=875%2C536" alt="" width="875" height="536" class="alignnone size-full wp-image-65549 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Kernel-2.png?resize=875%2C536?w=875 875w, https://github.blog/wp-content/uploads/2022/06/Kernel-2.png?resize=875%2C536?w=300 300w, https://github.blog/wp-content/uploads/2022/06/Kernel-2.png?resize=875%2C536?w=768 768w" sizes="(max-width: 875px) 100vw, 875px" data-recalc-dims="1" /></div>
<p>The following figure illustrates what happens in the real world:</p>
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Kernel-3.png?resize=1024%2C775" alt="" width="1024" height="775" class="alignnone size-full wp-image-65550 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Kernel-3.png?resize=1024%2C775?w=1186 1186w, https://github.blog/wp-content/uploads/2022/06/Kernel-3.png?resize=1024%2C775?w=300 300w, https://github.blog/wp-content/uploads/2022/06/Kernel-3.png?resize=1024%2C775?w=768 768w, https://github.blog/wp-content/uploads/2022/06/Kernel-3.png?resize=1024%2C775?w=1024 1024w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /></div>
<p>For object replacement, I&#8217;ll use <code><a href="https://blog.lexfo.fr/cve-2017-11176-linux-kernel-exploitation-part3.html">sendmsg</a></code>, which is a standard way to replace free&#8217;d objects in the linux kernel with controlled data. As the method is fairly standard, I won‚Äôt give the details here, but refer readers to the link above. From now on, I&#8217;ll assume that the free&#8217;d <code>dma_fence</code> object is replaced by arbitrary data. (There are some restrictions in the first 12 bytes using this method, but that does not affect our exploit.)</p>
<p>Assuming that the free&#8217;d <code>dma_fence</code> object can be replaced with arbitrary data, let&#8217;s take a look at how this fake object is used. After the <code>dma_fence</code> is replaced, it is then used in <code>kgsl_ioctl_timeline_destroy</code> as follows:</p>
<pre><code>    spin_lock_irq(&amp;timeline-&gt;lock);
    list_for_each_entry_safe(fence, tmp, &amp;temp, node) {
        dma_fence_set_error(&amp;fence-&gt;base, -ENOENT);
        dma_fence_signal_locked(&amp;fence-&gt;base);
        dma_fence_put(&amp;fence-&gt;base);
    }
    spin_unlock_irq(&amp;timeline-&gt;lock);
</code></pre>
<p>Three different functions, <code>dma_fence_set_error</code>, <code>dma_fence_signal_locked</code> and <code>dma_fence_put</code> will be called with the argument <code>fence</code>. The function <code>dma_fence_set_error</code> will write an error code to the <code>fence</code> object, which may be useful with a suitable object replacement, but not for <code>sendmsg</code> object replacements and I&#8217;ll not be investigating this possibility here. The function <code>dma_fence_signal_locked</code> does the following:</p>
<pre><code>int dma_fence_signal_locked(struct dma_fence *fence)
{
    ...
    if (unlikely(test_and_set_bit(DMA_FENCE_FLAG_SIGNALED_BIT,   //&lt;-- 1.
                      &amp;fence-&gt;flags)))
        return -EINVAL;

    /* Stash the cb_list before replacing it with the timestamp */
    list_replace(&amp;fence-&gt;cb_list, &amp;cb_list);                    //&lt;-- 2.
    ...
    list_for_each_entry_safe(cur, tmp, &amp;cb_list, node) {        //&lt;-- 3.
        INIT_LIST_HEAD(&amp;cur-&gt;node);
        cur-&gt;func(fence, cur);
    }

    return 0;
}
</code></pre>
<p>It first checks <code>fence-&gt;flags</code> (1. in the above): if the <code>DMA_FENCE_FLAG_SIGNALED_BIT</code> flag is set, then the <code>fence</code> has been signaled, and the function exits early. If the <code>fence</code> has not been signaled, then <code>list_replace</code> is called to remove objects in <code>fence-&gt;cb_list</code> and place them in a temporary <code>cb_list</code> (2. in the above). After that, functions stored in <code>cb_list</code> are called (3. above). As explained in the section, &#8220;kCFI&#8221; because of the CFI mitigation, this will only allow me to call functions of a certain type; besides, at this stage I have no knowledge of function addresses, so I&#8217;m most likely just going to crash the kernel if I reach this path. So, at this stage, I have little choice but to set the <code>DMA_FENCE_FLAG_SIGNALED_BIT</code>flag in my fake object so that <code>dma_fence_signal_locked</code> exits early.</p>
<p>This leaves me the <code>dma_fence_put</code> function, which decreases the refcount of <code>fence</code> and calls <code>dma_fence_release</code> if the refcount reaches zero:</p>
<pre><code>void dma_fence_release(struct kref *kref)
{
    ...
    if (fence-&gt;ops-&gt;release)
        fence-&gt;ops-&gt;release(fence);
    else
        dma_fence_free(fence);
}
</code></pre>
<p>If <code>dma_fence_release</code> is called, then eventually it&#8217;ll check the <code>fence-&gt;ops</code> and call <code>fence-&gt;ops-&gt;release</code>. This gives me two problems: First, <code>fence-&gt;ops</code> needs to point to valid memory, otherwise the dereference will fail, and even if the dereference succeeds, <code>fence-&gt;ops-&gt;release</code> either needs to be zero, or it has to be the address of a function of an appropriate type.</p>
<p>All these present me with two choices. I can either follow the standard path: try to replace the <code>fence</code> object with another object or try to make use of the limited write primitives that <code>dma_fence_put</code> and <code>dma_fence_set_error</code> offer me, while hoping that I can still control the <code>flags</code> and <code>refcount</code> fields to avoid <code>dma_fence_signal_locked</code> or <code>dma_fence_release</code> crashing the kernel.</p>
<p>Or, I can try something else.</p>
<h2>The ultimate fake object store</h2>
<p>While exploiting <a href="https://securitylab.github.com/research/one_day_short_of_a_fullchain_android/">another bug</a>, I came across the Software Input Output Translation Lookaside Buffer (SWIOTLB), which is a memory region that is allocated at a very early stage during boot time. As such, the physical address of the SWIOTLB is very much fixed and depends only on the hardware configuration. Moreover, as this memory is in the &#8220;low memory&#8221; region (Android devices do not seem to have a &#8220;high memory&#8221; region) and not in the kernel image, the virtual address is simply the physical address with a fixed offset (readers who are interested in the details can, for example, follow the implementation of the <code>kmap</code> function):</p>
<pre><code>#define __virt_to_phys_nodebug(x) ({                   \
    phys_addr_t __x = (phys_addr_t)(__tag_reset(x));        \
    __is_lm_address(__x) ? __lm_to_phys(__x) : __kimg_to_phys(__x); \
})
#define __is_lm_address(addr)  (!(((u64)addr) &amp; BIT(vabits_actual - 1)))

#define __lm_to_phys(addr) (((addr) + physvirt_offset))
</code></pre>
<p>The above definitions are from <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/5e3cf80f1b6a12fcf54b007f3c9f235f35b9b7f1/arch/arm64/include/asm/memory.h">arch/arm64/include/asm/memory.h</a></code>, which is the relevant implementation for Android. The variable <code>physvirt_offset</code> used for translating the address is a fixed constant set in <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/5e3cf80f1b6a12fcf54b007f3c9f235f35b9b7f1/arch/arm64/mm/init.c#L517">arm64_memblock_init</a></code>:</p>
<pre><code>void __init arm64_memblock_init(void)
{...
    memstart_addr = round_down(memblock_start_of_DRAM(),
                   ARM64_MEMSTART_ALIGN);
    physvirt_offset = PHYS_OFFSET - PAGE_OFFSET;

 ...
}
</code></pre>
<p>On top of that, the memory in the SWIOTLB can be accessed via the <code>adsp</code> driver that is reachable from an untrusted app, so this seems to be a good place to store fake objects and redirect fake pointers to. However, in the 5.x version of the kernel, the SWIOTLB is only allocated when the kernel is compiled with the <code>CONFIG_DMA_ZONE32</code> flag, which is not the case for our device.</p>
<p>There is, however, something better. The fact that the early allocation of SWIOTLB gives it a predictable address prompted me to inspect the boot log to see if there are other regions of memory that are allocated early during the boot, and it turns out that there are indeed other memory regions that are allocated very early during the boot.</p>
<pre><code>&lt;6&gt;[    0.000000] [0:        swapper:    0]  Reserved memory: created CMA memory pool at 0x00000000f2800000, size 212 MiB
&lt;6&gt;[    0.000000] [0:        swapper:    0]  OF: reserved mem: initialized node secure_display_region, compatible id shared-dma-pool
...
&lt;6&gt;[    0.000000] [0:        swapper:    0]  OF: reserved mem: initialized node user_contig_region, compatible id shared-dma-pool
&lt;6&gt;[    0.000000] [0:        swapper:    0]  Reserved memory: created CMA memory pool at 0x00000000f0c00000, size 12 MiB

&lt;6&gt;[    0.578613] [7:      swapper/0:    1]  platform soc:qcom,ion:qcom,ion-heap@22: assigned reserved memory node sdsp_region
...
&lt;6&gt;[    0.578829] [7:      swapper/0:    1]  platform soc:qcom,ion:qcom,ion-heap@26: assigned reserved memory node user_contig_region
...
</code></pre>
<p>The <code>Reserved memory</code> regions in the above seem to be the memory pools that are used for allocating ion buffers.</p>
<p>On Android, the <code><a href="https://lwn.net/Articles/480055/">ion_allocator</a></code> is used to allocate memory regions used for DMA (direct memory access) that allows kernel drivers and userspace processes to share the same underlying memory. The ion allocator is accessible by an untrusted app via the <code>/dev/ion</code> file, and the <code>ION_IOC_ALLOC</code> ioctl can be used to allocate an ion buffer. The ioctl returns a new file descriptor to the user, which can then be used in the <code>mmap</code> syscall to map the backing store of the ion buffer to userspace.</p>
<p>One particular reason for using the ion buffers is that the user can request memory that has contiguous physical addresses. This is particularly important as some devices (as in devices on the hardware, not the phone itself) access physical memory directly and having contiguous memory addresses can greatly improve the performance of such memory accesses, while some devices cannot handle non contiguous physical memory.</p>
<p>Similar to SWIOTLB, in order to ensure a region of contiguous physical memory with the requested size is available, the ion driver allocates these memory regions very early in the boot and uses them as memory pools (&#8220;carved out regions&#8221;), which are then used to allocate ion buffers later on when requested. Not all memory pools in the ion device are contiguous memory (for example, the general purpose &#8220;system heap&#8221; may not be a physically contiguous region), but the user can specify the <code>heap_id_mask</code> when using <code>ION_IOC_ALLOC</code> to specify the ion heap with specific properties (for example, contiguous physical memory).</p>
<p>The fact that these memory pools are allocated at such an early stage means that their addresses are predictable and depend only on the configuration of the hardware (device tree, available memory, start of memory address, various boot parameters, etc.). This, in particular, means that if I allocate an ion buffer from a rarely used memory pool using <code>ION_IOC_ALLOC</code>, the buffer will most likely be allocated at a predictable address. If I then use <code>mmap</code> to map the buffer to userspace, I&#8217;ll be able to access the memory at this predictable address at any time!</p>
<p>After some experimentation, it seems that the <code>user_contig_region</code> is almost never used and I was able to map the entire region to userspace everytime. So in the exploit, I used this memory pool and assumed that I can allocate the entire region to keep it simple. (It would be easy to modify the exploit to accommodate the case where part of the region is not available without compromising reliability.)</p>
<p>Now that I am able to put controlled data at a predictable address, I can resolve the problem I encountered previously in the exploit. Recall that, when <code>dma_fence_release</code> is called on my fake <code>fence</code> object:</p>
<pre><code>void dma_fence_release(struct kref *kref)
{
    ...
    if (fence-&gt;ops-&gt;release)
        fence-&gt;ops-&gt;release(fence);
    else
        dma_fence_free(fence);
}
</code></pre>
<p>I had a problem where I needed <code>fence-&gt;ops</code> to point to a valid address that contains all zeros, so that <code>fence-&gt;ops-&gt;release</code> will not be called (as I do not have a valid function address that matches the signature of <code>fence-&gt;ops-&gt;release</code> at this stage and taking this path would crash the kernel)</p>
<p>With the ion buffer at a predictable address, I can simply fill it with zero and have <code>fence-&gt;ops</code> point there. This will ensure the path <code>dma_fence_free</code> is taken, which will then free my fake object, giving me a double free primitive while preventing a kernel crash. Before proceeding to exploit this double free primitive, there is, however, another issue that needs resolving first.</p>
<h2>Escaping an infinite loop</h2>
<p>Recall that, in the <code>kgsl_ioctl_timeline_destroy</code> function, after the <code>fence</code> object is destroyed and replaced, the following loop is executed:</p>
<pre><code>    spin_lock_irq(&amp;timeline-&gt;lock);
    list_for_each_entry_safe(fence, tmp, &amp;temp, node) {
        dma_fence_set_error(&amp;fence-&gt;base, -ENOENT);
        dma_fence_signal_locked(&amp;fence-&gt;base);
        dma_fence_put(&amp;fence-&gt;base);
    }
    spin_unlock_irq(&amp;timeline-&gt;lock);
</code></pre>
<p>The <code>list_for_each_entry_safe</code> will first take the <code>next</code> pointer from the <code>list_head</code> <code>temp</code> to find the first <code>fence</code> entry in the list, and then iterate by following the <code>next</code> pointer in <code>fence-&gt;node</code> until the <code>next</code> entry points back to <code>temp</code> again. If the <code>next</code> entry does not point back to <code>temp</code>, then the loop will just carry on following the <code>next</code> pointer indefinitely. This is a place where variable initialization makes life more difficult. Look at the layout of <code>kgsl_timeline_fence</code>, which embeds a <code>dma_fence</code> object that is added to the <code>kgsl_timeline</code>:</p>
<pre><code>struct kgsl_timeline_fence {
    struct dma_fence base;
    struct kgsl_timeline *timeline;
    struct list_head node;
};
</code></pre>
<p>I can see that the <code>node</code> field is the last field in <code>kgsl_timeline_fence</code>, while to construct the exploit, I only need to replace <code>base</code> with controlled data. The above problem would have been solved easily with partial object replacement. Without automatic variable initialization, if I only replace the free&#8217;d <code>kgsl_timeline_fence</code> with an object that is of the size of a <code>dma_fence</code>, then the fields <code>timeline</code> and <code>node</code> would remain intact and contain valid data. This would both cause the <code>next</code> pointer in <code>node</code> to be valid and allow the loop in <code>kgsl_ioctl_timeline_destroy</code> to exit normally. However, with automatic variable initialization, even if I replace the free&#8217;d <code>kgsl_timeline_fence</code> object with a smaller object, the entire memory chunk would be set to zero first, erasing both <code>kgsl_timeline</code> and <code>node</code>, meaning that I now have to fake the <code>node</code> field so that:</p>
<ol>
<li>The <code>next</code> pointer points to a valid address to avoid an immediate crash, in fact, more than that, it needs to point to an object that is another fake <code>kgsl_timeline_fence</code> that can be operated by the functions in the loop (<code>dma_fence_set_error</code>, <code>dma_fence_signal_locked</code> and <code>dma_fence_put</code>) without crashing. That means more fake objects need to be crafted.</li>
<li>One of the <code>next</code> pointers in these fake <code>kgsl_timeline_fence</code> objects points back to the <code>temp</code> list to exit the loop, which is a stack allocated variable.</li>
</ol>
<p>The first requirement is not too hard, as I can now use the ion buffer to create these fake <code>kgsl_timeline_fence</code> objects. The second requirement, however, is much harder.</p>
<p>On the face of it, this obstacle may seem more like an aesthetic issue rather than a real problem. After all, I can create the fake objects so that the list becomes circular within the fake <code>kgsl_timeline_fence</code> objects:</p>
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Kernel-4.png?resize=623%2C525" alt="" width="623" height="525" class="alignnone size-full wp-image-65551 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Kernel-4.png?resize=623%2C525?w=623 623w, https://github.blog/wp-content/uploads/2022/06/Kernel-4.png?resize=623%2C525?w=300 300w" sizes="(max-width: 623px) 100vw, 623px" data-recalc-dims="1" /></div>
<p>This would cause an infinite loop and hold up a CPU. While it is ugly, the fake objects should take care of the dereferencing issues and avoid crashes, so it may not be a fatal issue after all. Unfortunately, as the loop runs inside a <code>spinlock</code>, after running for a short while, it seems that the watchdog will flag it as a CPU hogging issue and trigger a kernel panic. So, I do need to find a way to exit the loop, and exit it quickly.</p>
<p>Let&#8217;s take a step back and take a look at the function <code>dma_fence_signal_locked</code>:</p>
<pre><code>int dma_fence_signal_locked(struct dma_fence *fence)
{
    ...
    struct list_head cb_list;
    ...
    /* Stash the cb_list before replacing it with the timestamp */
    list_replace(&amp;fence-&gt;cb_list, &amp;cb_list);             //&lt;-- 1.
    ...
    list_for_each_entry_safe(cur, tmp, &amp;cb_list, node) { //&lt;-- 2.
        INIT_LIST_HEAD(&amp;cur-&gt;node);
        cur-&gt;func(fence, cur);
    }

    return 0;
}
</code></pre>
<p>This function will be run for each of the fake <code>dma_fence</code> in the list <code>temp</code> (the original free&#8217;d and replaced <code>dma_fence</code>, plus the ones that it links to in the ion buffer). As mentioned before, if the code at 2. in the above is run, then the kernel will probably crash because I cannot provide a valid <code>func</code>, so I still would like to avoid running that path.</p>
<p>In order to be able to run this code but not the loop code in 2. above, I need to initialize <code>fence.cb_list</code> to be an empty list, so that its <code>next</code> and <code>prev</code> both point to itself. This is not possible with the initial fake <code>dma_fence</code> that was free&#8217;d by the vulnerability, because the address of <code>fence</code> and hence <code>fence.cb_list</code> is unknown, so I had to avoid the <code>list_replace</code> code altogether for this first fake object. However, because the subsequent fake <code>dma_fence</code> objects that are linked to it are in an ion buffer with a known address, I can now create an empty <code>cb_list</code> for these objects, setting both the <code>next</code> and <code>prev</code> pointers to the addresses of the <code>fence.cb_list</code> field. The function <code>list_replace</code> will then do the following:</p>
<pre><code>static inline void list_replace(struct list_head *old,
                struct list_head *new)
{
    //old-&gt;next = &amp;(fence-&gt;cb_list)
    new-&gt;next = old-&gt;next;
    //new-&gt;next = &amp;(fence-&gt;cb_list) =&gt; fence-&gt;cb_list.prev = &amp;cb_list
    new-&gt;next-&gt;prev = new;
    //new-&gt;prev = fence-&gt;cb_list.prev =&gt; &amp;cb_list
    new-&gt;prev = old-&gt;prev;
    //&amp;cb_list-&gt;next = &amp;cb_list
    new-&gt;prev-&gt;next = new;
}
</code></pre>
<p>As we can see, after <code>list_replace</code>, the address of the stack variable <code>cb_list</code> has been written to <code>fence-&gt;cb_list.prev</code>, which is somewhere in the ion buffer. As the ion buffer is mapped to user space, I can simply read this address by polling the ion buffer. As <code>dma_fence_signal_locked</code> is run inside <code>kgsl_ioctl_timeline_destroy</code> after the stack variable <code>temp</code> is allocated:</p>
<pre><code>long kgsl_ioctl_timeline_destroy(struct kgsl_device_private *dev_priv,
        unsigned int cmd, void *data)
{
    ...
    struct list_head temp;
    ...


    spin_lock_irq(&amp;timeline-&gt;lock);
    list_for_each_entry_safe(fence, tmp, &amp;temp, node) {
        dma_fence_set_error(&amp;fence-&gt;base, -ENOENT);
        //cb_list, is a stack variable allocated inside `dma_fence_signal_locked`
        dma_fence_signal_locked(&amp;fence-&gt;base);
        dma_fence_put(&amp;fence-&gt;base);
    }
    spin_unlock_irq(&amp;timeline-&gt;lock);
    ...
}
</code></pre>
<p>Having the address of <code>cb_list</code> allows me to compute the address of <code>temp</code>, (which will be at a fixed offset from the address of <code>cb_list</code>), so by polling for the address of <code>cb_list</code> and then using this to compute the address of <code>temp</code> and write it back into the <code>next</code> pointer of one of the fake <code>kgsl_timeline_fence</code> objects in the ion buffer, I can exit the loop before the watch dog bites.</p>
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Kernel-5.png?resize=960%2C540" alt="" width="960" height="540" class="alignnone size-full wp-image-65552 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Kernel-5.png?resize=960%2C540?w=960 960w, https://github.blog/wp-content/uploads/2022/06/Kernel-5.png?resize=960%2C540?w=300 300w, https://github.blog/wp-content/uploads/2022/06/Kernel-5.png?resize=960%2C540?w=768 768w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1" /></div>
<h2>Hijacking the freelist</h2>
<p>Now that I am able to avoid kernel crashes, I can continue to exploit the double free primitive mentioned earlier. To recap, once the initial use-after-free vulnerability is triggered and the free&#8217;d object is successfully replaced with controlled data using <code>sendmsg</code>, the replaced object will be used in the following loop as <code>fence</code>:</p>
<pre><code>    spin_lock_irq(&amp;timeline-&gt;lock);
    list_for_each_entry_safe(fence, tmp, &amp;temp, node) {
        dma_fence_set_error(&amp;fence-&gt;base, -ENOENT);
        dma_fence_signal_locked(&amp;fence-&gt;base);
        dma_fence_put(&amp;fence-&gt;base);
    }
    spin_unlock_irq(&amp;timeline-&gt;lock);
</code></pre>
<p>In particular, <code>dma_fence_put</code> will reduce the refcount of the fake object and if the refcount reaches zero, it&#8217;ll call <code>dma_fence_free</code>, which will then free the object with <code>kfree_rcu</code>. Since the fake object is in complete control and I was able to resolve various issues that may lead to kernel crashes, I will now assume that this is the code path that is taken and that the fake object will be freed by <code>kfree_rcu</code>. By replacing the fake object again with another object, I can then obtain two references to the same object, which I will be able to free at any time using either of these object handles. The general idea is that, when a memory chunk is freed, the freelist pointer, which points to the next free chunk, will be written to the first 8 bytes of the memory chunk. If I free the object from one handle, and then modify the first 8 bytes of this free&#8217;d object using another handle, then I can hijack the freelist pointer and have it point to an address of my choice, which is where the next allocation will happen. (This is an overly simplified version of what happens as this is only true when the free and allocation are from the same slab, pages used by the memory allocator‚ÄîSLUB allocator in this case‚Äîto allocate memory chunks, with allocation done via the fast path, but this scenario is not difficult to achieve.)</p>
<p>In order to be able to modify the first 8 bytes of the object after it was allocated, I&#8217;ll use the <code>signalfd</code> object used in ‚Äú<a href="https://googleprojectzero.blogspot.com/2020/02/mitigations-are-attack-surface-too.html">Mitigations are attack surface too</a>‚Äù. The <code>signalfd</code> syscall allocates an 8 byte object to store a mask for the <code>signalfd</code> file, which can be specified by the user with some minor restrictions. The lifetime of the allocated object is tied to the <code>signalfd</code> file that is returned to the user and can be controlled easily by closing the file. Moreover, the first 8 bytes in this object can be changed by calling <code>signalfd</code> again with a different mask. This makes <code>signalfd</code> ideal for my purpose.</p>
<p>To hijack the freelist pointer, I have to do the following:</p>
<ol>
<li>Trigger the UAF bug and replaced the free&#8217;d <code>dma_fence</code> object with a fake <code>dma_fence</code> object allocated via <code>sendmsg</code> such that <code>dma_fence_free</code> will be called to free this fake object with <code>kfree_rcu</code>.</li>
<li>Spray the heap with <code>signalfd</code> to allocate another object at the same address as the <code>sendmsg</code> object after it was freed. </li>
<li>Free the <code>sendmsg</code> object so that the freelist pointer is written to the mask of the <code>signalfd</code> object in step two. </li>
<li>Modify the mask of the <code>signalfd</code> object so that the freelist pointer now points to an address of my choice, then spray the heap again to allocate objects at that address.</li>
</ol>
<p>If I set the address of the freelist pointer to the address of an ion buffer that I control, then subsequent allocations will place objects in the ion buffer, which I can then access and modify at any time. This gives me a very strong primitive in that I can read and modify any object that I allocate. Essentially, I can fake my own kernel heap in a region where I have both read and write access.</p>
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Kernel-6.png?resize=960%2C540" alt="" width="960" height="540" class="alignnone size-full wp-image-65553 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Kernel-6.png?resize=960%2C540?w=960 960w, https://github.blog/wp-content/uploads/2022/06/Kernel-6.png?resize=960%2C540?w=300 300w, https://github.blog/wp-content/uploads/2022/06/Kernel-6.png?resize=960%2C540?w=768 768w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1" /></div>
<p>The main hurdle to this plan comes from the combination of <code>kfree_rcu</code> and the fact that the CPU running <code>dma_fence_put</code> will be temporarily trapped in a busy loop after <code>kfree_rcu</code> is called. Recall from the previous section that, until I am able to exit the loop by writing the address of the <code>temp</code> list to the <code>next</code> pointer of one of the fake <code>kgsl_timeline_fence::node</code> objects, the loop will be running. This, in particular, means that once <code>kfree_rcu</code> is called and <code>dma_fence_put</code> is exited, the loop will continue to process the other fake <code>dma_fence</code> objects on the CPU that is running <code>kfree_rcu</code>. As explained earlier, <code>kfree_rcu</code> does not immediately free an object, but rather schedules its removal. Most of the time, the free will actually happen on the same CPU that calls <code>kfree_rcu</code>. However, in this case, because the CPU running <code>kfree_rcu</code> is kept busy inside a <code>spinlock</code> by running the loop, the object will almost certainly not be free‚Äôd on that same CPU. Instead, a different CPU will be used to free the object. This causes a problem because the reliability of object replacement depends on the CPU that is used for freeing the object. When an object is freed on a CPU, the memory allocator will place it in a per CPU cache. An allocation that follows immediately on the same CPU will first look for free space in the CPU cache and is most likely going to replace that newly freed object. However, if the allocation happens on a different CPU, then it&#8217;ll most likely replace an object in the cache of a different CPU, rather than the newly freed object. Not knowing which CPU is responsible for freeing the object, together with the uncertainty of when the object is freed (because of the delay introduced by <code>kfree_rcu</code>) means that it may be difficult to replace the object. In practice, however, I was able to achieve reasonable results on the testing device (>70% success rate) with a rather simple scheme: Simply run a loop that spray objects on each CPU and repeat the spraying in intervals to account for the uncertainty in the timing. There is probably room for improvement here to make the exploit more reliable.</p>
<p>Another slight modification used in the exploit was to also replace the <code>sendmsg</code> objects after they are freed with another round of <code>signalfd</code> heap spray. This is to ensure that those <code>sendmsg</code> objects don&#8217;t accidentally get replaced by objects that I don&#8217;t control which may interfere with the exploit, as well as to make it easier to identify the actual corrupted object.</p>
<p>Now that I can hijack the freelist and redirect new object allocations to the ion buffer that I can freely access at any time, I need to turn this into an arbitrary memory read and write primitive.</p>
<h2>The Device Memory Mirroring Attack</h2>
<p>Kernel drivers often need to map memory to the user space, and as such, there are often structures that contain pointers to the <code>page</code> struct or the <code>sg_table</code> struct. These structures often hold pointers to pages that would be mapped to user space when, for example, <code>mmap</code> is called. This makes them very good corruption targets. For example, the <code>ion_buffer</code> object that I have already used is available on all Android devices. It has a <code>sg_table</code> struct that contains information about the pages that will get mapped to user space when <code>mmap</code> is used.</p>
<p>Apart from being widely available and accessible from untrusted apps, <code>ion_buffer</code> objects also solve a few other problems, so in what follows, I&#8217;ll use the freelist hijacking primitive above to allocate an <code>ion_buffer</code> struct in an ion buffer backing store that I have arbitrary read and write access to. By doing so, I can freely corrupt the data in all of the <code>ion_buffer</code> structs that are allocated. To avoid confusion, from now on, I&#8217;ll use the term &#8220;fake kernel heap&#8221; to indicate the ion buffer backing store that I use as the fake kernel heap and <code>ion_buffer</code> struct as the structures that I allocate in the fake heap for use as corruption targets.</p>
<p>The general idea here is that, by allocating <code>ion_buffer</code> structs in the fake kernel heap, I&#8217;ll be able to modify the <code>ion_buffer</code> struct and replace its <code>sg_table</code> with controlled data. The <code>sg_table</code> structure contains a <code>scatterlist</code> structure that represents a collection of pages that back the <code>ion_buffer</code> structure:</p>
<pre><code>struct sg_table {
    struct scatterlist *sgl;    /* the list */
    unsigned int nents;     /* number of mapped entries */
    unsigned int orig_nents;    /* original size of list */
};

struct scatterlist {
    unsigned long   page_link;
    unsigned int    offset;
    unsigned int    length;
    dma_addr_t  dma_address;
#ifdef CONFIG_NEED_SG_DMA_LENGTH
    unsigned int    dma_length;
#endif
};
</code></pre>
<p>The <code>page_link</code> field in the <code>scatterlist</code> is an encoded form of a <code>page</code> pointer, indicating the actual <code>page</code> where the backing store of the <code>ion_buffer</code> structure is:</p>
<pre><code>static inline struct page *sg_page(struct scatterlist *sg)
{
#ifdef CONFIG_DEBUG_SG
    BUG_ON(sg_is_chain(sg));
#endif
    return (struct page *)((sg)-&gt;page_link &amp; ~(SG_CHAIN | SG_END));
}
</code></pre>
<p>When <code>mmap</code> is called, the page encoded by <code>page_link</code> will be mapped to user space:</p>
<pre><code>int ion_heap_map_user(struct ion_heap *heap, struct ion_buffer *buffer,
              struct vm_area_struct *vma)
{
    struct sg_table *table = buffer-&gt;sg_table;
    ...
    for_each_sg(table-&gt;sgl, sg, table-&gt;nents, i) {
        struct page *page = sg_page(sg);
        ...
        //Maps pages to user space
        ret = remap_pfn_range(vma, addr, page_to_pfn(page), len,
                      vma-&gt;vm_page_prot);
        ...
    }

    return 0;
}
</code></pre>
<p>As the <code>page</code> pointer is simply a logical shift of the physical address of the page followed by a constant linear offset (see the definition of <a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/5e3cf80f1b6a12fcf54b007f3c9f235f35b9b7f1/arch/arm64/include/asm/memory.h#L285">phys_to_page</a>), being able to control <code>page_link</code> allows me to map an arbitrary page to user space. For many devices, this would be sufficient to achieve arbitrary kernel memory read and write because the kernel image is mapped at a fixed physical address (KASLR randomizes the <em>virtual</em> address offset from this fixed physical address), so there is no need to worry about KASLR when working with physical addresses.</p>
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Kernel-7.png?resize=960%2C540" alt="" width="960" height="540" class="alignnone size-full wp-image-65554 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Kernel-7.png?resize=960%2C540?w=960 960w, https://github.blog/wp-content/uploads/2022/06/Kernel-7.png?resize=960%2C540?w=300 300w, https://github.blog/wp-content/uploads/2022/06/Kernel-7.png?resize=960%2C540?w=768 768w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1" /></div>
<p>Samsung devices, however, do KASLR differently. Instead of mapping the kernel image to a fixed physical address, the physical address of the kernel image is randomized (strictly speaking, the intermediate physical address as perceived by the kernel, which is not the real physical address but rather a virtual address given by the hypervisor) instead. So in our case, I still need to leak an address to defeat KASLR. With the fake kernel heap, however, this is fairly easy to achieve. An <code>ion_buffer</code> object contains a pointer to an <code>ion_heap</code>, which is responsible for allocating the backing stores for the <code>ion_buffer</code>:</p>
<pre><code>struct ion_buffer {
    struct list_head list;
    struct ion_heap *heap;
    ...
};
</code></pre>
<p>While the <code>ion_heap</code> is not an global object in the kernel image, each <code>ion_heap</code> contains an <code>ion_heap_ops</code> field, which points to the corresponding &#8220;vtable&#8221; of the specific <code>ion_heap</code> object:</p>
<pre><code>struct ion_heap {
    struct plist_node node;
    enum ion_heap_type type;
    struct ion_heap_ops *ops;
    ...
}
</code></pre>
<p>The <code>ops</code> field in the above is a global object in the kernel image. If I can read <code>ion_buffer-&gt;heap-&gt;ops</code>, then I&#8217;m also able to get an address to defeat KASLR and translate addresses in the kernel image to physical addresses. This can be done as follows:</p>
<p>1) First locate the <code>ion_buffer</code> struct in the fake kernel heap. This can be done using the <code>flags</code> field in the <code>ion_buffer</code>:</p>
<pre><code>struct ion_buffer {
    struct list_head list;
    struct ion_heap *heap;
    unsigned long flags;
    ...
</code></pre>
<p>which is a 4 byte value passed from the parameters of the <code>ION_IOC_ALLOC</code> ioctl when the <code>ion_buffer</code> is created. I can set these to specific &#8220;magic&#8221; values and search for them in the fake kernel heap.</p>
<p>2) Once the <code>ion_buffer</code> struct is located, read its <code>heap</code> pointer. This will be a virtual address in the low memory area outside of the kernel image, and as such, its physical address can be obtained by applying a constant offset.</p>
<p>3) Once the physical address of the corresponding <code>ion_heap</code> object is obtained, modify the <code>sg_table</code> of the <code>ion_buffer</code> so that its backing store points to the page containing the <code>ion_heap</code>. 4. Call <code>mmap</code> on the <code>ion_buffer</code> file descriptor, this will map the page containing the <code>ion_heap</code> to user space. This page can then be read directly from user space to obtain the <code>ops</code> pointer, which will give the KASLR offset.</p>
<p>The use of the <code>ion_buffer</code> struct also solves another problem. While the fake kernel heap is convenient, it is not perfect. Whenever an object in the fake kernel heap is freed, <code>kfree</code> will check whether the page containing the object is a single page slab from the SLUB allocator by using the <code>PageSlab</code> check. If the check fails, then the <code>PageCompound</code> check will be performed to check whether the page is part of a bigger slab.</p>
<pre><code>void kfree(const void *x)
{
    struct page *page;
    void *object = (void *)x;

    trace_kfree(_RET_IP_, x);

    if (unlikely(ZERO_OR_NULL_PTR(x)))
        return;

    page = virt_to_head_page(x);
    if (unlikely(!PageSlab(page))) {     //&lt;-------- check if the page allocated is a single page slab
        unsigned int order = compound_order(page);

        BUG_ON(!PageCompound(page));     //&lt;-------- check if the page is allocated as part of a multipage slab
        ...
    }
    ...
}
</code></pre>
<p>As these checks are performed on the <code>page</code> struct itself, which contains metadata to the page, they will fail and cause the kernel to crash whenever an object is freed. This can be fixed by using the arbitrary read and write primitives that I now have to overwrite the respective metadata in the <code>page</code> struct (the address of a <code>page</code> struct that corresponds to a physical address is simply a logical shift of the physical address followed by a translation of a fixed offset, so I can map the <code>page</code> containing the <code>page</code> struct to user space and modify its content). It would, however, be simpler if I can ensure that the objects that occupy the fake kernel heap never get freed. Before an <code>ion_buffer</code> struct is freed, <code>ion_buffer_destroy</code> is called:</p>
<pre><code>int ion_buffer_destroy(struct ion_device *dev, struct ion_buffer *buffer)
{
    ...
    heap = buffer-&gt;heap;
    ...
    if (heap-&gt;flags &amp; ION_HEAP_FLAG_DEFER_FREE)
        ion_heap_freelist_add(heap, buffer);    //&lt;--------- does not free immediately
    else
        ion_buffer_release(buffer);

    return 0;
}
</code></pre>
<p>If the <code>ion_heap</code> contains the flag <code>ION_HEAP_FLAG_DEFER_FREE</code>, then the <code>ion_buffer</code> will not be freed immediately, but instead gets added to the <code>free_list</code> of the <code>ion_heap</code> using <code>ion_heap_freelist_add</code>. The <code>ion_buffer</code> objects added to this list will only be freed at a later stage when needed and only if the <code>ION_HEAP_FLAG_DEFER_FREE</code> flag is set. Normally, of course, the <code>ION_HEAP_FLAG_DEFER_FREE</code> does not change over the lifetime of the <code>ion_heap</code>, but with our arbitrary memory write primitive, I can simply add <code>ION_HEAP_FLAG_DEFER_FREE</code> to the <code>ion_heap-&gt;flags</code>, free the <code>ion_buffer</code>, and then remove <code>ION_HEAP_FLAG_DEFER_FREE</code> again and the <code>ion_buffer</code> will just get stuck in the freelist of the <code>ion_heap</code> and never get freed. Moreover, the page containing the <code>ion_heap</code> object is already mapped for the purpose of defeating KASLR, so toggling the flag is fairly trivial. By spraying the fake kernel heap so that it is filled with <code>ion_buffer</code> objects and their dependents, I can ensure that those objects are never freed and avoid the kernel crash.</p>
<h2>Bypassing SELinux</h2>
<p>When SELinux is enabled, it can be run in either the <code>permissive</code> mode or the <code>enforcing</code> mode. When in <code>permissive</code> mode, it will only audit and log unauthorized accesses but will not block them. The mode in which SELinux is run is controlled by the <code>selinux_enforcing</code> variable. If this variable is zero, then SELinux is run in <code>permissive</code> mode. Normally, variables that are critical to the security of the system are protected by Samsung&#8217;s Kernel Data Protection (KDP), by marking them as read-only using the <code>__kdp_ro</code> or the <code>__rkp_ro</code> attribute. This attribute indicates that the variable is in a read-only page and its modification is guarded by hypervisor calls. However, to my surprise, it seems that Samsung has forgotten to protect this variable (<a href="https://securitylab.github.com/research/qualcomm_npu/">again?!</a>) in the 5.x branch Qualcomm kernel:</p>
<pre><code>//In security/selinux/hooks.c
#ifdef CONFIG_SECURITY_SELINUX_DEVELOP
static int selinux_enforcing_boot;
int selinux_enforcing;
</code></pre>
<p>So, I can just overwrite <code>selinux_enforcing</code> to zero and set SELinux to the <code>permissive</code> mode. While there are other means to bypass SELinux (such as the one used in this <a href="https://github.com/chompie1337/s8_2019_2215_poc/blob/master/poc/selinux_bypass.c">exploit</a> by Valentina Palmiotti) that work more universally, a shortcut at this point is more than welcome, so I‚Äôll just set the <code>selinux_enforcing</code> variable.</p>
<h2>Running arbitrary root commands using ret2kworker(TM)</h2>
<p>A well-known problem with getting root on Samsung devices is the protection imposed by the Samsung&#8217;s RKP (Realtime Kernel Protection). A common way to gain root on Android devices is to overwrite the credentials of our own process with the root credentials. However, Samsung&#8217;s RKP write protects the credentials of each process, so that is not possible here. In my <a href="https://securitylab.github.com/research/qualcomm_npu/">last exploit</a>, I was able to execute arbitrary code as root because the particular UAF exploited led to a controlled function pointer being executed in code run by a <code>kworker</code>, which is run as root. In that exploit, I was able to corrupt objects that are then added to a work queue, which was then consumed by a <code>kworker</code> and executed by running a function supplied as a function pointer. This made it relatively easy to run arbitrary functions as root.</p>
<p>Of course, with arbitrary memory read and write primitives, it is possible to simply add objects to one of these work queues (which are basically linked lists containing <code>work</code> structs) and wait for a <code>kworker</code> to pick up the work. As it turns out, many of these work queues are indeed static global objects, with fixed addresses in the kernel image:</p>
<pre><code>ffffffc012c8f7e0 D system_wq
ffffffc012c8f7e8 D system_highpri_wq
ffffffc012c8f7f0 D system_long_wq
ffffffc012c8f7f8 D system_unbound_wq
ffffffc012c8f800 D system_freezable_wq
ffffffc012c8f808 D system_power_efficient_wq
ffffffc012c8f810 D system_freezable_power_efficient_wq
</code></pre>
<p>So, it is relatively straightforward to add entries to these work queues and have a <code>kworker</code> pick up the work. However, because of <code>kCFI</code>, I would only be able to call functions with the following signatures:</p>
<pre><code>void (func*)(struct work_struct *work)
</code></pre>
<p>The problem is whether I can find a powerful enough function to run. It turns out to be fairly simple. The function <code><a href="https://git.codelinaro.org/clo/la/kernel/msm-5.4/-/blob/5e3cf80f1b6a12fcf54b007f3c9f235f35b9b7f1/kernel/umh.c#L190">call_usermodehelper_exec_work</a></code>, which is commonly used in kernel exploits to run shell commands, fits the bill and will run a shell command supplied by me. So by modifying, say, the <code>system_unbound_wq</code> and adding an entry to it that holds a pointer to <code>call_usermodehelper_exec_work</code>, I can bypass both Samsung&#8217;s RKP and kCFI to run arbitrary commands as root.</p>
<p>The exploit can be found <a href="https://github.com/github/securitylab/tree/main/SecurityExploits/Android/Qualcomm/CVE-2022-22057">here</a> with some setup notes.</p>
<h2>Conclusions</h2>
<p>In this post, I exploited a UAF with fairly typical primitives and examined how various mitigations affected the exploit. While in the end, I was able to bypass all the mitigations and develop an exploit that is no less reliable than another one that I did <a href="https://securitylab.github.com/research/one_day_short_of_a_fullchain_android/">last year</a>, the mitigations did force the exploit to take a very different, and longer path.</p>
<p>The biggest hurdle was the kCFI, which turned a relatively straightforward exploit into a rather complex one. As explained in the post, the UAF bug offers many primitives to execute an arbitrary function pointer. Combined with a separate information leak (which I happen to have, and is pending disclosure) the bug would have been trivial to exploit as in the case of the <a href="https://securitylab.github.com/research/qualcomm_npu/">NPU bugs</a> I wrote about last year. Instead, the combination of Samsung&#8217;s RKP and kCFI made this impossible and forced me to look into an alternative path, which is far less straightforward.</p>
<p>On the other hand, many of the techniques introduced here, such as the ones in &#8220;The ultimate fake object store&#8221; and &#8220;The device memory mirroring attack&#8221; can be readily standardized to turn common primitives into arbitrary memory read and write. As we&#8217;ve seen, having arbitrary memory read and write, even with the restrictions imposed by Samsung&#8217;s RKP, is already powerful enough for many purposes. In this regard, it seems to me that the effect of kCFI may be to shift exploitation techniques in favor of certain primitives over others rather than rendering many bugs unexploitable. It is, after all, as many have said, a mitigation that happens fairly late in the process.</p>
<p>A rather underrated mitigation, perhaps, is automatic variable initialization. While this mitigation mainly targets vulnerabilities that exploit uninitialized variables, we&#8217;ve seen that it also prevents partial object replacement, which is a common exploitation technique. In fact, it nearly broke the exploit if it hadn‚Äôt been for a piece of luck where I was able to leak the address of a stack variable (see &#8220;Escaping an infinite loop&#8221;). Not only does this mitigation kill a whole class of bugs, but it also breaks some useful exploit primitives.</p>
<p>We&#8217;ve also seen how primitives in manipulating the kernel scheduler enabled us to widen many race windows to allow time for object replacements. This allowed me to overcome the problem posed by the delayed free caused by <code>kfree_rcu</code> (not a mitigation), without significantly compromising the reliability. It seems that, in the context of the kernel, mitigating a UAF with a quarantine and delayed free of objects (an approach adopted by the Scudo allocator for Android user processes) may not be that effective after all.</p>
<h2>Disclosure practices, patching time, and patch gapping</h2>
<p>I reported this vulnerability to Qualcomm on November 16 2021 and they publicly disclosed it in early May 2022. It took about six months, which seems to be the standard time Qualcomm takes between receiving a report and publicly disclosing it. In the past, the long disclosure times have led to some researchers disclosing full exploits before vulnerabilities were patched in public (see, for example, <a href="https://googleprojectzero.blogspot.com/2020/09/attacking-qualcomm-adreno-gpu.html">this</a>). Qualcomm employs a rather ‚Äúspecial‚Äù disclosure process, in which it normally discloses the patch of a vulnerability privately to its customers within 90 days (this is indicated by the &#8220;Customer Notified Date&#8221; in <a href="https://docs.qualcomm.com/product/publicresources/securitybulletin/may-2022-bulletin.html">Qualcomm&#8217;s advisories</a>), but the patch may only be fully integrated and publicly disclosed several months after that.</p>
<p>While this practice gives customers (OEM) time to patch the vulnerabilities before they become public, it creates an opportunity for what is known as &#8220;patch gapping.&#8221; As patches are first disclosed privately to customers, it means that some customers may decide to apply the patch before it is publicly disclosed, while others may wait until the patch is fully integrated and disclosed publicly and then apply the monthly patch. For example, <a href="https://source.codeaurora.org/quic/la/kernel/msm-5.4/commit/?id=6c58829f4428f2564833743de7135f4451077e75">this patch</a> was applied to the Samsung S21 in July 2021 (by inspecting the <a href="https://opensource.samsung.com/">Samsung open source</a> code), but only publicly disclosed as CVE-2021-30305 in October 2021. By comparing firmware between different vendors, it may be possible to discover vulnerabilities that are privately patched by one vendor, but not the other. As the private disclosure time and public disclosure time are often a few months apart, this leaves ample time to develop an exploit. Moreover, the <a href="https://source.codeaurora.org/quic/la/kernel/msm-5.4/commit/?id=339824df70a0e9e08f2a7151b776d72421050f04">patch</a> for the vulnerability described in this post was publicly visible sometime in December 2021 with a very clear commit message, leaving a five‚Äêmonth (or two‚Äêmonth for private disclosure) gap in between. While the exploit is complex, a skilled hacking team could easily have developed and deployed it within that time window. I hope that Qualcomm will improve their patching time or reduce the gap between their private and public disclosure time.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://github.blog/2022-06-15-git-merge-2022/">Git Merge 2022</a> <span>Git Merge, the conference dedicated to bringing the Git community together returns
                     on September 14-15 in Chicago, Illinois.
                     We hope you can join us in-person for a full day of technical talks, plus a day of
                     pre-conference workshops for Git users of all levels. Tickets and registration details
                     will be available soon. As always, we will donate all proceeds from Git Merge tickets
                     to the Software Freedom Conservancy to support their work in improving and defending
                     free and open source software, including the Git project itself.
                     Call for speakers
                     In the meantime, we‚Äôre accepting proposals starting today through Sunday, July 10.
                     Simply submit a proposal, and we‚Äôll email you back by Friday, July 22. For more information
                     on the kind of talks we‚Äôre seeking, please check out our Call for Speakers.
                     A teaser of what to expect. Caren Garcia pictured delivering a talk at Git Merge in
                     2017.
                     Sponsorship
                     Git Merge is about the Git community, not about any one company. Therefore, the conference
                     would not be possible without the help of our sponsors and community partners. If
                     you‚Äôre interested in sponsorship opportunities at Git Merge, please see the prospectus
                     for more details.
                     See you in Chicago!
                     </span></summary><time datetime="2022-06-16T02:29:33+02:00">Thu, 16 Jun 2022 00:29</time><article><p><a href="http://git-merge.com/">Git Merge</a>, the conference dedicated to bringing the Git community together returns on September 14-15 in Chicago, Illinois.</p>
<p>We hope you can join us in-person for a full day of technical talks, plus a day of pre-conference workshops for Git users of all levels. Tickets and registration details will be available soon. As always, we will donate all proceeds from Git Merge tickets to the <a href="https://sfconservancy.org/">Software Freedom Conservancy</a> to support their work in improving and defending free and open source software, including the Git project itself.</p>
<h2>Call for speakers</h2>
<p>In the meantime, we‚Äôre accepting proposals starting today through Sunday, July 10. Simply submit a proposal, and we‚Äôll email you back by Friday, July 22. For more information on the kind of talks we‚Äôre seeking, please check out our <a href="https://sessionize.com/git-merge-2022">Call for Speakers</a>.</p>
<figure id="attachment_65727"  class="wp-caption aligncenter mx-0"><img loading="lazy" class="width-fit size-full wp-image-65727 width-fit" src="https://github.blog/wp-content/uploads/2022/06/gitmerge-community.jpeg" alt="Caren Garcia pictured giving a talk at Git Merge 2017" srcset="https://github.blog/wp-content/uploads/2022/06/gitmerge-community.jpeg?w=1600 1600w, https://github.blog/wp-content/uploads/2022/06/gitmerge-community.jpeg?w=300 300w, https://github.blog/wp-content/uploads/2022/06/gitmerge-community.jpeg?w=768 768w, https://github.blog/wp-content/uploads/2022/06/gitmerge-community.jpeg?w=1024 1024w, https://github.blog/wp-content/uploads/2022/06/gitmerge-community.jpeg?w=1536 1536w" sizes="(max-width: 1000px) 100vw, 1000px" /><figcaption class="text-mono color-fg-muted mt-14px f5-mktg">A teaser of what to expect. Caren Garcia pictured delivering a talk at Git Merge in 2017.</figcaption></figure>
<h2>Sponsorship</h2>
<p>Git Merge is about the Git community, not about any one company. Therefore, the conference would not be possible without the help of our sponsors and community partners. If you‚Äôre interested in sponsorship opportunities at Git Merge, please see the <a href="https://event-sponsorship.github.com/events/gitmerge/">prospectus</a> for more details.</p>
<p>See you in Chicago!</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://github.blog/2022-06-15-prebuilding-codespaces-is-generally-available/">Prebuilding codespaces is generally available</a> <span>Prebuilding codespaces is generally available 
                     We‚Äôre excited to announce that the ability to prebuild codespaces is now generally
                     available. As a quick recap, a prebuilt codespace serves as a ‚Äúready-to-go‚Äù template
                     where your source code, editor extensions, project dependencies, commands, and configurations
                     have already been downloaded, installed, and applied, so that you don‚Äôt have to wait
                     for these tasks to finish each time you create a new codespace. This helps significantly
                     speed up codespace creations‚Äìespecially for complex or large codebases.
                     Codespaces prebuilds entered public beta earlier this year, and we received a ton
                     of feedback around experiences you loved, as well as areas we could improve on. We‚Äôre
                     excited to share those with you today.
                     How Vanta doubled its engineering team with Codespaces
                     With Codespaces prebuilds, Vanta was able to significantly reduce the time it takes
                     for a developer to onboard. This was important, because Vanta‚Äôs Engineering Team doubled
                     in size in the last few months. When a new developer joined the company, they would
                     need to manually set up their dev environment; and once it was stable, it would diverge
                     within weeks, often making testing difficult.
                     ‚ÄúBefore Codespaces, the onboarding process was tedious. Instead of taking two days,
                     now it only takes a minute for a developer to access a pristine, steady-state environment,
                     thanks to prebuilds,‚Äù said Robbie Ostrow, Software Engineering Manager at Vanta. ‚ÄúNow,
                     our dev environments are ephemeral, always updated and ready to go.‚Äù
                     Scheduled prebuilds to manage GitHub Actions usage
                     Repository admins can now decide how and when they want to update prebuild configurations
                     based on their team‚Äôs needs. While creating or updating prebuilds for a given repository
                     and branch, admins can choose from three available triggers to initiate a prebuild
                     refresh:
                     
                     Every push (default): Prebuild configurations are updated on every push made to the
                     given branch. This ensures that new Codespaces always contain the latest configuration,
                     including any recently added or updated dependencies.
                     On configuration change: Prebuild configurations are updated every time configuration
                     files change. This ensures that the latest configuration changes appear in new Codespaces.
                     The Actions workflow that generates the prebuild template will run less often, so
                     this option will use fewer Actions minutes.
                     Scheduled: With this setting, you can have your prebuild configurations update on
                     a custom schedule. This can help further reduce the consumption of Actions minutes.
                     
                     
                     With increased control, repository admins can make more nuanced trade-offs between
                     &amp;#8220;environment freshness&amp;#8221; and Actions usage. For example, an admin working
                     in a large organization may decide to update their prebuild configuration every hour
                     rather than on every push to get the most economy and efficiency out of their Actions
                     usage.
                     
                     Failure notifications for efficient monitoring
                     Many of you shared with us the need to be notified when a prebuild workflow fails,
                     primarily to be able to watch and fix issues if and when they arise. We heard you
                     loud and clear and have added support for failure notifications within prebuilds.
                     With this, repository admins can specify a set of individuals or teams to be informed
                     via email in case a workflow associated with that prebuild configuration fails. This
                     will enable team leads or developers in charge of managing prebuilds for their repository
                     to stay up to date on any failures without having to manually monitor them. This will
                     also enable them to make fixes faster, thus ensuring developers working on the project
                     continue getting prebuilt codespaces.
                     
                     To help with investigating failures, we‚Äôve also added the ability to disable a prebuild
                     configuration in the instance repository admins would like to temporarily pause the
                     update of a prebuild template while fixing an underlying issue.
                     
                     Improved ‚Äòprebuild readiness‚Äô indicators
                     Lastly, to help you identify prebuild-enabled machine types to avail fast creations,
                     we have introduced a ‚Äòprebuild in progress‚Äô label in addition to the ‚Äòprebuild ready‚Äô
                     label in cases where a prebuild template creation for a given branch is in progress.
                     
                     Billing for prebuilds
                     With general availability, organizations will be billed for Actions minutes required
                     to run prebuild associated workflows and storage of templates associated with each
                     prebuild configuration for a given repository and region. As an admin, you can download
                     the usage report for your organization to get a detailed view of prebuild-associated
                     Actions and storage costs for your organization-owned repositories to help you manage
                     usage.
                     Alongside enabling billing, we‚Äôve also added a functionality to help manage prebuild-associated
                     storage costs based on the valuable feedback that you shared with us.
                     Template retention to manage storage costs
                     Repository administrators can now specify the number of prebuild template versions
                     to be retained with a default template retention setting of two. A default of two
                     means that the codespace service will retain the latest and one previous prebuild
                     template version by default, thus helping you save on storage for older versions.
                     
                     How to get started
                     Prebuilds are generally available for the GitHub Enterprise Cloud and GitHub Team
                     plans as of today.
                     As an organization or repository admin, you can head over to your repository‚Äôs settings
                     page and create prebuild configurations under the ‚ÄúCodespaces‚Äù tab. As a developer,
                     you can create a prebuilt codespace by heading over to a prebuild-enabled branch in
                     your repository and selecting a machine type that has the ‚Äúprebuild ready‚Äù label on
                     it.
                     Here‚Äôs a link to the prebuilds documentation to help you get started!
                     Post general availability, we‚Äôll continue working on functionalities to enable prebuilds
                     on monorepos and multi-repository scenarios based on your feedback. If you have any
                     feedback to help improve this experience, be sure to post it on our GitHub Discussions
                     forum.
                     </span></summary><time datetime="2022-06-15T19:00:27+02:00">Wed, 15 Jun 2022 17:00</time><article><h1>Prebuilding codespaces is generally available <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f389.png" alt="üéâ" class="wp-smiley" style="height: 1em; max-height: 1em;" /></h1>
<p>We‚Äôre excited to announce that the ability to prebuild codespaces is now generally available. As a quick recap, a prebuilt codespace serves as a ‚Äúready-to-go‚Äù template where your source code, editor extensions, project dependencies, commands, and configurations have already been downloaded, installed, and applied, so that you don‚Äôt have to wait for these tasks to finish each time you create a new codespace. This helps significantly speed up codespace creations‚Äìespecially for complex or large codebases.</p>
<p><a href="https://github.blog/2022-02-23-codespaces-largest-repositories-faster/">Codespaces prebuilds entered public beta </a>earlier this year, and we received a ton of feedback around experiences you loved, as well as areas we could improve on. We‚Äôre excited to share those with you today.</p>
<h2>How Vanta doubled its engineering team with Codespaces</h2>
<p>With Codespaces prebuilds, <a href="http://vanta.com?utm_source=blog&#038;utm_medium=blog&#038;utm_campaign=github-codespaces-launch-blog">Vanta</a> was able to significantly reduce the time it takes for a developer to onboard. This was important, because Vanta‚Äôs Engineering Team doubled in size in the last few months. When a new developer joined the company, they would need to manually set up their dev environment; and once it was stable, it would diverge within weeks, often making testing difficult.</p>
<blockquote><p>‚ÄúBefore Codespaces, the onboarding process was tedious. Instead of taking two days, now it only takes a minute for a developer to access a pristine, steady-state environment, thanks to prebuilds,‚Äù said Robbie Ostrow, Software Engineering Manager at Vanta. ‚ÄúNow, our dev environments are ephemeral, always updated and ready to go.‚Äù</p></blockquote>
<h2>Scheduled prebuilds to manage GitHub Actions usage</h2>
<p>Repository admins can now decide how and when they want to update prebuild configurations based on their team‚Äôs needs. While creating or updating prebuilds for a given repository and branch, admins can choose from three available triggers to initiate a prebuild refresh:</p>
<ul>
<li><strong>Every push (default):</strong> Prebuild configurations are updated on every push made to the given branch. This ensures that new Codespaces always contain the latest configuration, including any recently added or updated dependencies.</li>
<li><strong>On configuration change:</strong> Prebuild configurations are updated every time configuration files change. This ensures that the latest configuration changes appear in new Codespaces. The Actions workflow that generates the prebuild template will run less often, so this option will use fewer Actions minutes.</li>
<li><strong>Scheduled:</strong> With this setting, you can have your prebuild configurations update on a custom schedule. This can help further reduce the consumption of Actions minutes. </li>
</ul>
<p>With increased control, repository admins can make more nuanced trade-offs between &#8220;environment freshness&#8221; and Actions usage. For example, an admin working in a large organization may decide to update their prebuild configuration every hour rather than on every push to get the most economy and efficiency out of their Actions usage.</p>
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Codespaces-1.png?resize=1024%2C569" alt="" width="1024" height="569" class="alignnone size-full wp-image-65690 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Codespaces-1.png?resize=1024%2C569?w=1228 1228w, https://github.blog/wp-content/uploads/2022/06/Codespaces-1.png?resize=1024%2C569?w=300 300w, https://github.blog/wp-content/uploads/2022/06/Codespaces-1.png?resize=1024%2C569?w=768 768w, https://github.blog/wp-content/uploads/2022/06/Codespaces-1.png?resize=1024%2C569?w=1024 1024w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /></div>
<h2>Failure notifications for efficient monitoring</h2>
<p>Many of you shared with us the need to be notified when a prebuild workflow fails, primarily to be able to watch and fix issues if and when they arise. We heard you loud and clear and have added support for failure notifications within prebuilds. With this, repository admins can specify a set of individuals or teams to be informed via email in case a workflow associated with that prebuild configuration fails. This will enable team leads or developers in charge of managing prebuilds for their repository to stay up to date on any failures without having to manually monitor them. This will also enable them to make fixes faster, thus ensuring developers working on the project continue getting prebuilt codespaces.</p>
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Codespaces-2.png?resize=1024%2C245" alt="" width="1024" height="245" class="alignnone size-full wp-image-65691 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Codespaces-2.png?resize=1024%2C245?w=1240 1240w, https://github.blog/wp-content/uploads/2022/06/Codespaces-2.png?resize=1024%2C245?w=300 300w, https://github.blog/wp-content/uploads/2022/06/Codespaces-2.png?resize=1024%2C245?w=768 768w, https://github.blog/wp-content/uploads/2022/06/Codespaces-2.png?resize=1024%2C245?w=1024 1024w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /></div>
<p>To help with investigating failures, we‚Äôve also added the ability to disable a prebuild configuration in the instance repository admins would like to temporarily pause the update of a prebuild template while fixing an underlying issue.</p>
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Codespaces-3.png?resize=631%2C198" alt="" width="631" height="198" class="alignnone size-full wp-image-65692 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Codespaces-3.png?resize=631%2C198?w=631 631w, https://github.blog/wp-content/uploads/2022/06/Codespaces-3.png?resize=631%2C198?w=300 300w" sizes="(max-width: 631px) 100vw, 631px" data-recalc-dims="1" /></div>
<h2>Improved ‚Äòprebuild readiness‚Äô indicators</h2>
<p>Lastly, to help you identify prebuild-enabled machine types to avail fast creations, we have introduced a ‚Äòprebuild in progress‚Äô label in addition to the ‚Äòprebuild ready‚Äô label in cases where a prebuild template creation for a given branch is in progress.</p>
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Codespaces-4.png?resize=604%2C139" alt="" width="604" height="139" class="alignnone size-full wp-image-65693 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Codespaces-4.png?resize=604%2C139?w=604 604w, https://github.blog/wp-content/uploads/2022/06/Codespaces-4.png?resize=604%2C139?w=300 300w, https://github.blog/wp-content/uploads/2022/06/Codespaces-4.png?resize=604%2C139?w=600 600w" sizes="(max-width: 604px) 100vw, 604px" data-recalc-dims="1" /></div>
<h2>Billing for prebuilds</h2>
<p>With general availability, organizations will be billed for <a href="https://docs.github.com/en/billing/managing-billing-for-github-actions/about-billing-for-github-actions">Actions minutes </a>required to run prebuild associated workflows and <a href="https://docs.github.com/en/billing/managing-billing-for-github-codespaces/about-billing-for-codespaces#calculating-storage-usage">storage of templates</a> associated with each prebuild configuration for a given repository and region. As an admin, you can <a href="https://docs.github.com/en/billing/managing-billing-for-github-codespaces/viewing-your-codespaces-usage">download the usage report for your organization </a>to get a detailed view of prebuild-associated Actions and storage costs for your organization-owned repositories to help you manage usage.</p>
<p>Alongside enabling billing, we‚Äôve also added a functionality to help manage prebuild-associated storage costs based on the valuable feedback that you shared with us.</p>
<h3>Template retention to manage storage costs</h3>
<p>Repository administrators can now specify the number of prebuild template versions to be retained with a <a href="aka.ms/ghcs-prebuild-retention">default template retention setting of two</a>. A default of two means that the codespace service will retain the latest and one previous prebuild template version by default, thus helping you save on storage for older versions.</p>
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Codespaces-5.png?resize=775%2C151" alt="" width="775" height="151" class="alignnone size-full wp-image-65694 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Codespaces-5.png?resize=775%2C151?w=775 775w, https://github.blog/wp-content/uploads/2022/06/Codespaces-5.png?resize=775%2C151?w=300 300w, https://github.blog/wp-content/uploads/2022/06/Codespaces-5.png?resize=775%2C151?w=768 768w" sizes="(max-width: 775px) 100vw, 775px" data-recalc-dims="1" /></div>
<h1>How to get started</h1>
<p>Prebuilds are generally available for the GitHub Enterprise Cloud and GitHub Team plans as of today.</p>
<p>As an organization or repository admin, you can head over to your repository‚Äôs settings page and create prebuild configurations under the ‚ÄúCodespaces‚Äù tab. As a developer, you can create a prebuilt codespace by heading over to a prebuild-enabled branch in your repository and selecting a machine type that has the ‚Äúprebuild ready‚Äù label on it.</p>
<p>Here‚Äôs a link to the <a href="https://aka.ms/ghcs-prebuild">prebuilds documentation</a> to help you get started!</p>
<p>Post general availability, we‚Äôll continue working on functionalities to enable prebuilds on monorepos and multi-repository scenarios based on your feedback. If you have any feedback to help improve this experience, be sure to post it on our GitHub <a href="https://aka.ms/ghcs-feedback">Discussions forum</a>.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://github.blog/2022-06-15-github-now-publishes-malware-advisories-in-the-github-advisory-database/">GitHub now publishes malware advisories in the GitHub Advisory Database</a> <span>Mistakes are the most common cause of vulnerabilities in open source software, but
                     they are not the only cause. Bad actors also attempt to introduce malicious software,
                     known as malware, into open source. Details about malware can be hard to keep track
                     of because malware is typically taken down and is not eligible for the usual disclosure
                     process where vulnerabilities are assigned a CVE and placed in the National Vulnerability
                     Database (NVD).
                     GitHub discovers malware through multiple means such as automated scanning, security
                     research, and community discovery. Starting today, after a malicious package is removed,
                     we will also create an advisory to document the malware in the GitHub Advisory Database.
                     Dependabot alerts for malware advisories
                     Malware advisories already power Dependabot alerts for impacted GitHub users. If you
                     already use Dependabot, you‚Äôre covered with no additional action. To receive alerts
                     on malware advisories and vulnerabilities, you can enable Dependabot by selecting
                     enable all under the ‚ÄúCode security and analysis&amp;#8221; tab.
                     ‚Äã‚Äã
                     
                     Learn more about GitHub supply chain security solutions
                     The GitHub Advisory Database publishes security advisories that power GitHub‚Äôs supply
                     chain security capabilities, including Dependabot alerts and Dependabot security updates.
                     The data is licensed under a Creative Commons license and has been since the database‚Äôs
                     inception, making it forever free and usable by the community. For more information
                     about our supply chain security capabilities, check out the following pages:
                     
                     Learn about managing vulnerable dependencies on GitHub
                     Visit the GitHub Advisory Database
                     
                     </span></summary><time datetime="2022-06-15T17:46:26+02:00">Wed, 15 Jun 2022 15:46</time><article><p>Mistakes are the most common cause of vulnerabilities in open source software, but they are not the only cause. Bad actors also attempt to introduce malicious software, known as malware, into open source. Details about malware can be hard to keep track of because malware is typically taken down and is not eligible for the usual disclosure process where vulnerabilities are assigned a CVE and placed in the National Vulnerability Database (NVD).</p>
<p>GitHub discovers malware through multiple means such as automated scanning, security research, and community discovery. Starting today, after a malicious package is removed, we will also create an advisory to document the malware in the GitHub Advisory Database.</p>
<h2>Dependabot alerts for malware advisories</h2>
<p>Malware advisories already power Dependabot alerts for impacted GitHub users. If you already use Dependabot, you‚Äôre covered with no additional action. To receive alerts on malware advisories and vulnerabilities, you can <a href="https://docs.github.com/en/code-security/dependabot/dependabot-alerts/configuring-dependabot-alerts#managing-dependabot-alerts-for-your-personal-account">enable Dependabot</a> by selecting enable all under the ‚ÄúCode security and analysis&#8221; tab.<br />
‚Äã‚Äã<br />
<div class="image-frame image-frame-full border rounded-2 overflow-hidden d-flex flex-row flex-justify-center" style="background: #EAEEF2"><img loading="lazy" src="https://github.blog/wp-content/uploads/2022/06/Code-security-and-analysis.png?resize=1024%2C417" alt="" width="1024" height="417" class="alignnone size-full wp-image-65710 width-fit" srcset="https://github.blog/wp-content/uploads/2022/06/Code-security-and-analysis.png?resize=1024%2C417?w=1600 1600w, https://github.blog/wp-content/uploads/2022/06/Code-security-and-analysis.png?resize=1024%2C417?w=300 300w, https://github.blog/wp-content/uploads/2022/06/Code-security-and-analysis.png?resize=1024%2C417?w=768 768w, https://github.blog/wp-content/uploads/2022/06/Code-security-and-analysis.png?resize=1024%2C417?w=1024 1024w, https://github.blog/wp-content/uploads/2022/06/Code-security-and-analysis.png?resize=1024%2C417?w=1536 1536w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /></div></p>
<h2>Learn more about GitHub supply chain security solutions</h2>
<p>The GitHub Advisory Database publishes security advisories that power GitHub‚Äôs supply chain security capabilities, including Dependabot alerts and Dependabot security updates. The data is licensed under a Creative Commons license and has been since the database‚Äôs inception, making it forever free and usable by the community. For more information about our supply chain security <a href="https://docs.github.com/en/code-security/supply-chain-security/managing-vulnerabilities-in-your-projects-dependencies/editing-security-advisories-in-the-github-advisory-database">capabilities</a>, check out the following pages:</p>
<ul>
<li>Learn about <a href="https://docs.github.com/en/code-security/supply-chain-security/managing-vulnerabilities-in-your-projects-dependencies/about-managing-vulnerable-dependencies">managing vulnerable dependencies on GitHub</a></li>
<li>Visit the <a href="https://github.com/advisories">GitHub Advisory Database</a></li>
</ul>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://github.blog/2022-06-14-accelerating-github-theme-creation-with-color-tooling/">Accelerating GitHub theme creation with color tooling</a> <span>Dark mode is no longer a nice-to-have feature. It‚Äôs an expectation. Yet, for many
                     teams, implementing dark mode is still a daunting task.
                     Creating a palette for dark interfaces is not as simple as inverting colors and complexity
                     increases if your team is planning multiple themes. Many people find themselves using
                     a combination of disjointed color tools, which can be a painful experience.
                     GitHub dark mode (unveiled at GitHub Universe in December 2020) was the result of
                     trial and error, copy and paste, as well as back and forth in a Figma file (with more
                     than 370,000 layers!).
                     
                     A screenshot of the Figma file we made while designing GitHub dark mode
                     A few months after shipping dark mode, we began working on a dark high contrast theme
                     to provide an option that maximizes legibility. While we were designing this new theme,
                     we set out to improve our workflow by building an experimental tool to solve some
                     of the challenges we encountered while designing the original dark color palette.
                     We‚Äôre calling our experimental color tool Primer Prism.
                     
                     A sneak peek of Primer Prism
                     Part of GitHub&amp;#8217;s Primer ecosystem, Primer Prism is a tool for creating and maintaining
                     cohesive, consistent, and accessible color palettes. It allows us to:
                     
                     Create or import color scales.
                     Adjust colors in a perceptually uniform color space (HSLuv).
                     Check contrast of color pairs.
                     Edit lightness curves across multiple color scales at once.
                     Export color palettes to production-ready code (JSON).
                     
                     Our workflow
                     Our improved workflow for creating color palettes with Primer Prism is an iterative
                     cycle comprised of three steps:
                     
                     &nbsp;Defining tones
                     Choosing colors
                     Testing colors
                     
                     Defining tones
                     We start by defining the color palette‚Äôs tonal character and contrast needs:
                     
                     How light or dark should the background be?
                     What should the contrast ratio between the foreground and background be?
                     
                     Although each palette will have a unique tonal character, we are mindful that all
                     palettes meet contrast accessibility guidelines.
                     In Primer Prism, we start a new color palette by creating a new color scale and adjusting
                     the lightness curve. In this phase, we‚Äôre only concerned with lightness and contrast.
                     We‚Äôll revisit hue and saturation later.
                     As we change the lightness of each color, Primer Prism checks the contrast of potential
                     color pairings in the scale using the WCAG 2 standard.
                     
                     Dragging lightness sliders up and down to adjust the lightness curve of a scale
                     Primer Prism also allows us to share curves across multiple color scales. So, when
                     we have more scales, we can quickly change the tonal character of the entire color
                     palette by adjusting a single lightness curve.
                     
                     Adjusting the lightness curve of all color scales at once
                     Primer Prism uses the HSLuv color space to ensure that the lightness values are perceptually
                     uniform across the entire palette. In the HSLuv color space, two colors with the same
                     lightness value will look equally bright.
                     Choosing colors
                     Next, we define the overall color character of our palette:
                     
                     What hues do we need (for example: red, blue, green, etc.)?
                     How vibrant do we want the colors to be?
                     
                     We create a color scale for every hue using the same lightness curve we made earlier.
                     Then, we compare and adjust the base color (the fifth step in the scale) across all
                     the color scales until the palette feels cohesive and consistent.
                     
                     A side-by-side comparison of every color scale
                     After deciding on the base color for each scale, we fine-tune the tints (lighter colors)
                     and shades (darker colors). Blue, for example, shifts towards green hues in the tints
                     and purple hues in the shades.
                     
                     The hue, saturation, and lightness curves of the blue color scale
                     Fine-tuning color scales is more of an art than a science and often requires many
                     micro-adjustments before the colors ‚Äúfeel right.‚Äù Check out Color in UI Design: A
                     (Practical) Framework by Eric D. Kennedy to learn more about the fundamentals of designing
                     color scales.
                     Testing colors
                     To test our colors in real-world scenarios, we export the palette from Primer Prism
                     as a JSON object and add it to Primer Primitives, our repository of design tokens.
                     We use pre-releases of the Primer Primitives package to test new color palettes on
                     GitHub.com.
                     
                     The dark color palette applied to GitHub.com
                     What&amp;#8217;s next
                     We used Primer Prism to design several new color palettes, accelerating the creation
                     of dark high contrast, light high contrast, and colorblind themes for GitHub. Next,
                     we plan to improve our tooling to support the following key workflows.
                     Visual testing workflow
                     We plan to integrate visual testing directly into Primer Prism. Currently, visual
                     testing of color palettes happens outside of Primer Prism, typically in Figma or production
                     applications. However, we want a more convenient way to visualize how the colors will
                     look when mapped to functional variables and used in actual user interfaces.
                     GitHub workflow
                     We plan to integrate GitHub into Primer Prism. Right now, it‚Äôs a hassle to edit existing
                     color palettes because Primer Prism is not connected to the GitHub repository where
                     we store color variables (Primer Primitives). A GitHub integration will allow us to
                     directly pull from and push to the Primer Primitives repository.
                     Figma workflow
                     Our designers use Figma to explore and test new design ideas. We plan to create a
                     Figma plugin to seamlessly integrate Primer Prism into their workflow.
                     Try it out
                     Primer Prism is open source and available for anyone to use at primer.style/prism.
                     We‚Äôd love to hear what you think. If you have feedback, please create an issue or
                     start a discussion in the GitHub repository.
                     Warning: Primer Prism is experimental. Expect bugs and breaking changes as we continue
                     to iterate.
                     Thanks
                     Huge shout-out to @Juliusschaeper, @auareyou, @edokoa, and @broccolini for their incredible
                     work on the GitHub dark mode color palette.
                     Primer Prism was inspired by many existing color tools:
                     &amp;#8211; ColorBox by Lyft
                     &amp;#8211; Components AI
                     &amp;#8211; Huetone by Alexey Ardov
                     &amp;#8211; Leonardo by Adobe
                     &amp;#8211; Palettte by Gabriel Adorf
                     &amp;#8211; Palx by Brent Jackson
                     &amp;#8211; Scale by Hayk An
                     Further reading
                     
                     Designing accessible color systems by Daryl Koopersmith and Wilson Miner
                     Color in UI Design: A (Practical) Framework by Erik D. Kennedy 
                     The Secret Lives Of Color Systems by Diana Mounter
                     Perceptually uniform color spaces by Rune Madsen
                     
                     </span></summary><time datetime="2022-06-14T21:04:41+02:00">Tue, 14 Jun 2022 19:04</time><article><p>Dark mode is no longer a nice-to-have feature. It‚Äôs an expectation. Yet, for many teams, implementing dark mode is still a daunting task.</p>
<p>Creating a palette for dark interfaces is not as simple as inverting colors and complexity increases if your team is planning multiple themes. Many people find themselves using a combination of disjointed color tools, which can be a painful experience.</p>
<p>GitHub dark mode (<a href="https://twitter.com/github/status/1336362679506784256">unveiled</a> at GitHub Universe in December 2020) was the result of trial and error, copy and paste, as well as back and forth in a Figma file (with more than 370,000 layers!).</p>
<p><img src="https://i0.wp.com/user-images.githubusercontent.com/4608155/171808924-082e1177-d53b-4f10-82ce-a3a142223745.png?ssl=1" alt="A screenshot of the Figma file we made while designing GitHub dark mode" data-recalc-dims="1" /><br />
<em>A screenshot of the Figma file we made while designing GitHub dark mode</em></p>
<p>A few months after shipping dark mode, we began working on a dark high contrast theme to provide an option that maximizes legibility. While we were designing this new theme, we set out to improve our workflow by building an experimental tool to solve some of the challenges we encountered while designing the original dark color palette.</p>
<p>We‚Äôre calling our experimental color tool <a href="https://primer.style/prism">Primer Prism</a>.</p>
<p><img src="https://i0.wp.com/user-images.githubusercontent.com/4608155/172266834-b0ad41be-25bf-41d7-b33c-d6c6ce0ebb1d.gif?ssl=1" alt="A sneak peek of Primer Prism" data-recalc-dims="1" /><br />
<em>A sneak peek of Primer Prism</em></p>
<p>Part of GitHub&#8217;s <a href="https://primer.style">Primer ecosystem</a>, Primer Prism is a tool for creating and maintaining cohesive, consistent, and accessible color palettes. It allows us to:</p>
<ul>
<li>Create or import color scales.</li>
<li>Adjust colors in a <a href="https://programmingdesignsystems.com/color/perceptually-uniform-color-spaces/index.html">perceptually uniform color space</a> (<a href="https://www.hsluv.org/">HSLuv</a>).</li>
<li>Check contrast of color pairs.</li>
<li>Edit lightness curves across multiple color scales at once.</li>
<li>Export color palettes to production-ready code (JSON).</li>
</ul>
<h2>Our workflow</h2>
<p>Our improved workflow for creating color palettes with Primer Prism is an iterative cycle comprised of three steps:</p>
<ol>
<li>¬†Defining tones</li>
<li>Choosing colors</li>
<li>Testing colors</li>
</ol>
<h3>Defining tones</h3>
<p>We start by defining the color palette‚Äôs tonal character and contrast needs:</p>
<ul>
<li>How light or dark should the background be?</li>
<li>What should the contrast ratio between the foreground and background be?</li>
</ul>
<p>Although each palette will have a unique tonal character, we are mindful that all palettes meet <a href="https://webaim.org/articles/contrast/">contrast accessibility guidelines</a>.</p>
<p>In Primer Prism, we start a new color palette by creating a new color scale and adjusting the lightness curve. In this phase, we‚Äôre only concerned with lightness and contrast. We‚Äôll revisit hue and saturation later.</p>
<p>As we change the lightness of each color, Primer Prism checks the contrast of potential color pairings in the scale using the <a href="https://webaim.org/resources/contrastchecker/">WCAG 2 standard</a>.</p>
<p><img src="https://i0.wp.com/user-images.githubusercontent.com/4608155/173153801-1313930f-59f2-4cb5-a8c6-3e695f931783.gif?ssl=1" alt="Dragging lightness sliders up and down to adjust the lightness curve of a scale" data-recalc-dims="1" /><br />
<em>Dragging lightness sliders up and down to adjust the lightness curve of a scale</em></p>
<p>Primer Prism also allows us to share curves across multiple color scales. So, when we have more scales, we can quickly change the tonal character of the entire color palette by adjusting a single lightness curve.</p>
<p><img src="https://i0.wp.com/user-images.githubusercontent.com/4608155/173153889-35a37219-28e0-41aa-b0cb-d3888cf2474b.gif?ssl=1" alt="Adjusting the lightness curve of all color scales at once" data-recalc-dims="1" /><br />
<em>Adjusting the lightness curve of all color scales at once</em></p>
<p>Primer Prism uses the <a href="https://www.hsluv.org/">HSLuv</a> color space to ensure that the lightness values are <a href="https://programmingdesignsystems.com/color/perceptually-uniform-color-spaces/index.html">perceptually uniform</a> across the entire palette. In the HSLuv color space, two colors with the same lightness value will look equally bright.</p>
<h3>Choosing colors</h3>
<p>Next, we define the overall color character of our palette:</p>
<ul>
<li>What hues do we need (for example: red, blue, green, etc.)?</li>
<li>How vibrant do we want the colors to be?</li>
</ul>
<p>We create a color scale for every hue using the same lightness curve we made earlier. Then, we compare and adjust the base color (the fifth step in the scale) across all the color scales until the palette feels cohesive and consistent.</p>
<p><img src="https://i0.wp.com/user-images.githubusercontent.com/4608155/172440045-7c7f4101-d524-4881-87a2-41a6691a05ca.png?ssl=1" alt="A side-by-side comparison of every color scale" data-recalc-dims="1" /><br />
<em>A side-by-side comparison of every color scale</em></p>
<p>After deciding on the base color for each scale, we fine-tune the tints (lighter colors) and shades (darker colors). Blue, for example, shifts towards green hues in the tints and purple hues in the shades.</p>
<p><img src="https://i0.wp.com/user-images.githubusercontent.com/4608155/172440110-580b9395-22b3-41c7-bc22-575fa0dc5ff0.png?ssl=1" alt="The hue, saturation, and lightness curves of the blue color scale" data-recalc-dims="1" /><br />
<em>The hue, saturation, and lightness curves of the blue color scale</em></p>
<p>Fine-tuning color scales is more of an art than a science and often requires many micro-adjustments before the colors ‚Äúfeel right.‚Äù Check out <a href="https://learnui.design/blog/color-in-ui-design-a-practical-framework.html">Color in UI Design: A (Practical) Framework</a> by <a href="https://erikdkennedy.com/">Eric D. Kennedy</a> to learn more about the fundamentals of designing color scales.</p>
<h3>Testing colors</h3>
<p>To test our colors in real-world scenarios, we export the palette from Primer Prism as a JSON object and add it to <a href="https://github.com/primer/primitives">Primer Primitives</a>, our repository of design tokens. We use pre-releases of the Primer Primitives package to test new color palettes on GitHub.com.</p>
<p><img src="https://i0.wp.com/user-images.githubusercontent.com/4608155/172440156-fc06a408-4926-4a56-bfd8-70b32a7bdd63.png?ssl=1" alt="The dark color palette applied to GitHub.com" data-recalc-dims="1" /><br />
<em>The dark color palette applied to GitHub.com</em></p>
<h2>What&#8217;s next</h2>
<p>We used Primer Prism to design several new color palettes, accelerating the creation of <a href="https://github.blog/changelog/2021-08-25-dark-high-contrast-theme-ga/">dark high contrast</a>, <a href="https://github.blog/changelog/2022-02-08-light-high-contrast-theme-ga/">light high contrast</a>, and <a href="https://github.blog/changelog/2021-09-29-colorblind-themes-beta/">colorblind</a> themes for GitHub. Next, we plan to improve our tooling to support the following key workflows.</p>
<h3>Visual testing workflow</h3>
<p>We plan to integrate visual testing directly into Primer Prism. Currently, visual testing of color palettes happens outside of Primer Prism, typically in Figma or production applications. However, we want a more convenient way to visualize how the colors will look when mapped to functional variables and used in actual user interfaces.</p>
<h3>GitHub workflow</h3>
<p>We plan to integrate GitHub into Primer Prism. Right now, it‚Äôs a hassle to edit existing color palettes because Primer Prism is not connected to the GitHub repository where we store color variables (<a href="https://github.com/primer/primitives">Primer Primitives</a>). A GitHub integration will allow us to directly pull from and push to the Primer Primitives repository.</p>
<h3>Figma workflow</h3>
<p>Our designers use Figma to explore and test new design ideas. We plan to create a Figma plugin to seamlessly integrate Primer Prism into their workflow.</p>
<h2>Try it out</h2>
<p>Primer Prism is <a href="https://github.com/primer/prism">open source</a> and available for anyone to use at <a href="https://primer.style/prism">primer.style/prism</a>.</p>
<p>We‚Äôd love to hear what you think. If you have feedback, please create an issue or start a discussion in the <a href="https://github.com/primer/prism">GitHub repository</a>.</p>
<p><strong>Warning:</strong> Primer Prism is experimental. Expect bugs and breaking changes as we continue to iterate.</p>
<h2>Thanks</h2>
<p>Huge shout-out to <a href="https://github.com/Juliusschaeper">@Juliusschaeper</a>, <a href="https://github.com/auareyou">@auareyou</a>, <a href="https://github.com/edokoa">@edokoa</a>, and <a href="https://github.com/broccolini">@broccolini</a> for their incredible work on the GitHub dark mode color palette.</p>
<p>Primer Prism was inspired by many existing color tools:<br />
&#8211; <a href="https://lyft-colorbox.herokuapp.com/">ColorBox by Lyft</a><br />
&#8211; <a href="https://components.ai/">Components AI</a><br />
&#8211; <a href="https://huetone.ardov.me/">Huetone by Alexey Ardov</a><br />
&#8211; <a href="https://leonardocolor.io/theme.html">Leonardo by Adobe</a><br />
&#8211; <a href="https://palettte.app/">Palettte by Gabriel Adorf</a><br />
&#8211; <a href="https://palx.jxnblk.com/">Palx by Brent Jackson</a><br />
&#8211; <a href="https://hihayk.github.io/scale">Scale by Hayk An</a></p>
<h2>Further reading</h2>
<ul>
<li><a href="https://stripe.com/blog/accessible-color-systems">Designing accessible color systems by Daryl Koopersmith and Wilson Miner</a></li>
<li><a href="https://learnui.design/blog/color-in-ui-design-a-practical-framework.html">Color in UI Design: A (Practical) Framework by Erik D. Kennedy </a></li>
<li><a href="https://vimeo.com/showcase/6133041/video/351297451">The Secret Lives Of Color Systems by Diana Mounter</a></li>
<li><a href="https://programmingdesignsystems.com/color/perceptually-uniform-color-spaces/index.html">Perceptually uniform color spaces by Rune Madsen</a></li>
</ul>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://github.blog/2022-06-14-how-can-the-united-states-build-its-open-source-software-policy/">How can the United States build its Open Source Software policy?</a> <span>The world runs on open source, which serves as the foundation for globally interconnected
                     digital infrastructure. With an estimated 97% of codebases containing open source,
                     both the private and public sectors depend on the maintenance of open source software
                     (OSS), but governments have untapped potential as leaders in the OSS community. Currently,
                     most engagement has centered around security. Earlier this year, GitHub attended the
                     White House meeting on software security where participants discussed the unique value
                     and security challenges of OSS. While securing OSS is an important concern, full stack
                     federal OSS policy requires dialogue between policymakers, practitioners, and wonks.
                     To this end, GitHub was proud to attend the June 8 roundtable discussion, From Procurement
                     to Policy: Towards OSS Infrastructure, in Washington, D.C.
                     The event kicked off with a bold question posed by Harvard Business School professor,
                     Frank Nagle: ‚ÄúWhat would a federal open source policy agenda look like?‚Äù Building
                     off of his Brookings policy brief, Nagle proposed actionable steps the public sector
                     could take to cultivate a thriving OSS ecosystem, from understanding and supporting
                     the open source ecosystem to establishing a federal Open Source Program Office (OSPO).
                     A federal OSPO could coordinate OSS efforts across agencies, including those among
                     the thousands of government organizations around the world already using GitHub for
                     government work. As the meeting progressed, speakers addressed the political and organizational
                     challenges to federal open source publishing and discussed how renewed commitment
                     to 18F and the Federal Source Code Policy could bolster US support for digital infrastructure.
                     Digital infrastructure is global, and US OSS policy should reflect the globally interconnected
                     nature of OSS contributors while building domestic capacity. To that end, OpenForum
                     Europe presented a survey of EU open source policy objectives and how governmental
                     OSPOs can learn from the private sector. Explaining the architecture of Germany‚Äôs
                     Sovereign Tech Fund, speakers argued that OSS policy is about more than simply public
                     sector adoption, but also using strategic funding and legal mechanisms to support
                     healthy and secure OSS. When it comes to OSS support, sustained funding is crucial.
                     When support for the Open Technology Fund was at risk, GitHub joined the public call
                     to renew support. Finally, the Digital Impact Alliance presented the Digital Public
                     Goods Charter, a multistakeholder effort to enable developing countries to build safe,
                     trusted, and inclusive digital public infrastructure at scale. GitHub affirms that
                     OSS is a public good, and has launched research projects to define a list of platform
                     usage metrics by country for international development, public policy, and economics
                     disciplines, as well as measure the economic impact of open source.
                     Building off of the ideas posed by speakers, participants joined breakout sessions
                     where they posed and led the topics of discussion. While the speakers posed bold questions,
                     the participants discussed detailed solutions, from where to broker global cooperation
                     on OSS policy to how to implement federal open source programs that survive changes
                     of administration. The ideas posed by the roundtable were a rich background for bringing
                     a diverse group of people‚Äìwhether it be the founder of a startup or a lifelong policymaker‚Äìin
                     one room to imagine the transformative potential of open source. GitHub intends to
                     foster open source policy champions in all sectors and levels of government. We‚Äôve
                     advocated for open source collaboration to policymakers in Congress, at the US Copyright
                     Office, and at events, including RightsCon and the Internet Governance Forum. These
                     efforts continue with people getting together, thinking big, and talking it out, and
                     on June 8, 2022, we did just that.
                     If you have ideas for the next roundtable, or want to be added to the mailing list,
                     get in touch via the US Open Source Policy Google Group.
                     </span></summary><time datetime="2022-06-14T18:55:42+02:00">Tue, 14 Jun 2022 16:55</time><article><p>The world runs on open source, which serves as the foundation for globally interconnected digital infrastructure. With an estimated <a href="https://www.synopsys.com/content/dam/synopsys/sig-assets/reports/rep-ossra-2022.pdf">97% of codebases containing open source</a>, both the private and public sectors depend on the maintenance of open source software (OSS), but governments have untapped potential as leaders in the OSS community. Currently, most engagement has centered around security. Earlier this year, GitHub attended the <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2022/01/13/readout-of-white-house-meeting-on-software-security/">White House meeting on software security</a> where participants discussed the unique value and security challenges of OSS. While securing OSS is an important concern, full stack federal OSS policy requires dialogue between policymakers, practitioners, and wonks. To this end, GitHub was proud to attend the June 8 roundtable discussion, <em><a href="https://www.eventbrite.com/e/from-procurement-to-policy-towards-oss-infrastructure-tickets-318956335987">From Procurement to Policy: Towards OSS Infrastructure</a></em>, in Washington, D.C.</p>
<p>The event kicked off with a bold question posed by Harvard Business School professor, Frank Nagle: ‚ÄúWhat would a federal open source policy agenda look like?‚Äù Building off of his <a href="https://www.brookings.edu/research/strengthening-digital-infrastructure-a-policy-agenda-for-free-and-open-source-software/">Brookings policy brief</a>, Nagle proposed actionable steps the public sector could take to cultivate a thriving OSS ecosystem, from understanding and supporting the open source ecosystem to establishing a federal Open Source Program Office (OSPO). A federal OSPO could coordinate OSS efforts across agencies, including those among the thousands of government organizations around the world already using <a href="https://government.github.com/community/">GitHub for government work.</a> As the meeting progressed, speakers addressed the political and organizational challenges to federal open source publishing and discussed how renewed commitment to <a href="https://github.com/18F/open-source-policy/blob/master/policy.md">18F</a> and the <a href="https://www.cio.gov/2016/08/11/peoples-code.html">Federal Source Code Policy</a> could bolster US support for digital infrastructure.</p>
<p>Digital infrastructure is global, and US OSS policy should reflect the globally interconnected nature of OSS contributors while building domestic capacity. To that end, <a href="https://openforumeurope.org/">OpenForum Europe</a> presented a survey of EU open source policy objectives and <a href="https://openforumeurope.org/what-can-governmental-ospos-learn-from-the-private-sector-ones/">how governmental OSPOs can learn from the private sector</a>. Explaining the architecture of Germany‚Äôs <a href="https://sovereigntechfund.de/en">Sovereign Tech Fund</a>, speakers argued that OSS policy is about more than simply public sector adoption, but also using strategic funding and legal mechanisms to support healthy and secure OSS. When it comes to OSS support, sustained funding is crucial. When support for the Open Technology Fund was at risk, <a href="https://github.blog/2020-07-02-github-supports-the-open-technology-fund/">GitHub joined the public call to renew support</a>. Finally, the Digital Impact Alliance presented the <a href="https://dial.global/charter/">Digital Public Goods Charter</a>, a multistakeholder effort to enable developing countries to build safe, trusted, and inclusive digital public infrastructure at scale. GitHub affirms that OSS is a public good, and has launched research projects to <a href="https://github.blog/2021-08-31-request-for-proposals-defining-standardized-github-metrics/">define a list of platform usage metrics by country for international development, public policy, and economics disciplines</a>, as well as <a href="https://github.blog/2022-01-20-open-source-creates-value-but-how-do-you-measure-it/">measure the economic impact of open source</a>.</p>
<p>Building off of the ideas posed by speakers, participants joined breakout sessions where they posed and led the topics of discussion. While the speakers posed bold questions, the participants discussed detailed solutions, from where to broker global cooperation on OSS policy to how to implement federal open source programs that survive changes of administration. The ideas posed by the roundtable were a rich background for bringing a diverse group of people‚Äìwhether it be the founder of a startup or a lifelong policymaker‚Äìin one room to imagine the transformative potential of open source. GitHub intends to foster open source policy champions in all sectors and levels of government. We‚Äôve advocated for open source collaboration to policymakers in Congress, at the US Copyright Office, and at events, including RightsCon and the <a href="https://github.blog/2021-12-13-github-at-the-un-internet-governance-forum/">Internet Governance Forum</a>. These efforts continue with people getting together, thinking big, and talking it out, and on June 8, 2022, we did just that.</p>
<p>If you have ideas for the next roundtable, or want to be added to the mailing list, get in touch via the <a href="https://groups.google.com/g/us-oss-policy">US Open Source Policy Google Group</a>.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://github.blog/2022-06-13-open-source-monthly-june-2022-edition/">Open Source Monthly: June 2022 Edition</a> <span>I hope you enjoyed the May edition of Open Source Monthly, which showcased Sharing
                     Excess, a project solving food scarcity with food surplus, and Mulheres no mundo da
                     tecnologia, a non-code project written in Portuguese. This month, I&amp;#8217;m exploring
                     an often overlooked topic within open source‚Äìopen hardware. While open source software
                     makes source code accessible to everyone, open hardware allows individuals and organizations
                     to use, modify, and view schematics, blueprints, logic designs, and Computer Aided
                     Design (CAD) drawings under the correct licenses.
                     According to Opensource.com, open hardware is &amp;#8220;a set of design principles and
                     legal practices, not a specific type of object. The term can, therefore, refer to
                     any number of objects‚Äîautomobiles, chairs, computers, robots or even houses.&amp;#8221;
                     Check out the Open Hardware Association to learn more about open hardware.
                     My favorite project this month is an open hardware project known as the world‚Äôs first
                     E-Ink laptop.
                     Open source project of the month: Modos
                     Modos is a project, community, and company focused on software and hardware that designs
                     digital devices with respect for users&amp;#8217; time, attention, and well-being. The
                     community‚Äôs first project is the Modos Paper Laptop. The Modos Paper Laptop doesn‚Äôt
                     intend to replace gaming laptops or ultrabooks. Instead, it is filling a need in the
                     market for users who:
                     
                     Experience eye strain and eye fatigue
                     Prefer to work outside without the glare of the sun
                     Aim to practice deep work in a &amp;#8220;distraction-free&amp;#8221; environment
                     Are unable to use their laptops due to sensitivity to blue light, among other reasons
                     
                     The Paper Laptop is built entirely with open source tools, including FreeCAD to develop
                     the chassis, or the outer shell of the laptop, and KiCad, a free software suite for
                     electronic design automation, to design the printed circuit board (PCB). Two other
                     projects are in the works: a Paper Monitor and a Development Board.
                     The Paper Monitor works as a standalone monitor; the Development Board will enable
                     early adopters to build their own E-Ink applications.
                     This is an image of the Development Board.
                     
                     			
                     		
                     After chatting with the maintainer via Twitter Spaces, I was elated to learn that
                     he envisions the organization will eventually produce an ecosystem of e-ink devices
                     (laptops, monitors, displays, and tablets), enabling users to unplug from technology
                     and live mindfully.
                     About the maintainer
                     Outside of his work in open hardware and software, Alexander Soto, fondly known as
                     Flex or Alex, works as an Expert-in-Residence at Resilient Coders, a nonprofit coding
                     bootcamp I attended. Alex recalls helping his brother fix and build computers as one
                     of the reasons he loves technology. However, he&amp;#8217;s interested in more than technology‚Äìand
                     has a passion for education and social justice.
                     His interests are evident in his career. He has worked as a labor rights organizer
                     and teacher. He also holds a master&amp;#8217;s in Computer Science. In his free time,
                     Alex explores human-computer interaction, humane technology, decentralization, and
                     open-source design. Unfortunately, his hobbies, studies, and occupation require him
                     to sit in front of a device for hours, which causes him to suffer from eye strain
                     and eye fatigue. To remedy this issue, Alex created Modos. You can read his blog posts
                     chronicling his wins and challenges developing E-Ink devices.
                     How to contribute
                     To continue bringing this vision to life, the team at Modos is currently solving the
                     following technical challenges:
                     
                     Building an open-source electrophoretic display controller (EPDC)
                     Constructing the interface of the electronic paper display (EPD)
                     Designing a laptop chassis with an aspect ratio of 4:3
                     Creating native E-Ink optimized applications
                     Low-level implementation of the necessary drivers and development of Wayland protocols
                     
                     If you&amp;#8217;re interested in contributing to this mission, Modos is looking for technologists
                     with various skills:
                     
                     UI/UX designers to design for their EPD
                     Software engineers with experience in Python, Docker, and TypeScript
                     Embedded developers
                     FPGA developers with an aptitude for Verilog and common bus architecture
                     Mechanical engineers to drive CAD development
                     
                     If the above roles aren&amp;#8217;t suitable for you, there are more ways to help! Modos
                     is aiming to reach an ambitious goal. They hope to have at least 50,000 individuals
                     interested in purchasing their devices and provide feedback within their Community
                     Pilot Program. To get started, fill out their Community Survey and apply to join their
                     Community Pilot Program.
                     Learn more about getting involved with Modos here. Stay up to date with their progress
                     by following Modos on Twitter and GitHub.
                     Why I love this project
                     I love this project because I&amp;#8217;m looking forward to working and typing out blog
                     posts on my porch without sun glare. I&amp;#8217;m half-kidding. More importantly, it&amp;#8217;s
                     exciting to see open source push the envelope of innovation to create accessible devices
                     for people who struggle with eye strain and experience difficulty using modern LCDs.
                     There&amp;#8217;s also a bit of sentimental value for me. I feel inspired to know that
                     someone who is from a similar background and community as me is creating a new ecosystem
                     of digital devices.
                     Star pick of the month: Theater.js
                     I chatted with GitHub Star and Head of Developer Experience and Education at Remote,
                     Cassidy Williams. Her favorite open source project this month is Theater.js. Theatre.js
                     is a JavaScript animation library with a GUI. It animates the DOM, WebGL, and any
                     other JavaScript variable. Cassidy shared, &amp;#8220;Theater lets you make some really
                     cool, high-quality motion graphics, both programmatically and visually! I love how
                     intuitive it is and how open their core team is to talking about their work.&amp;#8221;
                     Thank you, Cassidy, for sharing one of your favorite open source projects with us!
                     
                     How to contribute
                     Check out the website if you&amp;#8217;re looking to implement Theater.js into your next
                     project. However, if you want to contribute to this project, read through the repository
                     and contributing guidelines.
                     Stay in touch!
                     Are you interested in discovering more open source projects?
                     
                     Join me on GitHub&amp;#8217;s Twitch on Fridays at 1 pm ET as maintainers show me how
                     to use their open source projects.
                     Rewatch past conversations with maintainers on GitHub&amp;#8217;s YouTube channel.
                     Follow GitHub Community and me on Twitter to stay up to date with my Open Source Friday
                     Twitter Spaces.
                     Follow All In, a community dedicated to advancing diversity, equity, and inclusion
                     within open source, on Twitter.
                     Explore the events and resources GitHub planned for Maintainer Month.
                     
                     Stay tuned for our July edition of Open Source Monthly!
                     </span></summary><time datetime="2022-06-13T22:47:28+02:00">Mon, 13 Jun 2022 20:47</time><article><p>I hope you enjoyed the <a href="https://github.blog/2022-05-11-open-source-monthly-may-2022-edition/" target="_blank" rel="noopener">May edition of Open Source Monthly</a>, which showcased <a href="https://sharingexcess.com/">Sharing Excess</a>, a project solving food scarcity with food surplus, and <a href="https://github.com/morgannadev/mulherestecnologia">Mulheres no mundo da tecnologia</a>, a non-code project written in Portuguese. This month, I&#8217;m exploring an often overlooked topic within open source‚Äìopen hardware. While open source software makes source code accessible to everyone, open hardware allows individuals and organizations to use, modify, and view schematics, blueprints, logic designs, and Computer Aided Design (CAD) drawings under the correct licenses.</p>
<p>According to <a href="https://opensource.com/resources/what-open-hardware">Opensource.com</a>, open hardware is &#8220;a set of design principles and legal practices, not a specific type of object. The term can, therefore, refer to any number of objects‚Äîautomobiles, chairs, computers, robots or even houses.&#8221; Check out the <a href="https://www.oshwa.org/">Open Hardware Association</a> to learn more about open hardware.</p>
<p>My favorite project this month is an open hardware project known as the world‚Äôs first E-Ink laptop.</p>
<h2>Open source project of the month: Modos</h2>
<p><a href="https://www.modos.tech/">Modos</a> is a project, community, and company focused on software and hardware that designs digital devices with respect for users&#8217; time, attention, and well-being. The community‚Äôs first project is the Modos Paper Laptop. The Modos Paper Laptop doesn‚Äôt intend to replace gaming laptops or ultrabooks. Instead, it is filling a need in the market for users who:</p>
<ul>
<li>Experience eye strain and eye fatigue</li>
<li>Prefer to work outside without the glare of the sun</li>
<li>Aim to practice <a href="https://blog.doist.com/deep-work/">deep work</a> in a &#8220;distraction-free&#8221; environment</li>
<li>Are unable to use their laptops due to sensitivity to blue light, among other reasons</li>
</ul>
<p>The Paper Laptop is built entirely with open source tools, including <a href="https://www.freecadweb.org/">FreeCAD</a> to develop the chassis, or the outer shell of the laptop, and <a href="https://www.kicad.org/">KiCad</a>, a free software suite for electronic design automation, to design the printed circuit board (PCB). Two other projects are in the works: a Paper Monitor and a Development Board.<br />
The Paper Monitor works as a standalone monitor; the Development Board will enable early adopters to build their own E-Ink applications.</p>
<figure id="attachment_65659"  class="wp-caption aligncenter mx-0"><img loading="lazy" class="width-fit width-fit wp-image-65659 size-full" src="https://github.blog/wp-content/uploads/2022/06/mondo_1.png" alt="" srcset="https://github.blog/wp-content/uploads/2022/06/mondo_1.png?w=1600 1600w, https://github.blog/wp-content/uploads/2022/06/mondo_1.png?w=300 300w, https://github.blog/wp-content/uploads/2022/06/mondo_1.png?w=768 768w, https://github.blog/wp-content/uploads/2022/06/mondo_1.png?w=1024 1024w, https://github.blog/wp-content/uploads/2022/06/mondo_1.png?w=1536 1536w" sizes="(max-width: 1000px) 100vw, 1000px" /><figcaption class="text-mono color-fg-muted mt-14px f5-mktg">This is an image of the Development Board.</figcaption></figure>
<div class="mod-vh position-relative" style="height: 0; padding-bottom: calc((9 / 16)*100%);">
			<iframe loading="lazy" class="position-absolute top-0 left-0 width-full height-full" src="https://www.youtube.com/embed/Ds38T8wVuDg?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent" title="YouTube video player" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0"></iframe>
		</div>
<p>After chatting with the maintainer via Twitter Spaces, I was elated to learn that he envisions the organization will eventually produce an ecosystem of e-ink devices (laptops, monitors, displays, and tablets), enabling users to unplug from technology and live mindfully.</p>
<h2>About the maintainer</h2>
<p>Outside of his work in open hardware and software, <a href="https://twitter.com/alexsotodev" target="_blank" rel="noopener">Alexander Soto</a>, fondly known as Flex or Alex, works as an Expert-in-Residence at <a href="http://www.resilientcoders.org/">Resilient Coders</a>, a nonprofit coding bootcamp I attended. Alex recalls helping his brother fix and build computers as one of the reasons he loves technology. However, he&#8217;s interested in more than technology‚Äìand has a passion for education and social justice.</p>
<p>His interests are evident in his career. He has worked as a labor rights organizer and teacher. He also holds a master&#8217;s in Computer Science. In his free time, Alex explores human-computer interaction, humane technology, decentralization, and open-source design. Unfortunately, his hobbies, studies, and occupation require him to sit in front of a device for hours, which causes him to suffer from eye strain and eye fatigue. To remedy this issue, Alex created Modos. You can read his <a href="https://alexsoto.dev/building-an-e-ink-laptop.html">blog posts</a> chronicling his wins and challenges developing E-Ink devices.</p>
<h2>How to contribute</h2>
<p>To continue bringing this vision to life, the team at Modos is currently solving the following technical challenges:</p>
<ul>
<li>Building an open-source electrophoretic display controller (EPDC)</li>
<li>Constructing the interface of the electronic paper display (EPD)</li>
<li>Designing a laptop chassis with an aspect ratio of 4:3</li>
<li>Creating native E-Ink optimized applications</li>
<li>Low-level implementation of the necessary drivers and development of Wayland protocols</li>
</ul>
<p>If you&#8217;re interested in contributing to this mission, Modos is looking for technologists with various skills:</p>
<ul>
<li>UI/UX designers to design for their EPD</li>
<li>Software engineers with experience in Python, Docker, and TypeScript</li>
<li>Embedded developers</li>
<li>FPGA developers with an aptitude for Verilog and common bus architecture</li>
<li>Mechanical engineers to drive CAD development</li>
</ul>
<p>If the above roles aren&#8217;t suitable for you, there are more ways to help! Modos is aiming to reach an ambitious goal. They hope to have at least 50,000 individuals interested in purchasing their devices and provide feedback within their Community Pilot Program. To get started, fill out their <a href="https://db.modos.tech/form/dWQUf1b4yaNX_s03V41VUeyvmFOIoqmL6Z2qHh8Ff-c">Community Survey</a> and apply to join their <a href="https://db.modos.tech/form/CNQpoDi7Dlb3Ux55gweYQ-EZz99Qlkjo2gH4GE0elrE">Community Pilot Program</a>.</p>
<p>Learn more about getting involved with Modos here. Stay up to date with their progress by following Modos on Twitter and GitHub.</p>
<h2>Why I love this project</h2>
<p>I love this project because I&#8217;m looking forward to working and typing out blog posts on my porch without sun glare. I&#8217;m half-kidding. More importantly, it&#8217;s exciting to see open source push the envelope of innovation to create accessible devices for people who struggle with eye strain and experience difficulty using modern LCDs. There&#8217;s also a bit of sentimental value for me. I feel inspired to know that someone who is from a similar background and community as me is creating a new ecosystem of digital devices.</p>
<h2>Star pick of the month: Theater.js</h2>
<p>I chatted with GitHub Star and Head of Developer Experience and Education at Remote, <a href="https://www.google.com/url?q=https://github.com/cassidoo&amp;sa=D&amp;source=docs&amp;ust=1655142744860410&amp;usg=AOvVaw2-Ftfy0ARHOYxLsKgdEDwH">Cassidy Williams</a>. Her favorite open source project this month is <a href="https://www.theatrejs.com/">Theater.js</a>. Theatre.js is a JavaScript animation library with a GUI. It animates the DOM, WebGL, and any other JavaScript variable. Cassidy shared, &#8220;Theater lets you make some really cool, high-quality motion graphics, both programmatically and visually! I love how intuitive it is and how open their core team is to talking about their work.&#8221;</p>
<p>Thank you, Cassidy, for sharing one of your favorite open source projects with us! <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/2b50.png" alt="‚≠ê" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<h2>How to contribute</h2>
<p>Check out <a href="https://www.theatrejs.com/">the website</a> if you&#8217;re looking to implement Theater.js into your next project. However, if you want to contribute to this project, read through the <a href="https://github.com/theatre-js/theatre">repository</a> and contributing guidelines<a href="https://github.com/theatre-js/theatre/blob/main/CONTRIBUTING.md">.</a></p>
<h2>Stay in touch!</h2>
<p>Are you interested in discovering more open source projects?</p>
<ul>
<li>Join me on <a href="https://www.twitch.tv/github">GitHub&#8217;s Twitch</a> on Fridays at 1 pm ET as maintainers show me how to use their open source projects.</li>
<li>Rewatch past conversations with maintainers on <a href="https://www.youtube.com/playlist?list=PL0lo9MOBetEFmtstItnKlhJJVmMghxc0P">GitHub&#8217;s YouTube channel</a>.</li>
<li>Follow <a href="https://twitter.com/GitHubCommunity">GitHub Community</a> and <a href="https://twitter.com/blackgirlbytes">me</a> on Twitter to stay up to date with my Open Source Friday Twitter Spaces.</li>
<li>Follow <a href="https://twitter.com/AllInOpenSource">All In</a>, a community dedicated to advancing diversity, equity, and inclusion within open source, on Twitter.</li>
<li>Explore the events and resources GitHub planned for <a href="https://maintainermonth.github.com/">Maintainer Month</a>.</li>
</ul>
<p>Stay tuned for our July edition of Open Source Monthly!</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://github.blog/2022-06-10-implementing-a-robust-digital-identity/">Implementing a robust digital identity</a> <span>This post is part seven of GitHub Security Lab‚Äôs series on the OWASP Top 10 Proactive
                     Controls, where we provide practical guidance for OSS developers on proactively improving
                     your security posture. How can you robustly assert and identify a user‚Äôs identity?
                     Guidance from the OWASP Proactive Controls: C6
                     One of the toughest problems is figuring out who&amp;#8217;s who online.
                     
                     Identity management is tricky, and improperly verifying identity can have massive
                     security implications.
                     We&amp;#8217;re here to help offer some guidance about how to securely implement authentication
                     (that is, how to verify digital identity) into your app. In the blog post below, we‚Äôll
                     discuss ways that will help you securely implement these features into your application.
                     OWASP breaks down the three main approaches to managing digital identity into the
                     following criteria:
                     
                     Passwords
                     Multi-factor authentication (MFA)
                     Cryptographic-based authentication
                     
                     Given the current authentication landscape, we‚Äôre adding another category:
                     
                     Single sign-on (SSO)
                     
                     We&amp;#8217;ll discuss single sign-on first, as it can be the simplest form of authentication
                     to implement into your application.
                     Using single sign-on (SSO)
                     The simplest way to assert whether a user is who they say they are is to have a trusted
                     third party confirm their identity. Technically speaking, this means having your users
                     sign into a platform with a robust authentication platform (like, GitHub, Google,
                     or Apple), then having that platform tell your application &amp;#8220;this user is &amp;lt;some
                     user&amp;gt;.&amp;#8221;
                     By using SSO, neither you nor your users need to deal with having an additional password.
                     You also don&amp;#8217;t need to deal with any of the infrastructure needed to verify
                     a user&amp;#8217;s digital identity. Instead, this is the responsibility of a service
                     provider.
                     The main tradeoff with using SSO is that you need to trust the identity provider.
                     However, generally speaking, identity providers tend to have robust security resources
                     dedicated to protecting user authentication. As such, in most cases we&amp;#8217;d advise
                     you to consider using an authentication provider as opposed to writing your own authentication.
                     There&amp;#8217;s a couple of ways to implement this into your application. The most common
                     way for a web application is to use OAuth or SAML. Check out our documentation for
                     more information about how to use GitHub for OAuth!
                     Next, let&amp;#8217;s discuss the oldest and most common way of handling authentication‚Äîpasswords.
                     Handling passwords
                     When handling user accounts and identities, the first thing that usually comes to
                     mind is passwords. Passwords are the way most users are accustomed to creating a new
                     account, and rely on a user generating a secret string, which they present to the
                     application each time they want to be authenticated.
                     There&amp;#8217;s a couple of key issues to look out for when using passwords in your
                     application as follows:
                     1. Creating (and facilitating the creation of) secure passwords
                     
                     Easily guessable passwords can lead to account compromises, so you need to ensure
                     that your users are setting strong credentials.
                     Provide guardrails to ensure users set strong passwords, without requirements that
                     upset or confuse them.
                     It‚Äôs important to strike a proper balance between a positive user experience and good
                     security! Studies have shown that complexity requirements on passwords have mixed
                     effectiveness, so you‚Äôre better off removing abstract character requirements in favor
                     of longer passwords.
                     OWASP‚Äôs guidance for good password guardrails are the following:
                     
                     Ensure all passwords are at least 10 characters in length.
                     Allow all printable ASCII characters.
                     Block commonly-used and compromised passwords.
                     
                     Allowing user-friendly, longer passwords means that your users will be happier, more
                     likely to remember them, and less vulnerable to a brute force attack.
                     This is a good place to mention that you should not prevent paste in password fields.
                     This is extremely user unfriendly, and makes it harder for users to use tooling, like
                     password managers, incentivizing bad passwords.
                     Remember, user friendly comes first!
                     2. Recovering passwords
                     We‚Äôve all forgotten our passwords at some point. Without them, how can we securely
                     assert our identity? We can do this by using an alternate method, like multi-factor
                     authentication!
                     Verify your user‚Äôs identity using multi-factor authentication (MFA)
                     We&amp;#8217;ll discuss MFA in more depth later, but in a nutshell, in a secure MFA implementation,
                     you should have a user prove their identity by using a combination of the following:
                     
                     Have them use something they know.
                     
                     For example, have them answer a security question.
                     For further guidance, specifically about security questions, check out the OWASP forgotten
                     password cheat sheets.
                     
                     
                     Have them use something they own.
                     
                     For example, send a reset token to a device or other account that they own (email).
                     
                     
                     
                     Make sure to provide a user friendly flow to reset their password using some combination
                     of the above methods.
                     Another good idea is to provide some instructions in your password recovery flow if
                     the user did not expect to receive a password recovery email (for example, please
                     change your password immediately).
                     3. Storing passwords securely
                     Passwords are the most critical part of a user asserting their identity. As such,
                     we need to treat them carefully in our infrastructure. Especially with password dumps
                     seemingly becoming more common every day, it‚Äôs extremely important to keep in mind
                     the following.
                     Never store passwords in plain text
                     Storing passwords in plain text means that any user that gets access to your password
                     database will be able to login as any other user. In the event of a breach, this would
                     be catastrophic.
                     On the other hand, only handling properly-hashed passwords ensures that a malicious
                     user that does get access to the password database will need to spend an enormous
                     amount of time trying to brute force each password.
                     In order to properly implement this, keep in mind two things:
                     
                     Choose the right hashing algorithm.
                     
                     Cryptography is tough! So, it&amp;#8217;s important to use an algorithm/library that has
                     been vetted by security experts and is well used.
                     A good hashing algorithm should be slow from both a CPU and GPU perspective (to prevent
                     brute forcing a password).
                     We&amp;#8217;d recommend using Argon2 or Bcrypt.
                     
                     
                     Salt your passwords.
                     
                     Attackers have access to rainbow tables, which are massive databases of precomputed
                     hash to string pairs. This can be used to map hashed versions of passwords back to
                     their strings (for commonly-used strings). In order to add entropy to the hashes and
                     thus make it less likely that the rainbow table contains the mapping, you should add
                     a random string, known as a salt, to user passwords when hashing. See the following
                     blog post for more information.
                     
                     
                     
                     4. Handling brute force attacks
                     Given an infinite number of guesses, any password can technically be guessed. To reduce
                     the probability of a successful password being tried, implement rate limiting or lockout
                     protections for a certain number of failed attempts. Dial this according to existing
                     policies and sensitivity of the data you are protecting.
                     For example, you could set up a counter to only allow five failed login attempts in
                     an hour. Then, lock the account, and let the user know of the failed attempts via
                     email.
                     To implement this, it&amp;#8217;s important to track the number of failed logins for any
                     given resource. You can use Redis, or another similar tool, to implement rate limiting.
                     This also helps prevent credential stuffing attacks, where leaked credentials from
                     another site are tried against several other websites.
                     Also consider adding monitoring services to reactively inform you when a resource
                     is being attacked! Time is everything in security, so having automated monitoring
                     can be the difference between a successful attack and one that is thwarted.
                     Even the best password can sometimes be compromised. Next, we&amp;#8217;ll talk about
                     ways to protect your users beyond the use of passwords.
                     Multi-factor authentication (MFA)
                     What happens if your users have their passwords compromised? Not all is lost, as long
                     as they have MFA enabled!
                     MFA means that users have to produce multiple pieces of information (factors) in order
                     to be granted access to your app. This also means that an attacker would need more
                     than just a password to take control of a user&amp;#8217;s account.
                     As mentioned previously, MFA consists of either something a user knows, or something
                     that a user owns. As such, there&amp;#8217;s a variety of methods that you can use to
                     implement this.
                     It&amp;#8217;s important to only support secure methods. These include TOTP, WebAuthn
                     or push-based notifications.
                     Make sure not to support insecure methods, such as texting a user with a token. Texting
                     a user is vulnerable to several kinds of attacks, such as spoofing, phishing, and
                     SIM swapping. As such, keep things safe and easy for your user by only allowing one
                     of the above options.
                     As an example of how important this is to protecting your users, GitHub recently announced
                     that we will require all users that contribute code on GitHub.com to enable one or
                     more methods of two-factor authentication (2FA) on their accounts.
                     Finally, let&amp;#8217;s take a look at how to securely handle your user&amp;#8217;s identity
                     using cryptographic-based authentication.
                     Cryptographic-based authentication
                     Now that you&amp;#8217;ve created the proper guardrails, and your users are behaving securely,
                     the last pillar of securing your user&amp;#8217;s identity is to ensure that your application
                     is properly handling your user&amp;#8217;s identity with cryptographic-based authentication.
                     This can happen in two main ways:
                     Session authentication
                     A &amp;#8220;session&amp;#8221; is a server side object that stores information about who
                     the current user is, when they logged in, and other aspects of their identity. Then,
                     in order to identify the user, we typically put a cookie on the user&amp;#8217;s browser
                     that contains a session identifier, which maps back to the server side identity object.
                     Some things to ensure when implementing session authentication are:
                     
                     Make sure to use secure configuration settings for the session cookie.
                     
                     In most cases, you should set the `http-only`, `secure`, and `samesite=lax` flags.
                     
                     
                     Ensure that the session identifier is long, unique, and random.
                     Generate a new session each time the user re-authenticates.
                     
                     Token authentication
                     Tokens are an alternative form of authentication, where instead of storing anything
                     on the server side, we cryptographically sign a piece of data and give it to the client.
                     Then, the client can give us this signed piece of data, and we can thus ensure that
                     we successfully authenticated them.
                     Token authentication can be tricky to implement safely, so we&amp;#8217;d recommend doing
                     more research before implementing this yourself. However, some pointers to start,
                     include:
                     
                     Use a long, random secret key.
                     
                     The entire security posture of your application relies on your tokens being generated
                     from a secure key.
                     Having a weak secret key will leave you vulnerable to a brute force attack, where
                     an attacker can try to guess your secret key, and thus hack all users at once.
                     
                     
                     Use a secure cryptographic algorithm, like ES256.
                     Set an appropriate expiration date with your tokens, based on your application&amp;#8217;s
                     usage patterns.
                     Store your tokens in a secure place.
                     
                     For a mobile app, you can use KeyStore (for Android) or KeyChain (for iOS).
                     For a web app, there are several places where you can store the token, each with different
                     security considerations. For general guidance, we would recommend storing tokens in
                     a cookie, with the `http-only`, `Secure` and `SameSite=Lax` being set.
                     
                     
                     
                     As mentioned previously, using token authentication in a web app can be tricky, and
                     best practices differ from framework to framework, so please do further research before
                     implementing this yourself.
                     Securely verifying your user&amp;#8217;s identity can be tricky, but following the guidance
                     above can help prevent common issues to keep your application and your users secure.
                     Until next time, stay secure!
                     </span></summary><time datetime="2022-06-10T21:12:59+02:00">Fri, 10 Jun 2022 19:12</time><article><p><em>This post is part seven of GitHub Security Lab‚Äôs <a href="https://github.blog/2021-12-06-write-more-secure-code-owasp-top-10-proactive-controls/">series on the OWASP Top 10 Proactive Controls</a>, where we provide practical guidance for OSS developers on proactively improving your security posture. How can you robustly assert and identify a user‚Äôs identity? Guidance from the <a href="https://owasp.org/www-project-proactive-controls/v3/en/c6-digital-identity.html">OWASP Proactive Controls: C6</a></em></p>
<p>One of the toughest problems is figuring out who&#8217;s who online.</p>
<p><img loading="lazy" class="width-fit aligncenter wp-image-65643 size-large" src="https://github.blog/wp-content/uploads/2022/06/image1-6.png?w=354&#038;resize=354%2C476" alt="" width="354" height="476" srcset="https://github.blog/wp-content/uploads/2022/06/image1-6.png?w=354&#038;resize=354%2C476 354w, https://github.blog/wp-content/uploads/2022/06/image1-6.png?w=223 223w" sizes="(max-width: 354px) 100vw, 354px" data-recalc-dims="1" /></p>
<p>Identity management is tricky, and improperly verifying identity can have <a href="https://krebsonsecurity.com/2018/04/panerabread-com-leaks-millions-of-customer-records/">massive</a> <a href="https://www.wired.com/story/cryptojacking-tesla-amazon-cloud/&amp;sa=D&amp;source=docs&amp;ust=1650565608337955&amp;usg=AOvVaw2O-UGNPQ0JaZ9j3YJ9DxvA">security</a> implications.</p>
<p>We&#8217;re here to help offer some guidance about how to securely implement authentication (that is, how to verify digital identity) into your app. In the blog post below, we‚Äôll discuss ways that will help you securely implement these features into your application.</p>
<p>OWASP breaks down the three main approaches to managing digital identity into the following criteria:</p>
<ol>
<li aria-level="1">Passwords</li>
<li aria-level="1">Multi-factor authentication (MFA)</li>
<li aria-level="1">Cryptographic-based authentication</li>
</ol>
<p>Given the current authentication landscape, we‚Äôre adding another category:</p>
<ul>
<li aria-level="1">Single sign-on (SSO)</li>
</ul>
<p>We&#8217;ll discuss single sign-on first, as it can be the simplest form of authentication to implement into your application.</p>
<h2>Using single sign-on (SSO)</h2>
<p>The simplest way to assert whether a user is who they say they are is to have a trusted third party confirm their identity. Technically speaking, this means having your users sign into a platform with a robust authentication platform (like, GitHub, Google, or Apple), then having that platform tell your application &#8220;this user is &lt;some user&gt;.&#8221;</p>
<p>By using SSO, neither you nor your users need to deal with having an additional password. You also don&#8217;t need to deal with any of the infrastructure needed to verify a user&#8217;s digital identity. Instead, this is the responsibility of a service provider.</p>
<p>The main tradeoff with using SSO is that you need to trust the identity provider. However, generally speaking, identity providers tend to have robust security resources dedicated to protecting user authentication. As such, in most cases we&#8217;d advise you to consider using an authentication provider as opposed to writing your own authentication.</p>
<p>There&#8217;s a couple of ways to implement this into your application. The most common way for a web application is to use <a href="https://www.digitalocean.com/community/tutorials/an-introduction-to-oauth-2">OAuth</a> or <a href="https://www.cloudflare.com/learning/access-management/what-is-saml/">SAML</a>. Check out <a href="https://docs.github.com/en/developers/apps/building-oauth-apps/authorizing-oauth-apps">our documentation</a> for more information about how to use GitHub for OAuth!</p>
<p>Next, let&#8217;s discuss the oldest and most common way of handling authentication‚Äîpasswords.</p>
<h2>Handling passwords</h2>
<p>When handling user accounts and identities, the first thing that usually comes to mind is passwords. Passwords are the way most users are accustomed to creating a new account, and rely on a user generating a secret string, which they present to the application each time they want to be authenticated.</p>
<p>There&#8217;s a couple of key issues to look out for when using passwords in your application as follows:</p>
<h3>1. Creating (and facilitating the creation of) secure passwords</h3>
<p><img loading="lazy" width="740" height="601" class="alignnone size-medium wp-image-65641 width-fit" src="https://github.blog/wp-content/uploads/2022/06/password_strength.png?w=300&#038;resize=740%2C601" alt="&quot;" srcset="https://github.blog/wp-content/uploads/2022/06/password_strength.png?w=740 740w, https://github.blog/wp-content/uploads/2022/06/password_strength.png?w=300&#038;resize=740%2C601 300w" sizes="(max-width: 740px) 100vw, 740px" data-recalc-dims="1" /></p>
<p>Easily guessable passwords can lead to account compromises, so you need to ensure that your users are setting strong credentials.</p>
<h4><strong>Provide guardrails to ensure users set strong passwords, without requirements that upset or confuse them.</strong></h4>
<p>It‚Äôs important to strike a proper balance between a positive user experience and good security! Studies have shown that complexity requirements on passwords have mixed effectiveness, so you‚Äôre better off removing abstract character requirements<a href="https://xkcd.com/936/"> in favor of longer passwords</a>.</p>
<p>OWASP‚Äôs guidance for good password guardrails are the following:</p>
<ol>
<li aria-level="1">Ensure all passwords are at least 10 characters in length.</li>
<li aria-level="1">Allow all printable ASCII characters.</li>
<li aria-level="1">Block <a href="https://en.wikipedia.org/wiki/List_of_the_most_common_passwords">commonly-used</a> and <a href="https://haveibeenpwned.com/Passwords">compromised passwords</a>.</li>
</ol>
<p>Allowing user-friendly, longer passwords means that your users will be happier, more likely to remember them, and less vulnerable to a brute force attack.</p>
<p>This is a good place to mention that you should not prevent paste in password fields. This is extremely user unfriendly, and makes it harder for users to use tooling, like password managers, incentivizing bad passwords.</p>
<p>Remember, user friendly comes first!</p>
<h3>2. Recovering passwords</h3>
<p>We‚Äôve all forgotten our passwords at some point. Without them, how can we securely assert our identity? We can do this by using an alternate method, like multi-factor authentication!</p>
<h4><strong>Verify your user‚Äôs identity using multi-factor authentication (MFA)</strong></h4>
<p>We&#8217;ll discuss MFA in more depth later, but in a nutshell, in a secure MFA implementation, you should have a user prove their identity by using a combination of the following:</p>
<ol>
<li aria-level="1">Have them use something they know.
<ul>
<li aria-level="2">For example, have them answer a security question.</li>
<li aria-level="2">For further guidance, specifically about security questions, check out the<a href="https://www.owasp.org/index.php/Forgot_Password_Cheat_Sheet"> OWASP forgotten password cheat sheets</a>.</li>
</ul>
</li>
<li aria-level="1">Have them use something they own.
<ul>
<li aria-level="1">For example, send a reset token to a device or other account that they own (email).</li>
</ul>
</li>
</ol>
<p>Make sure to provide a user friendly flow to reset their password using some combination of the above methods.</p>
<p>Another good idea is to provide some instructions in your password recovery flow if the user did not expect to receive a password recovery email (for example, please change your password immediately).</p>
<h3>3. Storing passwords securely</h3>
<p>Passwords are the most critical part of a user asserting their identity. As such, we need to treat them carefully in our infrastructure. Especially with password dumps seemingly becoming more common every day, it‚Äôs extremely important to keep in mind the following.</p>
<h4>Never store passwords in plain text</h4>
<p>Storing passwords in plain text means that any user that gets access to your password database will be able to login as any other user. In the event of a breach, this would be catastrophic.</p>
<p>On the other hand, only handling properly-hashed passwords ensures that a malicious user that does get access to the password database will need to spend an enormous amount of time trying to brute force each password.</p>
<p>In order to properly implement this, keep in mind two things:</p>
<ol>
<li aria-level="1">Choose the right hashing algorithm.
<ul>
<li aria-level="2">Cryptography is tough! So, it&#8217;s important to use an algorithm/library that has been vetted by security experts and is well used.</li>
<li aria-level="2">A good hashing algorithm should be slow from both a CPU and GPU perspective (to prevent brute forcing a password).</li>
<li aria-level="2">We&#8217;d recommend using <a href="https://en.wikipedia.org/wiki/Argon2">Argon2</a> or Bcrypt.</li>
</ul>
</li>
<li aria-level="1">Salt your passwords.
<ul>
<li aria-level="2">Attackers have access to <a href="https://en.wikipedia.org/wiki/Rainbow_table">rainbow tables,</a> which are massive databases of precomputed hash to string pairs. This can be used to map hashed versions of passwords back to their strings (for commonly-used strings). In order to add entropy to the hashes and thus make it less likely that the rainbow table contains the mapping, you should add a random string, known as a salt, to user passwords when hashing. See <a href="https://auth0.com/blog/adding-salt-to-hashing-a-better-way-to-store-passwords/">the following blog post</a> for more information.</li>
</ul>
</li>
</ol>
<h3>4. Handling brute force attacks</h3>
<p>Given an infinite number of guesses, any password can technically be guessed. To reduce the probability of a successful password being tried, implement rate limiting or lockout protections for a certain number of failed attempts. Dial this according to existing policies and sensitivity of the data you are protecting.</p>
<p>For example, you could set up a counter to only allow five failed login attempts in an hour. Then, lock the account, and let the user know of the failed attempts via email.</p>
<p>To implement this, it&#8217;s important to track the number of failed logins for any given resource. You can use Redis, or another similar tool, to implement rate limiting.</p>
<p>This also helps prevent credential stuffing attacks, where leaked credentials from another site are tried against several other websites.</p>
<p>Also consider adding monitoring services to reactively inform you when a resource is being attacked! Time is everything in security, so having automated monitoring can be the difference between a successful attack and one that is thwarted.</p>
<p>Even the best password can sometimes be compromised. Next, we&#8217;ll talk about ways to protect your users beyond the use of passwords.</p>
<h2>Multi-factor authentication (MFA)</h2>
<p>What happens if your users have their passwords compromised? Not all is lost, as long as they have MFA enabled!</p>
<p>MFA means that users have to produce multiple pieces of information (factors) in order to be granted access to your app. This also means that an attacker would need more than just a password to take control of a user&#8217;s account.</p>
<p>As mentioned previously, MFA consists of either something a user knows, or something that a user owns. As such, there&#8217;s a variety of methods that you can use to implement this.</p>
<p>It&#8217;s important to only support secure methods. These include <a href="https://www.twilio.com/docs/glossary/totp">TOTP</a>, <a href="https://webauthn.guide/">WebAuthn</a> or push-based notifications.</p>
<p>Make sure not to support insecure methods, such as texting a user with a token. Texting a user is vulnerable to several kinds of attacks, such as spoofing, phishing, and SIM swapping. As such, keep things safe and easy for your user by only allowing one of the above options.</p>
<p>As an example of how important this is to protecting your users, <a href="https://github.blog/2022-05-04-software-security-starts-with-the-developer-securing-developer-accounts-with-2fa/">GitHub recently announced</a> that we will require all users that contribute code on GitHub.com to enable one or more methods of two-factor authentication (2FA) on their accounts.</p>
<p>Finally, let&#8217;s take a look at how to securely handle your user&#8217;s identity using cryptographic-based authentication.</p>
<h2>Cryptographic-based authentication</h2>
<p>Now that you&#8217;ve created the proper guardrails, and your users are behaving securely, the last pillar of securing your user&#8217;s identity is to ensure that your application is properly handling your user&#8217;s identity with cryptographic-based authentication. This can happen in two main ways:</p>
<h3>Session authentication</h3>
<p>A &#8220;session&#8221; is a server side object that stores information about who the current user is, when they logged in, and other aspects of their identity. Then, in order to identify the user, we typically put a cookie on the user&#8217;s browser that contains a session identifier, which maps back to the server side identity object.</p>
<p>Some things to ensure when implementing session authentication are:</p>
<ol>
<li aria-level="1">Make sure to <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies#security">use secure configuration settings</a> for the session cookie.
<ul>
<li aria-level="1">In most cases, you should set the `http-only`, `secure`, and `samesite=lax` flags.</li>
</ul>
</li>
<li aria-level="1">Ensure that the session identifier is long, unique, and random.</li>
<li aria-level="1">Generate a new session each time the user re-authenticates.</li>
</ol>
<h3>Token authentication</h3>
<p>Tokens are an alternative form of authentication, where instead of storing anything on the server side, we cryptographically sign a piece of data and give it to the client. Then, the client can give us this signed piece of data, and we can thus ensure that we successfully authenticated them.</p>
<p>Token authentication can be tricky to implement safely, so we&#8217;d recommend doing more research before implementing this yourself. However, some pointers to start, include:</p>
<ol>
<li aria-level="1"><strong>Use a long, random secret key.</strong>
<ul>
<li aria-level="1">The entire security posture of your application relies on your tokens being generated from a secure key.</li>
<li aria-level="2">Having a weak secret key will leave you vulnerable to a brute force attack, where an attacker can try to guess your secret key, and thus hack all users at once.</li>
</ul>
</li>
<li aria-level="1">Use a secure cryptographic algorithm, like ES256.</li>
<li aria-level="1">Set an appropriate expiration date with your tokens, based on your application&#8217;s usage patterns.</li>
<li aria-level="1">Store your tokens in a secure place.
<ul>
<li aria-level="2">For a mobile app, you can use KeyStore (for Android) or KeyChain (for iOS).</li>
<li aria-level="2">For a web app, there are several places where you can store the token, each with different security considerations. For general guidance, we would recommend <strong>storing tokens in a cookie, with the `http-only`, `Secure` and `SameSite=Lax`</strong> being set.</li>
</ul>
</li>
</ol>
<p>As mentioned previously, using token authentication in a web app can be tricky, and best practices differ from framework to framework, so please do <a href="https://www.youtube.com/watch?v=-zD11ubPsFM">further research</a> before implementing this yourself.</p>
<p>Securely verifying your user&#8217;s identity can be tricky, but following the guidance above can help prevent common issues to keep your application and your users secure. Until next time, stay secure!</p>
</article>
            </details>
            <footer>&nbsp;<q>The GitHub Blog&nbsp;</q></footer>
         </section>
         
         <section id="d6e9">
            <header>
               <h2 title="Learn about Netflix‚Äôs world class engineering efforts, company culture, product developments and more. - Medium">Netflix TechBlog - Medium <a rel="noopener noreferrer" target="_blank" href="https://netflixtechblog.com?source=rss----2615bd06b42e---4">ùìó</a><a rel="noopener noreferrer" target="_blank" href="https://netflixtechblog.com/feed">ùìï</a></h2>
            </header>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-part-2-49348511c06c?source=rss----2615bd06b42e---4">How Netflix Content Engineering makes a federated graph searchable (Part 2)</a> <span>By Alex Hutter, Falguni Jhaveri, and Senthil SayeebabaIn a previous post, we described
                     the indexing architecture of Studio Search and how we scaled the architecture by building
                     a config-driven self-service platform that allowed teams in Content Engineering to
                     spin up search indices&nbsp;easily.This post will discuss how Studio Search supports querying
                     the data available in these&nbsp;indices.Data consumption from Studio Search&nbsp;DGSIntroductionWhen
                     we say Content Engineering teams are interested in searching against the federated
                     graph, the use-case is mainly focused on known-item search (a user has an item or
                     items in mind they are trying to view or navigate to but need to use an external information
                     system to locate them) and data retrieval (typically the data is structured and there
                     is no ambiguity as to whether a particular record matches the given search criteria
                     except in the case of textual fields where there is limited ambiguity) within a vertical
                     search experience (focus on enabling search for a specific sub-graph within the big
                     federated graph)Query LanguageGiven the above scope of the search (vertical search
                     experience with a focus on known-item search and data retrieval), one of the first
                     things we had to design was a language that users can use to easily express their
                     search criteria. With a goal of abstracting users away from the complexity of interacting
                     with Elasticsearch directly, we landed on a custom Studio Search DSL reminiscent of&nbsp;SQL.The
                     DSL supports specifying the search criteria as comparison expressions or inclusion/exclusion
                     filters. The filter expressions can be combined together through logical operators
                     (AND, OR, NOT) and grouped together through parentheses.Sample SyntaxFor example,
                     to find all comedies from France or Spain, the query would&nbsp;be:(genre == ‚Äòcomedy‚Äô)
                     AND (country ANY [‚ÄòFR‚Äô,&nbsp;‚ÄòSP‚Äô])We used ANTLR to build the grammar for the Query DSL.
                     From the grammar, ANTLR generates a parser that can walk the parse tree. By extending
                     the ANTLR generated parse tree visitor, we were able to implement an Elasticsearch
                     Query Builder component with the logic to generate the Elasticsearch query corresponding
                     to the custom search&nbsp;query.If you are familiar with Elasticsearch, then you might
                     be familiar with how complicated it can be to build up the correct Elasticsearch query
                     for complex queries, especially if the index includes nested JSON documents which
                     add an additional layer of complexity with respect to building nested queries (Incorrectly
                     constructed nested queries can lead to Elasticsearch quietly returning wrong results).
                     By exposing just a generic query language to the users and isolating the complexity
                     to just our Elasticsearch Query Builder, we have been able to empower users to write
                     search queries without requiring familiarity with Elasticsearch. This also leaves
                     the possibility of swapping Elasticsearch with a different search engine in the&nbsp;future.One
                     other challenge for the users when writing the search queries is to understand the
                     fields that are available in the index and the associated types. Since we index the
                     data as-is from the federated graph, the indexing query itself acts as self-documentation.
                     For example, given the indexing query&nbsp;-Sample GraphQL&nbsp;queryTo find movies based on
                     the actors‚Äô roles, the query filter is&nbsp;simply`actors.role ==&nbsp;‚Äòactor‚Äô`Text SearchWhile
                     the search DSL provides a powerful way to help narrow the scope of the search queries,
                     users can also find documents in the index through free form text‚Ää‚Äî‚Ääeither with just
                     the input text or in combination with a filter expression in the search DSL. Behind
                     the scenes during the indexing process, we have configured the Elasticsearch index
                     with the appropriate analyzers to ensure that the most relevant matches for the input
                     text are returned in the&nbsp;results.Hydration through FederationGiven the wide adoption
                     of the federated gateway within Content Engineering, we decided to implement the Studio
                     Search service as a DGS (Domain Graph Service) that integrated with the federated
                     gateway. The search APIs (besides search, we have other APIs to support faceted search,
                     typeahead suggestions, etc) are exposed as GraphQL queries within the federated graph.This
                     integration with the federation gateway allows the search DGS to just return the matching
                     entity keys from the search index instead of the whole matching document(s). Through
                     the power of federation, users are then able to hydrate the search results with any
                     data available in the federated graph. This allows the search indices to be lean by
                     indexing only the fields necessary for the search experience and at the same time
                     provides complete flexibility for the users to fetch any data available in the federated
                     graph instead of being restricted to just the data available in the search&nbsp;index.ExampleSample
                     Search&nbsp;queryIn the above example, users are able to fetch the production schedule
                     as part of the search results even though the search index doesn‚Äôt hold that&nbsp;data.AuthorizationWith
                     the API to query the data in the search indices in place, the next thing we needed
                     to tackle was figuring out how to secure access to the data in the indices. With several
                     of the indices including sensitive data, and the source teams already having restrictive
                     access policies in place to secure the data they own, the search indices which hosted
                     a secondary copy of the source data needed to be secured as&nbsp;well.We chose to apply
                     ‚Äúlate binding‚Äù (or ‚Äúquery time‚Äù) security‚Ää‚Äî‚Ääon every incoming search query, we make
                     an API call to the centralized access policy server with context including the identity
                     of the caller making the request and the search index they are trying to access. The
                     policy server evaluates the access policies defined by the source teams and returns
                     a set of constraints. Ex. The caller has access to Movies where the type is ‚Äòlicensed‚Äô
                     (The caller does not have access to Netflix-produced content, but just the licensed
                     content). The constraints are then translated to a set of filter expressions in the
                     search query DSL format (Ex. movie.type == ‚Äòlicensed‚Äô) and combined with the user-specified
                     search filter with a logical AND operator to form a new search query that then gets
                     executed against the&nbsp;index.By adding on the access constraints as additional filters
                     before executing the query, we ensure that the user gets back only the data they have
                     access to from the underlying search index. This also allows source teams to evolve
                     their access policies independently knowing that the correct constraints will be applied
                     at query&nbsp;time.Customizing SearchWith the decision to build Studio Search as a GraphQL
                     service using the DGS framework and relying on federation for hydrating results, onboarding
                     new search indices required updating various portions of the GraphQL schema (the enum
                     of available indices, the union of all federated result types, etc.) manually and
                     registering the updated schema with the federated gateway schema registry before the
                     new index was available for querying through the GraphQL&nbsp;API.Additionally, there are
                     additional configurations that users can provide while onboarding a new index to customize
                     the search behavior for their applications‚Ää‚Äî‚Ääincluding scripts to tune the relevance
                     scoring algorithm, configuring fields for faceted search, and configuration to control
                     the behavior of typeahead suggestions, etc. These configurations were initially stored
                     in our source control repository which meant any changes to the configuration of any
                     index required a deployment for the changes to take&nbsp;effect.Recently, we automated
                     this process as well by moving all the configurations to a persistence store and leveraging
                     the power of dynamic schemas in the DGS framework. Users can now use an API to create/update
                     search index configuration and we are able to validate the provided configuration,
                     generate the updated DGS schema dynamically and register the updated schema with the
                     federated gateway schema registry immediately. All configuration changes are reflected
                     immediately in subsequent search&nbsp;queries.Example configuration:Sample Search configurationUI
                     ComponentsWhile the primary goal of Studio Search was to build an easy-to-use self-service
                     platform to enable searching against the federated graph, another important goal was
                     to help the Content Engineering teams deliver a visually consistent search experience
                     to the users of their tools and workflows. To that end, we partnered with our UI/UX
                     teams to build a robust set of opinionated presentational components. Studio Search‚Äôs
                     offering of drop-in UI components based on our Hawkins design system for typeahead
                     suggestion, faceted search, and extensive filtering ensure visual and behavioral consistency
                     across the suite of applications within Content Engineering. Below are a couple of
                     examples.Typeahead Search ComponentFaceted Search ComponentWhat‚Äôs Next?As a config-driven,
                     self-serve platform, Studio Search has already been able to empower Content Engineering
                     teams to quickly enable the functionality to search against the Content federated
                     graph within their suite of applications. But, we are not quite done yet! There are
                     several upcoming features that are in various stages of development includingLeveraging
                     the percolate query functionality in Elasticsearch to support a notifications feature
                     (users save their search criteria and are notified when documents are updated in the
                     index that matches their search criteria)Add support for metrics aggregation in our&nbsp;APIsLeverage
                     the managed delivery functionality in Spinnaker to move to a declarative model for
                     onboarding the search&nbsp;indicesAnd, plenty&nbsp;moreIf this sounds interesting to you, connect
                     with us on LinkedIn.CreditsThanks to Anoop Panicker, Bo Lei, Charles Zhao, Chris Dhanaraj,
                     Hemamalini Kannan, Jim Isaacs, Johnny Chang, Kasturi Chatterjee, Kishore Banala, Kevin
                     Zhu, Tom Lee, Tongliang Liu, Utkarsh Shrivastava, Vince Bello, Vinod Viswanathan,
                     Yucheng&nbsp;ZengHow Netflix Content Engineering makes a federated graph searchable (Part
                     2) was originally published in Netflix TechBlog on Medium, where people are continuing
                     the conversation by highlighting and responding to this story.</span></summary><time datetime="2022-06-15T18:34:20+02:00">Wed, 15 Jun 2022 16:34</time><article><p>By <a href="https://www.linkedin.com/in/ahutter/">Alex Hutter</a>, <a href="https://www.linkedin.com/in/falgunijhaveri/">Falguni Jhaveri</a>, and <a href="https://www.linkedin.com/in/senthilsayeebaba/">Senthil Sayeebaba</a></p><p>In a <a href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf">previous post</a>, we described the indexing architecture of Studio Search and how we scaled the architecture by building a config-driven self-service platform that allowed teams in Content Engineering to spin up search indices¬†easily.</p><p>This post will discuss how Studio Search supports querying the data available in these¬†indices.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*LBuhmsQxMXkOPlRJ" /><figcaption>Data consumption from Studio Search¬†DGS</figcaption></figure><h3>Introduction</h3><p>When we say Content Engineering teams are interested in searching against the federated graph, the use-case is mainly focused on known-item search (<em>a user has an item or items in mind they are trying to view or navigate to but need to use an external information system to locate them</em>) and data retrieval (<em>typically the data is structured and there is no ambiguity as to whether a particular record matches the given search criteria except in the case of textual fields where there is limited ambiguity</em>) within a vertical search experience (f<em>ocus on enabling search for a specific sub-graph within the big federated graph</em>)</p><h3>Query Language</h3><p>Given the above scope of the search (<em>vertical search experience with a focus on known-item search and data retrieval</em>), one of the first things we had to design was a language that users can use to easily express their search criteria. With a goal of abstracting users away from the complexity of interacting with <a href="https://www.elastic.co/elasticsearch/">Elasticsearch</a> directly, we landed on a custom Studio Search DSL reminiscent of¬†SQL.</p><p>The DSL supports specifying the search criteria as comparison expressions or inclusion/exclusion filters. The filter expressions can be combined together through logical operators (<em>AND, OR, NOT</em>) and grouped together through parentheses.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*tB276kn6UqUiZanx" /><figcaption>Sample Syntax</figcaption></figure><p>For example, to find all comedies from France or Spain, the query would¬†be:</p><p><em>(genre == ‚Äòcomedy‚Äô) AND (country ANY [‚ÄòFR‚Äô,¬†‚ÄòSP‚Äô])</em></p><p>We used <a href="https://www.antlr.org/">ANTLR</a> to build the grammar for the Query DSL. From the grammar, ANTLR generates a parser that can walk the parse tree. By extending the ANTLR generated parse tree visitor, we were able to implement an Elasticsearch Query Builder component with the logic to generate the Elasticsearch query corresponding to the custom search¬†query.</p><p>If you are familiar with Elasticsearch, then you might be familiar with how complicated it can be to build up the correct Elasticsearch query for complex queries, especially if the index includes <a href="https://www.elastic.co/guide/en/elasticsearch/reference/8.1/nested.html">nested </a>JSON documents which add an additional layer of complexity with respect to building <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-nested-query.html">nested queries</a> (Incorrectly constructed nested queries can lead to Elasticsearch quietly returning wrong results). By exposing just a generic query language to the users and isolating the complexity to just our Elasticsearch Query Builder, we have been able to empower users to write search queries without requiring familiarity with Elasticsearch. This also leaves the possibility of swapping Elasticsearch with a different search engine in the¬†future.</p><p>One other challenge for the users when writing the search queries is to understand the fields that are available in the index and the associated types. Since we index the data as-is from the federated graph, the indexing query itself acts as self-documentation. For example, given the indexing query¬†-</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9QPrND7CVLd3fvS-Q0JAGA.png" /><figcaption>Sample GraphQL¬†query</figcaption></figure><p>To find movies based on the actors‚Äô roles, the query filter is¬†simply</p><p><em>`actors.role ==¬†‚Äòactor‚Äô`</em></p><h3>Text Search</h3><p>While the search DSL provides a powerful way to help narrow the scope of the search queries, users can also find documents in the index through free form text‚Ää‚Äî‚Ääeither with just the input text or in combination with a filter expression in the search DSL. Behind the scenes during the indexing process, we have configured the Elasticsearch index with the appropriate analyzers to ensure that the most relevant matches for the input text are returned in the¬†results.</p><h3>Hydration through Federation</h3><p>Given the wide adoption of the <a href="https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2">federated gateway</a> within Content Engineering, we decided to implement the Studio Search service as a DGS (<a href="https://netflixtechblog.com/open-sourcing-the-netflix-domain-graph-service-framework-graphql-for-spring-boot-92b9dcecda18">Domain Graph Service</a>) that integrated with the federated gateway. The search APIs (besides search, we have other APIs to support faceted search, typeahead suggestions, etc) are exposed as GraphQL queries within the federated graph.</p><p>This integration with the federation gateway allows the search DGS to just return the matching entity keys from the search index instead of the whole matching document(s). Through the power of federation, users are then able to hydrate the search results with any data available in the federated graph. This allows the search indices to be lean by indexing only the fields necessary for the search experience and at the same time provides complete flexibility for the users to fetch any data available in the federated graph instead of being restricted to just the data available in the search¬†index.</p><p>Example</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wR89b6QUOLnF8G7LPbV_-Q.png" /><figcaption>Sample Search¬†query</figcaption></figure><p>In the above example, users are able to fetch the production schedule as part of the search results even though the search index doesn‚Äôt hold that¬†data.</p><h3>Authorization</h3><p>With the API to query the data in the search indices in place, the next thing we needed to tackle was figuring out how to secure access to the data in the indices. With several of the indices including sensitive data, and the source teams already having restrictive access policies in place to secure the data they own, the search indices which hosted a secondary copy of the source data needed to be secured as¬†well.</p><p>We chose to apply ‚Äúlate binding‚Äù (or ‚Äúquery time‚Äù) security‚Ää‚Äî‚Ääon every incoming search query, we make an API call to the centralized access policy server with context including the identity of the caller making the request and the search index they are trying to access. The policy server evaluates the access policies defined by the source teams and returns a set of constraints. Ex. The caller has access to Movies where the type is ‚Äòlicensed‚Äô (The caller does not have access to Netflix-produced content, but just the licensed content). The constraints are then translated to a set of filter expressions in the search query DSL format (Ex. <em>movie.type == ‚Äòlicensed‚Äô</em>) and combined with the user-specified search filter with a logical <em>AND</em> operator to form a new search query that then gets executed against the¬†index.</p><p>By adding on the access constraints as additional filters before executing the query, we ensure that the user gets back only the data they have access to from the underlying search index. This also allows source teams to evolve their access policies independently knowing that the correct constraints will be applied at query¬†time.</p><h3>Customizing Search</h3><p>With the decision to build Studio Search as a GraphQL service using the<a href="https://netflix.github.io/dgs/"> DGS framework</a> and relying on federation for hydrating results, onboarding new search indices required updating various portions of the GraphQL schema (the enum of available indices, the union of all federated result types, etc.) manually and registering the updated schema with the federated gateway schema registry before the new index was available for querying through the GraphQL¬†API.</p><p>Additionally, there are additional configurations that users can provide while onboarding a new index to customize the search behavior for their applications‚Ää‚Äî‚Ääincluding scripts to tune the relevance scoring algorithm, configuring fields for faceted search, and configuration to control the behavior of typeahead suggestions, etc. These configurations were initially stored in our source control repository which meant any changes to the configuration of any index required a deployment for the changes to take¬†effect.</p><p>Recently, we automated this process as well by moving all the configurations to a persistence store and leveraging the power of <a href="https://netflix.github.io/dgs/advanced/dynamic-schemas/">dynamic schemas</a> in the DGS framework. Users can now use an API to create/update search index configuration and we are able to validate the provided configuration, generate the updated DGS schema dynamically and register the updated schema with the federated gateway schema registry immediately. All configuration changes are reflected immediately in subsequent search¬†queries.</p><p>Example configuration:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*H2NAkxBf-OLVUxGLB7yEbw.png" /><figcaption>Sample Search configuration</figcaption></figure><h3>UI Components</h3><p>While the primary goal of Studio Search was to build an easy-to-use self-service platform to enable searching against the federated graph, another important goal was to help the Content Engineering teams deliver a visually consistent search experience to the users of their tools and workflows. To that end, we partnered with our UI/UX teams to build a robust set of opinionated presentational components. Studio Search‚Äôs offering of drop-in UI components based on our <a href="https://netflixtechblog.com/hawkins-diving-into-the-reasoning-behind-our-design-system-964a7357547">Hawkins design system</a> for typeahead suggestion, faceted search, and extensive filtering ensure visual and behavioral consistency across the suite of applications within Content Engineering. Below are a couple of examples.</p><p>Typeahead Search Component</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rJp7gq4NQFfCDWfi" /></figure><p>Faceted Search Component</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*YK3IVF6B_qntQqgl" /></figure><h3>What‚Äôs Next?</h3><p>As a config-driven, self-serve platform, Studio Search has already been able to empower Content Engineering teams to quickly enable the functionality to search against the Content federated graph within their suite of applications. But, we are not quite done yet! There are several upcoming features that are in various stages of development including</p><ul><li>Leveraging the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-percolate-query.html">percolate query</a> functionality in Elasticsearch to support a notifications feature (users save their search criteria and are notified when documents are updated in the index that matches their search criteria)</li><li>Add support for <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics.html">metrics aggregation</a> in our¬†APIs</li><li>Leverage the <a href="https://spinnaker.io/docs/guides/user/managed-delivery/">managed delivery</a> functionality in <a href="https://spinnaker.io/">Spinnaker</a> to move to a declarative model for onboarding the search¬†indices</li><li>And, plenty¬†more</li></ul><p>If this sounds interesting to you, connect with us on LinkedIn.</p><h3>Credits</h3><p>Thanks to <a href="https://www.linkedin.com/in/anoop-panicker/">Anoop Panicker</a>, <a href="https://www.linkedin.com/in/bolei1007/">Bo Lei</a>, <a href="https://www.linkedin.com/in/czhao/">Charles Zhao</a>, <a href="https://www.linkedin.com/in/chrisdhanaraj/">Chris Dhanaraj</a>, <a href="https://www.linkedin.com/in/hemamalinikannan/">Hemamalini Kannan</a>,<a href="https://www.linkedin.com/in/jimpisaacs/"> Jim Isaacs</a>, <a href="https://www.linkedin.com/in/johnnycc321/">Johnny Chang</a>, <a href="https://www.linkedin.com/in/kasturi-chatterjee-a900715/">Kasturi Chatterjee</a>, <a href="https://www.linkedin.com/in/kishore-banala/">Kishore Banala</a>, <a href="https://www.linkedin.com/in/kevinzhu/">Kevin Zhu</a>, <a href="https://www.linkedin.com/in/thomaslee4/">Tom Lee</a>, <a href="https://www.linkedin.com/in/tonylxc/">Tongliang Liu</a>, <a href="https://www.linkedin.com/in/utkarshshrivastava/">Utkarsh Shrivastava</a>, <a href="https://www.linkedin.com/in/vincentbello/">Vince Bello</a>, <a href="https://www.linkedin.com/in/vinodvish/">Vinod Viswanathan</a>, <a href="https://www.linkedin.com/in/yuchengzeng/">Yucheng¬†Zeng</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=49348511c06c" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-part-2-49348511c06c">How Netflix Content Engineering makes a federated graph searchable (Part 2)</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://netflixtechblog.com/scaling-appsec-at-netflix-part-2-c9e0f1488bc5?source=rss----2615bd06b42e---4">Scaling Appsec at Netflix (Part 2)</a> <span>By Astha Singhal, Lakshmi Sudheer, Julia&nbsp;KnechtThe Application Security teams at Netflix
                     are responsible for securing the software footprint that we create to run the Netflix
                     product, the Netflix studio, and the business. Our customers are product and engineering
                     teams at Netflix that build these software services and platforms. The Netflix cultural
                     values of ‚ÄòContext not Control‚Äô and ‚ÄòFreedom and Responsibility‚Äô strongly influence
                     how we do Security at Netflix. Our goal is to manage security risks to Netflix via
                     clear, opinionated security guidance, and by providing risk context to Netflix engineering
                     teams to make pragmatic risk decisions at&nbsp;scale.A few years ago, we published this
                     blog post about how we had organized our team to focus our bandwidth on scalable investments
                     as opposed to just traditional Appsec functions, which were not scaling well in our
                     rapidly growing environment. We leaned into the idea of strategic security partnerships
                     and automation investments to create more leverage for application security. This
                     became the foundation for our current org structure with teams focused on Appsec Partnerships
                     and Appsec Engineering. In this operating model, we provided critical Appsec operational
                     services to Netflix‚Ää‚Äî‚Ääincluding bug bounty, pentesting, PSIRT (product security incident
                     response), security reviews, and developer security education‚Ää‚Äî‚Äävia a shared on-call
                     rotation.Over the past few years, this model has allowed us to focus on investments
                     like Secure by Default for baseline security controls, Security Self-Service for clear
                     actionable guidance and Vulnerability Scanning at scale for software supply chain
                     security. We wanted to share an update on learnings from this model, how our needs
                     have evolved, and where we expect to go from&nbsp;here.Among the most notable wins, we
                     have been able to utilize this scale focused approach to productize application security
                     for our rapidly growing studio engineering ecosystem, standardize security baseline
                     for all Enterprise apps, and build paved roads to provide Secure by Default Authentication
                     &amp;amp; Authorization capabilities for central data engineering tools. Our focus has
                     been on improving overall security assurance as opposed to just vulnerability prevention.
                     We are now expanding this approach to more parts of our ecosystem. This mindset has
                     also allowed us to invest our capacity for white-glove service towards reasonable
                     residual risk and standard guidance so we can reduce the need for white-glove engagements
                     in the long term (e.g., investment in an API proxy that provides baseline security
                     controls for free as opposed to pentesting all applications that would eventually
                     sit behind that API proxy). This approach has also allowed us to build strong relationships
                     with central engineering teams at Netflix (Data Platform, Developer Tools, Cloud Infrastructure,
                     IAM Product Engineering) that will continue to serve as central points of leverage
                     for security in the long&nbsp;term.However, it has not been all sunshine and rainbows.
                     On the partnership side, the bespoke nature of each partnership means that there isn‚Äôt
                     consistency and redundancy built into the operating model and the related partnership
                     artifacts (e.g., Security Strategy and Roadmap, Threat Model, Deliverable Tracking,
                     Residual Risk Criteria, etc). This leads to insufficient context sharing and high
                     operational churn every time we have personnel changes. The partnership charter has
                     also grown laterally into the infrastructure space as we stack our leverage bets on
                     infrastructure components (like Service Mesh, Container Platform, etc). The skill
                     sets and domain depth in those partnerships has further diversified the skills on
                     the team. But this is a tradeoff on our ability to serve generalized Appsec oncall
                     needs like bug bounty triage with high consistency. Given that partnerships focus
                     on long-running strategic initiatives, the wins can be few and far between and that
                     can be difficult for team motivation. We also found various areas in which security
                     partnership work bleeds into security product solutioning and it can be difficult
                     to identify the appropriate handoff&nbsp;points.Additionally, as the complexity of our
                     ecosystem grows, the goal of ‚Äúsingle PoC into information security‚Äù becomes increasingly
                     more difficult to maintain. The team is now investing in consistency and scalability
                     of partnership artifacts and communication channels, better redundancy and context
                     sharing on the team through squad operating models, crisper engagement criteria, and
                     definition of done for partnership engagements.Our Appsec Engineering team builds
                     products to help us scale, e.g.: a dynamic Asset Inventory that understands the nuances
                     of our bespoke engineering ecosystem and how our applications and data relate to each
                     other. This has evolved their identity to be a software engineering team that focuses
                     on security problems as opposed to a security engineering team that writes code/software.
                     Our hiring has reflected that shift, and we‚Äôve added more dedicated software engineers
                     (SWEs) to the team to help us build out software. With this shift, we‚Äôve incorporated
                     engineering best practices, and our products have appropriate investments toward reliability
                     and sustainability. As the team skews towards more software engineering focused talent,
                     ramping up to support the shared Appsec-focused on-call has been challenging.While
                     originally built to support AppSec use cases around providing guidance to developers
                     in a self-service way, interest in the rich data and relationships we have in our
                     tools, especially our Asset Inventory, has grown. As a result, we‚Äôve continued to
                     invest in making our solutions scalable and accessible, so security engineers can
                     get the data they need more easily to drive security use cases. We‚Äôve also discovered,
                     through interviews with engineers, that self-service guidance doesn‚Äôt stand on its
                     own. Moving forward, the team is investing in understanding our customer use cases
                     better, and shifting our self-service story toward higher-context, more opinionated
                     automated guidance to ensure developers have everything they need to make truly informed
                     decisions about the security of their applications (similar to how they might make
                     resiliency or other product decisions).As the Netflix business and engineering workforce
                     has grown, our software footprint has also grown and become more heterogeneous. At
                     the same time, partnerships have grown more and more strategic, and engineering has
                     grown more and more software-focused. As our team specialized, what emerged was a
                     loss of strategic focus for our AppSec Professional Services charter. These services
                     now need more dedicated strategic investment as the volume and support needs have
                     grown. So, we are now building out a dedicated capability focused on these critical
                     services that are important investments to be made and can no longer be served effectively
                     via a shared Appsec on-call. This will be our ‚ÄúAppsec Reviews and Assessments‚Äù function
                     and we are hiring for passionate, early career Appsec engineers to join this&nbsp;group.We
                     will continue to learn as we go through this next phase of evolution of our program.
                     We hope to continue to share these learnings with the broader community interested
                     in scalable product and application security.Scaling Appsec at Netflix (Part 2) was
                     originally published in Netflix TechBlog on Medium, where people are continuing the
                     conversation by highlighting and responding to this story.</span></summary><time datetime="2022-06-06T19:38:41+02:00">Mon, 6 Jun 2022 17:38</time><article><p>By <a href="https://twitter.com/astha_singhal">Astha Singhal</a>, <a href="https://twitter.com/Lak5hmi5udheer">Lakshmi Sudheer</a>, <a href="https://twitter.com/JuliaaMarieee">Julia¬†Knecht</a></p><p>The Application Security teams at Netflix are responsible for securing the software footprint that we create to run the Netflix product, the Netflix studio, and the business. Our customers are product and engineering teams at Netflix that build these software services and platforms. The Netflix <a href="https://jobs.netflix.com/culture">cultural values</a> of ‚ÄòContext not Control‚Äô and ‚ÄòFreedom and Responsibility‚Äô strongly influence how we do Security at Netflix. Our goal is to manage security risks to Netflix via clear, opinionated security guidance, and by providing risk context to Netflix engineering teams to make pragmatic risk decisions at¬†scale.</p><p>A few years ago, we published <a href="https://netflixtechblog.medium.com/scaling-appsec-at-netflix-6a13d7ab6043">this</a> blog post about how we had organized our team to focus our bandwidth on scalable investments as opposed to just traditional Appsec functions, which were not scaling well in our rapidly growing environment. We leaned into the idea of <strong>strategic security partnerships</strong> and<strong> automation investments </strong>to create more leverage for application security. This became the foundation for our current org structure with teams focused on <strong>Appsec Partnerships</strong> and <strong>Appsec Engineering</strong>. In this operating model, we provided critical Appsec operational services to Netflix‚Ää‚Äî‚Ääincluding bug bounty, pentesting, PSIRT (product security incident response), security reviews, and developer security education‚Ää‚Äî‚Äävia a shared on-call rotation.</p><figure><img alt="Team Structure v1" src="https://cdn-images-1.medium.com/max/478/1*iB0hJcUd299OSmlQrve92g.jpeg" /></figure><p>Over the past few years, this model has allowed us to focus on investments like<strong> Secure by Default</strong> for baseline security controls,<strong> Security Self-Service</strong> for clear actionable guidance and <strong>Vulnerability Scanning at scale</strong> for software supply chain security. We wanted to share an update on learnings from this model, how our needs have evolved, and where we expect to go from¬†here.</p><p>Among the most notable wins, we have been able to utilize this scale focused approach to <a href="https://netflixtechblog.com/the-show-must-go-on-securing-netflix-studios-at-scale-19b801c86479">productize</a> application security for our rapidly growing studio engineering ecosystem, standardize security baseline for all Enterprise apps, and build <a href="https://slideslive.com/38942583/securing-your-companys-crown-jewels-while-the-world-watches-the-crown?ref=tag-2196-latest">paved roads</a> to provide Secure by Default Authentication &amp; Authorization capabilities for central data engineering tools. Our focus has been on improving overall security assurance as opposed to just vulnerability prevention. We are now expanding this approach to more parts of our ecosystem. This mindset has also allowed us to invest our capacity for white-glove service towards reasonable residual risk and standard guidance so we can reduce the need for white-glove engagements in the long term (e.g., investment in an API proxy that provides baseline security controls for free as opposed to pentesting all applications that would eventually sit behind that API proxy). This approach has also allowed us to build strong relationships with central engineering teams at Netflix (Data Platform, Developer Tools, Cloud Infrastructure, IAM Product Engineering) that will continue to serve as central points of leverage for security in the long¬†term.</p><p>However, it has not been all sunshine and rainbows. On the partnership side, the bespoke nature of each partnership means that there isn‚Äôt consistency and redundancy built into the operating model and the related partnership artifacts (e.g., Security Strategy and Roadmap, Threat Model, Deliverable Tracking, Residual Risk Criteria, etc). This leads to insufficient context sharing and high operational churn every time we have personnel changes. The partnership charter has also grown laterally into the infrastructure space as we stack our leverage bets on infrastructure components (like Service Mesh, Container Platform, etc). The skill sets and domain depth in those partnerships has further diversified the skills on the team. But this is a tradeoff on our ability to serve generalized Appsec oncall needs like bug bounty triage with high consistency. Given that partnerships focus on long-running strategic initiatives, the wins can be few and far between and that can be difficult for team motivation. We also found various areas in which security partnership work bleeds into security product solutioning and it can be difficult to identify the appropriate handoff¬†points.</p><p>Additionally, as the complexity of our ecosystem grows, the goal of ‚Äúsingle PoC into information security‚Äù becomes increasingly more difficult to maintain. The team is now investing in <strong>consistency</strong> and <strong>scalability</strong> of partnership artifacts and communication channels, better <strong>redundancy</strong> and <strong>context sharing</strong> on the team through squad operating models, crisper <strong>engagement criteria</strong>, and <strong>definition of done</strong> for partnership engagements.</p><p>Our Appsec Engineering team builds products to help us scale, e.g.: a dynamic Asset Inventory that understands the nuances of our bespoke engineering ecosystem and how our applications and data relate to each other. This has evolved their identity to be a software engineering team that focuses on security problems as opposed to a security engineering team that writes code/software. Our hiring has reflected that shift, and we‚Äôve added more dedicated software engineers (SWEs) to the team to help us build out software. With this shift, we‚Äôve incorporated <strong>engineering best practices</strong>, and our products have appropriate investments toward <strong>reliability</strong> and <strong>sustainability</strong>. As the team skews towards more software engineering focused talent, ramping up to support the shared Appsec-focused on-call has been challenging.</p><p>While originally built to support AppSec use cases around providing guidance to developers in a self-service way, interest in the rich data and relationships we have in our tools, especially our Asset Inventory, has grown. As a result, we‚Äôve continued to invest in making our solutions scalable and accessible, so security engineers can get the data they need more easily to drive security use cases. We‚Äôve also discovered, through interviews with engineers, that self-service guidance doesn‚Äôt stand on its own. Moving forward, the team is investing in understanding our customer use cases better, and shifting our self-service story toward higher-context, more opinionated automated guidance to ensure developers have everything they need to make truly informed decisions about the security of their applications (similar to how they might make resiliency or other product decisions).</p><p>As the Netflix business and engineering workforce has grown, our software footprint has also grown and become more heterogeneous. At the same time, partnerships have grown more and more strategic, and engineering has grown more and more software-focused. As our team specialized, what emerged was a loss of strategic focus for our AppSec Professional Services charter. These services now need more dedicated strategic investment as the volume and support needs have grown. So, we are now building out a dedicated capability focused on these critical services that are important investments to be made and can no longer be served effectively via a shared Appsec on-call. This will be our <strong>‚ÄúAppsec Reviews and Assessments‚Äù</strong> function and we are <a href="https://jobs.netflix.com/jobs/210140918">hiring</a> for passionate, early career Appsec engineers to join this¬†group.</p><figure><img alt="Team Structure v2" src="https://cdn-images-1.medium.com/max/715/1*96PN1i-ScttDHm3NtKNCgw.jpeg" /></figure><p>We will continue to learn as we go through this next phase of evolution of our program. We hope to continue to share these learnings with the broader community interested in scalable product and application security.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c9e0f1488bc5" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/scaling-appsec-at-netflix-part-2-c9e0f1488bc5">Scaling Appsec at Netflix (Part 2)</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://netflixtechblog.com/a-survey-of-causal-inference-applications-at-netflix-b62d25175e6f?source=rss----2615bd06b42e---4">A Survey of Causal Inference Applications at Netflix</a> <span>At Netflix, we want to entertain the world through creating engaging content and helping
                     members discover the titles they will love. Key to that is understanding causal effects
                     that connect changes we make in the product to indicators of member&nbsp;joy.To measure
                     causal effects we rely heavily on AB testing, but we also leverage quasi-experimentation
                     in cases where AB testing is limited. Many scientists across Netflix have contributed
                     to the way that Netflix analyzes these causal&nbsp;effects.To celebrate that impact and
                     learn from each other, Netflix scientists recently came together for an internal Causal
                     Inference and Experimentation Summit. The weeklong conference brought speakers from
                     across the content, product, and member experience teams to learn about methodological
                     developments and applications in estimating causal effects. We covered a wide range
                     of topics including difference-in-difference estimation, double machine learning,
                     Bayesian AB testing, and causal inference in recommender systems among many&nbsp;others.We
                     are excited to share a sneak peek of the event with you in this blog post through
                     selected examples of the talks, giving a behind the scenes look at our community and
                     the breadth of causal inference at Netflix. We look forward to connecting with you
                     through a future external event and additional blog&nbsp;posts!Incremental Impact of LocalizationYinghong
                     Lan, Vinod Bakthavachalam, Lavanya Sharan, Marie Douriez, Bahar Azarnoush, Mason&nbsp;KrollAt
                     Netflix, we are passionate about connecting our members with great stories that can
                     come from anywhere, and be loved everywhere. In fact, we stream in more than 30 languages
                     and 190 countries and strive to localize the content, through subtitles and dubs,
                     that our members will enjoy the most. Understanding the heterogenous incremental value
                     of localization to member viewing is key to these&nbsp;efforts!In order to estimate the
                     incremental value of localization, we turned to causal inference methods using historical
                     data. Running large scale, randomized experiments has both technical and operational
                     challenges, especially because we want to avoid withholding localization from members
                     who might need it to access the content they&nbsp;love.Conceptual overview of using double
                     machine learning to control for confounders and compare similar titles to estimate
                     incremental impact of localizationWe analyzed the data across various languages and
                     applied double machine learning methods to properly control for measured confounders.
                     We not only studied the impact of localization on overall title viewing but also investigated
                     how localization adds value at different parts of the member journey. As a robustness
                     check, we explored various simulations to evaluate the consistency and variance of
                     our incrementality estimates. These insights have played a key role in our decisions
                     to scale localization and delight our members around the&nbsp;world.A related application
                     of causal inference methods to localization arose when some dubs were delayed due
                     to pandemic-related shutdowns of production studios. To understand the impact of these
                     dub delays on title viewing, we simulated viewing in the absence of delays using the
                     method of synthetic control. We compared simulated viewing to observed viewing at
                     title launch (when dubs were missing) and after title launch (when dubs were added&nbsp;back).To
                     control for confounders, we used a placebo test to repeat the analysis for titles
                     that were not affected by dub delays. In this way, we were able to estimate the incremental
                     impact of delayed dub availability on member viewing for impacted titles. Should there
                     be another shutdown of dub productions, this analysis enables our teams to make informed
                     decisions about delays with greater confidence.Holdback Experiments for Product InnovationTravis
                     Brooks, Cassiano Coria, Greg Nettles, Molly Jackman, Claire&nbsp;LacknerAt Netflix, there
                     are many examples of holdback AB tests, which show some users an experience without
                     a specific feature. They have substantially improved the member experience by measuring
                     long term effects of new features or re-examining old assumptions. However, when the
                     topic of holdback tests is raised, it can seem too complicated in terms of experimental
                     design and/or engineering costs.We aimed to share best practices we have learned about
                     holdback test design and execution in order to create more clarity around holdback
                     tests at Netflix, so they can be used more broadly across product innovation teams&nbsp;by:Defining
                     the types of holdbacks and their use cases with past&nbsp;examplesSuggesting future opportunities
                     where holdback testing may be&nbsp;valuableEnumerating the challenges that holdback tests&nbsp;poseIdentifying
                     future investments that can reduce the cost of deploying and maintaining holdback
                     tests for product and engineering teamsHoldback tests have clear value in many product
                     areas to confirm learnings, understand long term effects, retest old assumptions on
                     newer members, and measure cumulative value. They can also serve as a way to test
                     simplifying the product by removing unused features, creating a more seamless user
                     experience. In many areas at Netflix they are already commonly used for these purposes.Overview
                     of how holdback tests work where we keep the current experience for a subset of members
                     over the long term in order to gain valuable insights for improving the&nbsp;productWe
                     believe by unifying best practices and providing simpler tools, we can accelerate
                     our learnings and create the best product experience for our members to access the
                     content they&nbsp;love.Causal Ranker: A Causal Adaptation Framework for Recommendation
                     ModelsJeong-Yoon Lee, Sudeep&nbsp;DasMost machine learning algorithms used in personalization
                     and search, including deep learning algorithms, are purely associative. They learn
                     from the correlations between features and outcomes how to best predict a&nbsp;target.In
                     many scenarios, going beyond the purely associative nature to understanding the causal
                     mechanism between taking a certain action and the resulting incremental outcome becomes
                     key to decision making. Causal inference gives us a principled way of learning such
                     relationships, and when coupled with machine learning, becomes a powerful tool that
                     can be leveraged at&nbsp;scale.Compared to machine learning, causal inference allows us
                     to build a robust framework that controls for confounders in order to estimate the
                     true incremental impact to&nbsp;membersAt Netflix, many surfaces today are powered by recommendation
                     models like the personalized rows you see on your homepage. We believe that many of
                     these surfaces can benefit from additional algorithms that focus on making each recommendation
                     as useful to our members as possible, beyond just identifying the title or feature
                     someone is most likely to engage with. Adding this new model on top of existing systems
                     can help improve recommendations to those that are right in the moment, helping find
                     the exact title members are looking to stream&nbsp;now.This led us to create a framework
                     that applies a light, causal adaptive layer on top of the base recommendation system
                     called the Causal Ranker Framework. The framework consists of several components:
                     impression (treatment) to play (outcome) attribution, true negative label collection,
                     causal estimation, offline evaluation, and model&nbsp;serving.We are building this framework
                     in a generic way with reusable components so that any interested team within Netflix
                     can adopt this framework for their use case, improving our recommendations throughout
                     the&nbsp;product.Bellmania: Incremental Account Lifetime Valuation at Netflix and its ApplicationsReza
                     Badri, Allen&nbsp;TranUnderstanding the value of acquiring or retaining subscribers is
                     crucial for any subscription business like Netflix. While customer lifetime value
                     (LTV) is commonly used to value members, simple measures of LTV likely overstate the
                     true value of acquisition or retention because there is always a chance that potential
                     members may join in the future on their own without any intervention.We establish
                     a methodology and necessary assumptions to estimate the monetary value of acquiring
                     or retaining subscribers based on a causal interpretation of incremental LTV. This
                     requires us to estimate both on Netflix and off Netflix&nbsp;LTV.To overcome the lack of
                     data for off Netflix members, we use an approach based on Markov chains that recovers
                     off Netflix LTV from minimal data on non-subscriber transitions between being a subscriber
                     and canceling over&nbsp;time.Through Markov chains we can estimate the incremental value
                     of a member and non member that appropriately captures the value of potential joins
                     in the&nbsp;futureFurthermore, we demonstrate how this methodology can be used to (1) forecast
                     aggregate subscriber numbers that respect both addressable market constraints and
                     account-level dynamics, (2) estimate the impact of price changes on revenue and subscription
                     growth, and (3) provide optimal policies, such as price discounting, that maximize
                     expected lifetime revenue of&nbsp;members.Measuring causality is a large part of the data
                     science culture at Netflix, and we are proud to have so many stunning colleagues leverage
                     both experimentation and quasi-experimentation to drive member impact. The conference
                     was a great way to celebrate each other‚Äôs work and highlight the ways in which causal
                     methodology can create value for the business.We look forward to sharing more about
                     our work with the community in upcoming posts. To stay up to date on our work, follow
                     the Netflix Tech Blog, and if you are interested in joining us, we are currently looking
                     for new stunning colleagues to help us entertain the&nbsp;world!A Survey of Causal Inference
                     Applications at Netflix was originally published in Netflix TechBlog on Medium, where
                     people are continuing the conversation by highlighting and responding to this story.</span></summary><time datetime="2022-05-21T17:02:49+02:00">Sat, 21 May 2022 15:02</time><article><p>At Netflix, we want to entertain the world through creating engaging content and helping members discover the titles they will love. Key to that is understanding causal effects that connect changes we make in the product to indicators of member¬†joy.</p><p>To measure causal effects we rely heavily on <a href="https://netflixtechblog.com/decision-making-at-netflix-33065fa06481">AB testing</a>, but we also leverage <a href="https://netflixtechblog.com/quasi-experimentation-at-netflix-566b57d2e362">quasi-experimentation</a> in cases where AB testing is limited. Many scientists across Netflix have contributed to the way that Netflix analyzes these causal¬†effects.</p><p>To celebrate that impact and learn from each other, Netflix scientists recently came together for an internal Causal Inference and Experimentation Summit. The weeklong conference brought speakers from across the content, product, and member experience teams to learn about methodological developments and applications in estimating causal effects. We covered a wide range of topics including difference-in-difference estimation, double machine learning, Bayesian AB testing, and causal inference in recommender systems among many¬†others.</p><p>We are excited to share a sneak peek of the event with you in this blog post through selected examples of the talks, giving a behind the scenes look at our community and the breadth of causal inference at Netflix. We look forward to connecting with you through a future external event and additional blog¬†posts!</p><h4>Incremental Impact of Localization</h4><p><a href="https://www.linkedin.com/in/yinghong-lan-2368656b">Yinghong Lan</a>, <a href="https://www.linkedin.com/in/vinod-bakthavachalam">Vinod Bakthavachalam</a>, <a href="https://www.linkedin.com/in/lavanyasharan">Lavanya Sharan</a>, <a href="https://www.linkedin.com/in/mariedouriez/en">Marie Douriez</a>, <a href="https://www.linkedin.com/in/bahareh-azarnoush">Bahar Azarnoush</a>, <a href="https://www.linkedin.com/in/mason-kroll-19244946">Mason¬†Kroll</a></p><p>At Netflix, we are passionate about connecting our members with great stories that can come from anywhere, and be <a href="https://about.netflix.com/en/news/the-hottest-travel-destinations-have-been-one-story-away">loved everywhere</a>. In fact, we stream in more than 30 languages and 190 countries and strive to localize the content, through subtitles and dubs, that our members will enjoy the most. Understanding the heterogenous incremental value of localization to member viewing is key to these¬†efforts!</p><p>In order to estimate the incremental value of localization, we turned to causal inference methods using historical data. Running large scale, randomized experiments has both technical and operational challenges, especially because we want to avoid withholding localization from members who might need it to access the content they¬†love.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-C41nO6__c1UwCHK" /><figcaption>Conceptual overview of using double machine learning to control for confounders and compare similar titles to estimate incremental impact of localization</figcaption></figure><p>We analyzed the data across various languages and applied double machine learning methods to properly control for measured confounders. We not only studied the impact of localization on overall title viewing but also investigated how localization adds value at different parts of the member journey. As a robustness check, we explored various simulations to evaluate the consistency and variance of our incrementality estimates. These insights have played a key role in our decisions to scale localization and delight our members around the¬†world.</p><p>A related application of causal inference methods to localization arose when some dubs were delayed due to pandemic-related shutdowns of production studios. To understand the impact of these dub delays on title viewing, we simulated viewing in the absence of delays using the method of synthetic control. We compared simulated viewing to observed viewing at title launch (when dubs were missing) and after title launch (when dubs were added¬†back).</p><p>To control for confounders, we used a placebo test to repeat the analysis for titles that were not affected by dub delays. In this way, we were able to estimate the incremental impact of delayed dub availability on member viewing for impacted titles. Should there be another shutdown of dub productions, this analysis enables our teams to make informed decisions about delays with greater confidence.</p><h4>Holdback Experiments for Product Innovation</h4><p><a href="https://www.linkedin.com/in/traviscb1998">Travis Brooks</a>, <a href="https://www.linkedin.com/in/cassianocoria">Cassiano Coria</a>, <a href="https://www.linkedin.com/in/nettlesgreg">Greg Nettles</a>, <a href="https://www.linkedin.com/in/molly-jackman-1a757644">Molly Jackman</a>, <a href="https://www.linkedin.com/in/clairelackner">Claire¬†Lackner</a></p><p>At Netflix, there are many examples of holdback AB tests, which show some users an experience without a specific feature. They have substantially improved the member experience by measuring long term effects of new features or re-examining old assumptions. However, when the topic of holdback tests is raised, it can seem too complicated in terms of experimental design and/or engineering costs.</p><p>We aimed to share best practices we have learned about holdback test design and execution in order to create more clarity around holdback tests at Netflix, so they can be used more broadly across product innovation teams¬†by:</p><ol><li>Defining the types of holdbacks and their use cases with past¬†examples</li><li>Suggesting future opportunities where holdback testing may be¬†valuable</li><li>Enumerating the challenges that holdback tests¬†pose</li><li>Identifying future investments that can reduce the cost of deploying and maintaining holdback tests for product and engineering teams</li></ol><p>Holdback tests have clear value in many product areas to confirm learnings, understand long term effects, retest old assumptions on newer members, and measure cumulative value. They can also serve as a way to test simplifying the product by removing unused features, creating a more seamless user experience. In many areas at Netflix they are already commonly used for these purposes.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hAIxB_t6oOabezbS" /><figcaption>Overview of how holdback tests work where we keep the current experience for a subset of members over the long term in order to gain valuable insights for improving the¬†product</figcaption></figure><p>We believe by unifying best practices and providing simpler tools, we can accelerate our learnings and create the best product experience for our members to access the content they¬†love.</p><h4>Causal Ranker: A Causal Adaptation Framework for Recommendation Models</h4><p><a href="https://www.linkedin.com/in/jeongyoonlee">Jeong-Yoon Lee</a>, <a href="https://www.linkedin.com/in/datamusing">Sudeep¬†Das</a></p><p>Most machine learning algorithms used in personalization and search, including deep learning algorithms, are purely associative. They learn from the correlations between features and outcomes how to best predict a¬†target.</p><p>In many scenarios, going beyond the purely associative nature to understanding the causal mechanism between taking a certain action and the resulting incremental outcome becomes key to decision making. Causal inference gives us a principled way of learning such relationships, and when coupled with machine learning, becomes a powerful tool that can be leveraged at¬†scale.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*jTcnXuGax3_ZP2S2" /><figcaption>Compared to machine learning, causal inference allows us to build a robust framework that controls for confounders in order to estimate the true incremental impact to¬†members</figcaption></figure><p>At Netflix, many surfaces today are powered by recommendation models like the personalized rows you see on your homepage. We believe that many of these surfaces can benefit from additional algorithms that focus on making each recommendation as useful to our members as possible, beyond just identifying the title or feature someone is most likely to engage with. Adding this new model on top of existing systems can help improve recommendations to those that are right in the moment, helping find the exact title members are looking to stream¬†now.</p><p>This led us to create a framework that applies a light, causal adaptive layer on top of the base recommendation system called the Causal Ranker Framework. The framework consists of several components: impression (treatment) to play (outcome) attribution, true negative label collection, causal estimation, offline evaluation, and model¬†serving.</p><p>We are building this framework in a generic way with reusable components so that any interested team within Netflix can adopt this framework for their use case, improving our recommendations throughout the¬†product.</p><h4>Bellmania: Incremental Account Lifetime Valuation at Netflix and its Applications</h4><p><a href="https://www.linkedin.com/in/hamidreza-badri-97840347">Reza Badri</a>, <a href="https://www.linkedin.com/in/realallentran">Allen¬†Tran</a></p><p>Understanding the value of acquiring or retaining subscribers is crucial for any subscription business like Netflix. While customer lifetime value (LTV) is commonly used to value members, simple measures of LTV likely overstate the true value of acquisition or retention because there is always a chance that potential members may join in the future on their own without any intervention.</p><p>We establish a methodology and necessary assumptions to estimate the monetary value of acquiring or retaining subscribers based on a causal interpretation of incremental LTV. This requires us to estimate both on Netflix and off Netflix¬†LTV.</p><p>To overcome the lack of data for off Netflix members, we use an approach based on Markov chains that recovers off Netflix LTV from minimal data on non-subscriber transitions between being a subscriber and canceling over¬†time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dwm-RhoCP_S9XdkQ" /><figcaption>Through Markov chains we can estimate the incremental value of a member and non member that appropriately captures the value of potential joins in the¬†future</figcaption></figure><p>Furthermore, we demonstrate how this methodology can be used to (1) forecast aggregate subscriber numbers that respect both addressable market constraints and account-level dynamics, (2) estimate the impact of price changes on revenue and subscription growth, and (3) provide optimal policies, such as price discounting, that maximize expected lifetime revenue of¬†members.</p><p>Measuring causality is a large part of the <a href="https://netflixtechblog.com/netflix-a-culture-of-learning-394bc7d0f94c">data science culture</a> at Netflix, and we are proud to have so many stunning colleagues leverage both experimentation and quasi-experimentation to drive member impact. The conference was a great way to celebrate each other‚Äôs work and highlight the ways in which causal methodology can create value for the business.</p><p>We look forward to sharing more about our work with the community in upcoming posts. To stay up to date on our work, follow the <a href="https://netflixtechblog.com/">Netflix Tech Blog</a>, and if you are interested in joining us, we are currently looking for <a href="https://jobs.netflix.com/search?q=data%20science&amp;team=Data%20Science%20and%20Engineering">new stunning colleagues</a> to help us entertain the¬†world!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b62d25175e6f" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/a-survey-of-causal-inference-applications-at-netflix-b62d25175e6f">A Survey of Causal Inference Applications at Netflix</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://netflixtechblog.com/evolution-of-ml-fact-store-5941d3231762?source=rss----2615bd06b42e---4">Evolution of ML Fact Store</a> <span>by Vivek&nbsp;KaushalAt Netflix, we aim to provide recommendations that match our members‚Äô
                     interests. To achieve this, we rely on Machine Learning (ML) algorithms. ML algorithms
                     can be only as good as the data that we provide to it. This post will focus on the
                     large volume of high-quality data stored in Axion‚Ää‚Äî‚Ääour fact store that is leveraged
                     to compute ML features offline. We built Axion primarily to remove any training-serving
                     skew and make offline experimentation faster. We will share how its design has evolved
                     over the years and the lessons learned while building&nbsp;it.TerminologyAxion fact store
                     is part of our Machine Learning Platform, the platform that serves machine learning
                     needs across Netflix. Figure 1 below shows how Axion interacts with Netflix‚Äôs ML platform.
                     The overall ML platform has tens of components, and the diagram below only shows the
                     ones that are relevant to this post. To understand Axion‚Äôs design, we need to know
                     the various components that interact with&nbsp;it.Figure 1: Netflix ML ArchitectureFact:
                     A fact is data about our members or videos. An example of data about members is the
                     video they had watched or added to their My List. An example of video data is video
                     metadata, like the length of a video. Time is a critical component of Axion‚Ää‚Äî‚ÄäWhen
                     we talk about facts, we talk about facts at a moment in time. These facts are managed
                     and made available by services like viewing history or video metadata services outside
                     of&nbsp;Axion.Compute application: These applications generate recommendations for our
                     members. They fetch facts from respective data services, run feature encoders to generate
                     features and score the ML models to eventually generate recommendations.Offline feature
                     generator: We regenerate the values of the features that were generated for inferencing
                     in the compute application. Offline Feature Generator is a spark application that
                     enables on-demand generation of features using new, existing, or updated feature encoders.Shared
                     feature encoders: Feature encoders are shared between compute applications and offline
                     feature generators. We make sure there is no training/serving skew by using the same
                     data and the code for online and offline feature generation.MotivationFive years ago,
                     we posted and talked about the need for a ML fact store. The motivation has not changed
                     since then; the design has. This post focuses on the new design, but here is a summary
                     of why we built this fact&nbsp;store.Our machine learning models train on several weeks
                     of data. Thus, if we want to run an experiment with a new or modified feature encoder,
                     we need to build several weeks of feature data with this new or modified feature encoder.
                     We have two options to collect features using this updated feature&nbsp;encoder.The first
                     is to log features from the compute applications, popularly known as feature logging.
                     We can deploy updated feature encoders in our compute applications and then wait for
                     them to log the feature values. Since we train our models on several weeks of data,
                     this method is slow for us as we will have to wait for several weeks for the data
                     collection.An alternative to feature logging is to regenerate the features with updated
                     feature encoders. If we can access the historical facts, we can regenerate the features
                     using updated feature encoders. Regeneration takes hours compared to weeks taken by
                     the feature logging. Thus, we decided to go this route and started storing facts to
                     reduce the time it takes to run an experiment with new or modified features.Design
                     evolutionAxion fact store has four components‚Ää‚Äî‚Ääfact logging client, ETL, query client,
                     and data quality infrastructure. We will describe how the design evolved in these
                     components.Evolution of Fact Logging&nbsp;ClientCompute applications access facts (members‚Äô
                     viewing history, their likes and my list information, etc.) from various grpc services
                     that power the whole Netflix experience. These facts are used to generate features
                     using shared feature encoders, which in turn are used by ML models to generate recommendations.
                     After generating the recommendations, compute applications use Axion‚Äôs fact logging
                     client to log these&nbsp;facts.At a later stage in the offline pipelines, the offline feature
                     generator uses these logged facts to regenerate temporally accurate features. Temporal
                     accuracy, in this context, is the ability to regenerate the exact set of features
                     that were generated for the recommendations. This temporal accuracy of features is
                     key to removing the training-serving skew.The first version of our logger library
                     optimized for storage by deduplicating facts and optimized for network i/o using different
                     compression methods for each fact. Then we started hitting roadblocks while optimizing
                     the query performance. Since we were optimizing at the logging level for storage and
                     performance, we had less data and metadata to play with to optimize the query performance.Eventually,
                     we decided to simplify the logger. Now we asynchronously collect all the facts and
                     metadata into a protobuf, compress it, and send it to the keystone messaging service.Evolution
                     of ETL and Query&nbsp;ClientETL and Query Client are intertwined, as any ETL changes could
                     directly impact the query performance. ETL is the component where we experiment for
                     query performance, improving data quality, and storage optimization. Figure 2 shows
                     components of Axion‚Äôs ETL and its interaction with the query&nbsp;client.Fig 2: Internal
                     components of&nbsp;AxionAxion‚Äôs fact logging client logs facts to the keystone real-time
                     stream processing platform, which outputs data to an Iceberg table. We use Keystone
                     as it is easy to use, reliable, scalable, and provides aggregation of facts from different
                     cloud regions into a single AWS region. Having all data in a single AWS region exposes
                     us to a single point of failure but it significantly reduces the operational overhead
                     of our ETL pipelines which we believe makes it a worthwhile trade-off. We currently
                     send all the facts into a single Keystone stream which we have configured to write
                     to a single Iceberg table. We plan to split these Keystone streams into multiple streams
                     for horizontal scalability.The Iceberg table created by Keystone contains large blobs
                     of unstructured data. These large unstructured blogs are not efficient for querying,
                     so we need to transform and store this data in a different format to allow efficient
                     queries. One might think that normalizing it would make storage and querying more
                     efficient, albeit at the cost of writing more complex queries. Hence, our first approach
                     was to normalize the incoming data and store it in multiple tables. We soon realized
                     that, while space-optimized, it made querying very inefficient for the scale of data
                     we needed to handle. We ran into various shuffle issues in Spark as we were joining
                     several big tables at query&nbsp;time.We then decided to denormalize the data and store
                     all facts and metadata in one Iceberg table using nested Parquet format. While storing
                     in one Iceberg table was not as space-optimized, Parquet did provide us with significant
                     savings in storage costs, and most importantly, it made our Spark queries succeed.
                     However, Spark query execution remained slow. Further attempts to optimize query performance,
                     like using bloom filters and predicate pushdown, were successful but still far away
                     from where we wanted it to&nbsp;be.Why was querying the single Iceberg table&nbsp;slow?What‚Äôs
                     our end goal? We want to train our ML models to personalize the member experience.
                     We have a plethora of ML models that drive personalization. Each of these models are
                     trained with different datasets and features along with different stratification and
                     objectives. Given that Axion is used as the defacto Fact store for assembling the
                     training dataset for all these models, it is important for Axion to log and store
                     enough facts that would be sufficient for all these models. However, for a given ML
                     model, we only require a subset of the data stored in Axion for its training needs.
                     We saw queries filtering down an input dataset of several hundred million rows to
                     less than a million in extreme cases. Even with bloom filters, the query performance
                     was slow because the query was downloading all of the data from s3 and then dropping
                     it. As our label dataset was also random, presorting facts data also did not&nbsp;help.We
                     realized that our options with Iceberg were limited if we only needed data for a million
                     rows‚Ää‚Äî‚Ääout of several hundred million‚Ää‚Äî‚Ääand we had no additional information to optimize
                     our queries. So we decided not to further optimize joins with the Iceberg data and
                     instead move to an alternate approach.Low-latency QueriesTo avoid downloading all
                     of the fact data from s3 in a spark executor and then dropping it, we analyzed our
                     query patterns and figured out that there is a way to only access the data that we
                     are interested in. This was achieved by introducing an EVCache, a key-value store,
                     which stores facts and indices optimized for these particular query patterns.Let‚Äôs
                     see how the solution works for one of these query patterns‚Ää‚Äî‚Ääquerying by member id.
                     We first query the index by member id to find the keys for the facts of that member
                     and query those facts from EVCache in parallel. So, we make multiple calls to the
                     key-value store for each row in our training set. Even when accounting for these multiple
                     calls, the query performance is an order of magnitude faster than scanning several
                     hundred times more data stored in the Iceberg table. Depending on the use case, EVCache
                     queries can be 3x-50x faster than&nbsp;Iceberg.The only problem with this approach is that
                     EVCache is more expensive than Iceberg storage, so we need to limit the amount of
                     data stored. So, for the queries that request data not available in EVCache, our only
                     option is to query Iceberg. In the future, we want to store all facts in EVCache by
                     optimizing how we store data in&nbsp;EVCache.How do we monitor the quality of&nbsp;data?Over
                     the years, we learned the importance of having comprehensive data quality checks for
                     our datasets. Corruption in data can significantly impact production model performance
                     and A/B test results. From the ML researchers‚Äô perspective, it doesn‚Äôt matter if Axion
                     or a component outside of Axion corrupted the data. When they read the data from Axion,
                     if it is bad, it is a loss of trust in Axion. For Axion to become the defacto fact
                     store for all Personalization ML models, the research teams needed to trust the quality
                     of data stored. Hence, we designed a comprehensive system that monitors the quality
                     of data flowing through Axion to detect corruptions, whether introduced by Axion or
                     outside&nbsp;Axion.We bucketed data corruptions observed when reading data from Axion on
                     three dimensions:The impact on a value in data: Was the value missing? Did a new value
                     appear (unintentionally)? Was the value replaced with a different value?The spread
                     of data corruption: Did data corruption have a row or columnar impact? Did the corruption
                     impact one pipeline or multiple ML pipelines?The source of data corruption: Was data
                     corrupted by components outside of Axion? Did Axion components corrupt data? Was data
                     corrupted at&nbsp;rest?We came up with three different approaches to detect data corruption,
                     wherein each approach can detect corruption along multiple dimensions described above.AggregationsData
                     volume logged to Axion datastore is predictable. Compute applications follow daily
                     trends. Some log consistently every hour, others log for a few hours every day. We
                     aggregate the counts on dimensions like total records, compute application, fact counts
                     etc. Then we use a rule-based approach to validate the counts are within a certain
                     threshold of past trends. Alerts are triggered when counts vary outside these thresholds.
                     These trend-based alerts are helpful with missing or new data; row-level impact, and
                     pipelines impact. They help with column-level impact only on rare occasions.Consistent
                     samplingWe sample a small percentage of the data based on a predictable member id
                     hash and store it in separate tables. By consistent sampling across different data
                     stores and pipelines, we can run canaries on this smaller subset and get output quickly.
                     We also compare the output of these canaries against production to detect unintended
                     changes in data during new code deployment. One downside of consistent sampling is
                     that it may not catch rare issues, especially if the rate of data corruption is significantly
                     lower than our sampling rate. Consistent sampling checks help detect attribute impact‚Ää‚Äî‚Äänew,
                     missing, or replacement; columnar impact, and single pipeline&nbsp;issues.Random samplingWhile
                     the above two strategies combined can detect most data corruptions, they do occasionally
                     miss. For those rare occasions, we rely on random sampling. We randomly query a subset
                     of the data multiple times every hour. Both hot and cold data, i.e., recently logged
                     data and data logged a while ago, are randomly sampled. We expect these queries to
                     pass without issues. When they fail, it is either due to bad data or issues with the
                     underlying infrastructure. While we think of it as an ‚ÄúI‚Äôm feeling lucky‚Äù strategy,
                     it does work as long as we read significantly more data than the rate of corrupted
                     data.Another advantage to random sampling is maintaining the quality of unused facts.
                     Axion users do not read a significant percentage of facts logged to Axion, and we
                     need to make sure that these unused facts are of good quality as they can be used
                     in the future. We have pipelines that randomly read these unused facts and alert when
                     the query does not get the expected output. In terms of impact, these random checks
                     are like winning a lottery‚Ää‚Äî‚Ääyou win occasionally, and you never know how big it&nbsp;is.Results
                     from monitoring data&nbsp;qualityWe deployed the above three monitoring approaches more
                     than two years ago, and since then, we have identified more than 95% of data issues
                     early. We have also significantly improved the stability of our customer pipelines.
                     If you want to know more about how we monitor data quality in Axion, you can check
                     our spark summit talk and this&nbsp;podcast.Learnings from Axion‚Äôs evolutionWe learned
                     from designing this fact store to start with a simple design and avoid premature optimizations
                     that add complexity. Pay the storage, network, and compute cost. As the product becomes
                     available to the customers, new use cases will pop up that will be harder to support
                     with a complex design. Once the customers have adopted the product, start looking
                     into optimizations.While ‚Äúkeep the design simple‚Äù is a frequently shared learning
                     in software engineering, it is not always easy to achieve. For example, we learned
                     that our fact logging client can be simple with minimal business logic, but our query
                     client needs to be functionality-rich. Our learning is that if we need to add complexity,
                     add it in the least number of components instead of spreading it&nbsp;out.Another learning
                     is that we should have invested early into a robust testing framework. Unit tests
                     and integration tests only took us so far. We needed scalability testing and performance
                     testing as well. This scalability and performance testing framework helped stabilize
                     the system because, without it, we ran into issues that took us weeks to clean&nbsp;up.Lastly,
                     we learned that we should run data migrations and push the breaking API changes as
                     soon as possible. As more customers adopt Axion, running data migrations and making
                     breaking API changes are becoming harder and&nbsp;harder.Conclusion and future&nbsp;workAxion
                     is our primary data source that is used extensively by all our Personalization ML
                     models for offline feature generation. Given that it ensures that there is no training/serving
                     skew and that it has significantly reduced offline feature generation latencies we
                     are now starting to make it the defacto Fact store for other ML use cases within&nbsp;Netflix.We
                     do have use cases that are not served well with the current design, like bandits,
                     because our current design limits storing a map per row creating a limitation when
                     a compute application needs to log multiple values for the same key. Also, as described
                     in the design, we want to optimize how we store data in EVCache to enable us to store
                     more&nbsp;data.If you are interested in working on similar challenges, join&nbsp;us.Evolution
                     of ML Fact Store was originally published in Netflix TechBlog on Medium, where people
                     are continuing the conversation by highlighting and responding to this story.</span></summary><time datetime="2022-04-26T22:44:04+02:00">Tue, 26 Apr 2022 20:44</time><article><p>by <a href="https://www.linkedin.com/in/vkaushal21/">Vivek¬†Kaushal</a></p><p>At Netflix, we aim to provide recommendations that match our members‚Äô interests. To achieve this, we rely on Machine Learning (ML) algorithms. ML algorithms can be only as good as the data that we provide to it. This post will focus on the large volume of high-quality data stored in Axion‚Ää‚Äî‚Ääour fact store that is leveraged to compute ML features offline. We built Axion primarily to remove any training-serving skew and make offline experimentation faster. We will share how its design has evolved over the years and the lessons learned while building¬†it.</p><h3>Terminology</h3><p>Axion fact store is part of our Machine Learning Platform, the platform that serves machine learning needs across Netflix. Figure 1 below shows how Axion interacts with Netflix‚Äôs ML platform. The overall ML platform has tens of components, and the diagram below only shows the ones that are relevant to this post. To understand Axion‚Äôs design, we need to know the various components that interact with¬†it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*a37mfb9KtcdEL_mn" /><figcaption>Figure 1: Netflix ML Architecture</figcaption></figure><ul><li><strong>Fact: </strong>A fact is data about our members or videos. An example of data about members is the video they had watched or added to their My List. An example of video data is video metadata, like the length of a video. Time is a critical component of Axion‚Ää‚Äî‚ÄäWhen we talk about facts, we talk about facts at a moment in time. These facts are managed and made available by services like <a href="https://netflixtechblog.com/netflixs-viewing-data-how-we-know-where-you-are-in-house-of-cards-608dd61077da">viewing history</a> or video metadata services outside of¬†Axion.</li><li><strong>Compute application: </strong>These applications generate recommendations for our members. They fetch facts from respective data services, run feature encoders to generate features and score the ML models to eventually generate recommendations.</li><li><strong>Offline feature generator: </strong>We regenerate the values of the features that were generated for inferencing in the compute application. Offline Feature Generator is a spark application that enables on-demand generation of features using new, existing, or updated feature encoders.</li><li><strong>Shared feature encoders: </strong>Feature encoders are shared between compute applications and offline feature generators. We make sure there is no training/serving skew by using the same data and the code for online and offline feature generation.</li></ul><h3>Motivation</h3><p>Five years ago, we <a href="https://netflixtechblog.com/distributed-time-travel-for-feature-generation-389cccdd3907">posted</a> and <a href="https://www.youtube.com/watch?v=DiwKg8KynVU">talked</a> about the need for a ML fact store. The motivation has not changed since then; the design has. This post focuses on the new design, but here is a summary of why we built this fact¬†store.</p><p>Our machine learning models train on several weeks of data. Thus, if we want to run an experiment with a new or modified feature encoder, we need to build several weeks of feature data with this new or modified feature encoder. We have two options to collect features using this updated feature¬†encoder.</p><p>The first is to log features from the compute applications, popularly known as feature logging. We can deploy updated feature encoders in our compute applications and then wait for them to log the feature values. Since we train our models on several weeks of data, this method is slow for us as we will have to wait for several weeks for the data collection.</p><p>An alternative to feature logging is to regenerate the features with updated feature encoders. If we can access the historical facts, we can regenerate the features using updated feature encoders. Regeneration takes hours compared to weeks taken by the feature logging. Thus, we decided to go this route and started storing facts to reduce the time it takes to run an experiment with new or modified features.</p><h3>Design evolution</h3><p>Axion fact store has four components‚Ää‚Äî‚Ääfact logging client, ETL, query client, and data quality infrastructure. We will describe how the design evolved in these components.</p><h3>Evolution of Fact Logging¬†Client</h3><p>Compute applications access facts (members‚Äô viewing history, their likes and my list information, etc.) from various <a href="https://netflixtechblog.com/practical-api-design-at-netflix-part-1-using-protobuf-fieldmask-35cfdc606518">grpc services</a> that power the whole Netflix experience. These facts are used to generate features using shared feature encoders, which in turn are used by ML models to generate recommendations. After generating the recommendations, compute applications use Axion‚Äôs fact logging client to log these¬†facts.</p><p>At a later stage in the offline pipelines, the offline feature generator uses these logged facts to regenerate temporally accurate features. Temporal accuracy, in this context, is the ability to regenerate the exact set of features that were generated for the recommendations. This temporal accuracy of features is key to removing the training-serving skew.</p><p>The first version of our logger library optimized for storage by deduplicating facts and optimized for network i/o using different compression methods for each fact. Then we started hitting roadblocks while optimizing the query performance. Since we were optimizing at the logging level for storage and performance, we had less data and metadata to play with to optimize the query performance.</p><p>Eventually, we decided to simplify the logger. Now we asynchronously collect all the facts and metadata into a protobuf, compress it, and send it to the <a href="https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a">keystone messaging service</a>.</p><h3>Evolution of ETL and Query¬†Client</h3><p>ETL and Query Client are intertwined, as any ETL changes could directly impact the query performance. ETL is the component where we experiment for query performance, improving data quality, and storage optimization. Figure 2 shows components of Axion‚Äôs ETL and its interaction with the query¬†client.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*DGVK0429DO9BPMwd" /><figcaption>Fig 2: Internal components of¬†Axion</figcaption></figure><p>Axion‚Äôs fact logging client logs facts to the <a href="https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a">keystone real-time stream processing platform</a>, which outputs data to an <a href="https://github.com/Netflix/iceberg">Iceberg</a> table. We use Keystone as it is easy to use, reliable, scalable, and provides aggregation of facts from different cloud regions into a single AWS region. Having all data in a single AWS region exposes us to a single point of failure but it significantly reduces the operational overhead of our ETL pipelines which we believe makes it a worthwhile trade-off. We currently send all the facts into a single Keystone stream which we have configured to write to a single Iceberg table. We plan to split these Keystone streams into multiple streams for horizontal scalability.</p><p>The Iceberg table created by Keystone contains large blobs of unstructured data. These large unstructured blogs are not efficient for querying, so we need to transform and store this data in a different format to allow efficient queries. One might think that normalizing it would make storage and querying more efficient, albeit at the cost of writing more complex queries. Hence, our first approach was to normalize the incoming data and store it in multiple tables. We soon realized that, while space-optimized, it made querying very inefficient for the scale of data we needed to handle. We ran into various shuffle issues in Spark as we were joining several big tables at query¬†time.</p><p>We then decided to denormalize the data and store all facts and metadata in one Iceberg table using nested Parquet format. While storing in one Iceberg table was not as space-optimized, Parquet did provide us with significant savings in storage costs, and most importantly, it made our Spark queries succeed. However, Spark query execution remained slow. Further attempts to optimize query performance, like using bloom filters and predicate pushdown, were successful but still far away from where we wanted it to¬†be.</p><h3>Why was querying the single Iceberg table¬†slow?</h3><p>What‚Äôs our end goal? We want to train our ML models to personalize the member experience. We have a plethora of ML models that drive personalization. Each of these models are trained with different datasets and features along with different stratification and objectives. Given that Axion is used as the defacto Fact store for assembling the training dataset for all these models, it is important for Axion to log and store enough facts that would be sufficient for all these models. However, for a given ML model, we only require a subset of the data stored in Axion for its training needs. We saw queries filtering down an input dataset of several hundred million rows to less than a million in extreme cases. Even with bloom filters, the query performance was slow because the query was downloading all of the data from s3 and then dropping it. As our label dataset was also random, presorting facts data also did not¬†help.</p><p>We realized that our options with Iceberg were limited if we only needed data for a million rows‚Ää‚Äî‚Ääout of several hundred million‚Ää‚Äî‚Ääand we had no additional information to optimize our queries. So we decided not to further optimize joins with the Iceberg data and instead move to an alternate approach.</p><h3>Low-latency Queries</h3><p>To avoid downloading all of the fact data from s3 in a spark executor and then dropping it, we analyzed our query patterns and figured out that there is a way to only access the data that we are interested in. This was achieved by introducing an <a href="https://github.com/Netflix/EVCache">EVCache</a>, a key-value store, which stores facts and indices optimized for these particular query patterns.</p><p>Let‚Äôs see how the solution works for one of these query patterns‚Ää‚Äî‚Ääquerying by member id. We first query the index by member id to find the keys for the facts of that member and query those facts from EVCache in parallel. So, we make multiple calls to the key-value store for each row in our training set. Even when accounting for these multiple calls, the query performance is an order of magnitude faster than scanning several hundred times more data stored in the Iceberg table. Depending on the use case, EVCache queries can be 3x-50x faster than¬†Iceberg.</p><p>The only problem with this approach is that EVCache is more expensive than Iceberg storage, so we need to limit the amount of data stored. So, for the queries that request data not available in EVCache, our only option is to query Iceberg. In the future, we want to store all facts in EVCache by optimizing how we store data in¬†EVCache.</p><h3>How do we monitor the quality of¬†data?</h3><p>Over the years, we learned the importance of having comprehensive data quality checks for our datasets. Corruption in data can significantly impact production model performance and A/B test results. From the ML researchers‚Äô perspective, it doesn‚Äôt matter if Axion or a component outside of Axion corrupted the data. When they read the data from Axion, if it is bad, it is a loss of trust in Axion. For Axion to become the defacto fact store for all Personalization ML models, the research teams needed to trust the quality of data stored. Hence, we designed a comprehensive system that monitors the quality of data flowing through Axion to detect corruptions, whether introduced by Axion or outside¬†Axion.</p><p>We bucketed data corruptions observed when reading data from Axion on three dimensions:</p><ul><li>The impact on a value in data: Was the value missing? Did a new value appear (unintentionally)? Was the value replaced with a different value?</li><li>The spread of data corruption: Did data corruption have a row or columnar impact? Did the corruption impact one pipeline or multiple ML pipelines?</li><li>The source of data corruption: Was data corrupted by components outside of Axion? Did Axion components corrupt data? Was data corrupted at¬†rest?</li></ul><p>We came up with three different approaches to detect data corruption, wherein each approach can detect corruption along multiple dimensions described above.</p><h3>Aggregations</h3><p>Data volume logged to Axion datastore is predictable. Compute applications follow daily trends. Some log consistently every hour, others log for a few hours every day. We aggregate the counts on dimensions like total records, compute application, fact counts etc. Then we use a rule-based approach to validate the counts are within a certain threshold of past trends. Alerts are triggered when counts vary outside these thresholds. These trend-based alerts are helpful with missing or new data; row-level impact, and pipelines impact. They help with column-level impact only on rare occasions.</p><h3>Consistent sampling</h3><p>We sample a small percentage of the data based on a predictable member id hash and store it in separate tables. By consistent sampling across different data stores and pipelines, we can run canaries on this smaller subset and get output quickly. We also compare the output of these canaries against production to detect unintended changes in data during new code deployment. One downside of consistent sampling is that it may not catch rare issues, especially if the rate of data corruption is significantly lower than our sampling rate. Consistent sampling checks help detect attribute impact‚Ää‚Äî‚Äänew, missing, or replacement; columnar impact, and single pipeline¬†issues.</p><h3>Random sampling</h3><p>While the above two strategies combined can detect most data corruptions, they do occasionally miss. For those rare occasions, we rely on random sampling. We randomly query a subset of the data multiple times every hour. Both hot and cold data, i.e., recently logged data and data logged a while ago, are randomly sampled. We expect these queries to pass without issues. When they fail, it is either due to bad data or issues with the underlying infrastructure. While we think of it as an ‚ÄúI‚Äôm feeling lucky‚Äù strategy, it does work as long as we read significantly more data than the rate of corrupted data.</p><p>Another advantage to random sampling is maintaining the quality of unused facts. Axion users do not read a significant percentage of facts logged to Axion, and we need to make sure that these unused facts are of good quality as they can be used in the future. We have pipelines that randomly read these unused facts and alert when the query does not get the expected output. In terms of impact, these random checks are like winning a lottery‚Ää‚Äî‚Ääyou win occasionally, and you never know how big it¬†is.</p><h3>Results from monitoring data¬†quality</h3><p>We deployed the above three monitoring approaches more than two years ago, and since then, we have identified more than 95% of data issues early. We have also significantly improved the stability of our customer pipelines. If you want to know more about how we monitor data quality in Axion, you can check our <a href="https://databricks.com/session_na20/an-approach-to-data-quality-for-netflix-personalization-systems">spark summit talk</a> and <a href="https://databand.ai/mad-data-podcast/where-should-data-quality-sit-it-depends/">this¬†podcast</a>.</p><h3>Learnings from Axion‚Äôs evolution</h3><p>We learned from designing this fact store to start with a simple design and avoid premature optimizations that add complexity. Pay the storage, network, and compute cost. As the product becomes available to the customers, new use cases will pop up that will be harder to support with a complex design. Once the customers have adopted the product, start looking into optimizations.</p><p>While ‚Äú<em>keep the design simple</em>‚Äù is a frequently shared learning in software engineering, it is not always easy to achieve. For example, we learned that our fact logging client can be simple with minimal business logic, but our query client needs to be functionality-rich. Our learning is that if we need to add complexity, add it in the least number of components instead of spreading it¬†out.</p><p>Another learning is that we should have invested early into a robust testing framework. Unit tests and integration tests only took us so far. We needed scalability testing and performance testing as well. This scalability and performance testing framework helped stabilize the system because, without it, we ran into issues that took us weeks to clean¬†up.</p><p>Lastly, we learned that we should run data migrations and push the breaking API changes as soon as possible. As more customers adopt Axion, running data migrations and making breaking API changes are becoming harder and¬†harder.</p><h3>Conclusion and future¬†work</h3><p>Axion is our primary data source that is used extensively by all our <a href="https://research.netflix.com/research-area/recommendations">Personalization ML models</a> for offline feature generation. Given that it ensures that there is no training/serving skew and that it has significantly reduced offline feature generation latencies we are now starting to make it the defacto Fact store for other ML use cases within¬†Netflix.</p><p>We do have use cases that are not served well with the current design, like bandits, because our current design limits storing a map per row creating a limitation when a compute application needs to log multiple values for the same key. Also, as described in the design, we want to optimize how we store data in EVCache to enable us to store more¬†data.</p><p>If you are interested in working on similar challenges, <a href="https://jobs.netflix.com/search">join¬†us</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5941d3231762" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/evolution-of-ml-fact-store-5941d3231762">Evolution of ML Fact Store</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf?source=rss----2615bd06b42e---4">How Netflix Content Engineering makes a federated graph searchable</a> <span>By Alex Hutter, Falguni Jhaveri and Senthil SayeebabaOver the past few years Content
                     Engineering at Netflix has been transitioning many of its services to use a federated
                     GraphQL platform. GraphQL federation enables domain teams to independently build and
                     operate their own Domain Graph Services (DGS) and, at the same time, connect their
                     domain with other domains in a unified GraphQL schema exposed by a federated gateway.As
                     an example, let‚Äôs examine three core entities of the graph, each owned by separate
                     engineering teams:Movie: At Netflix, we make titles (shows, films, shorts etc.). For
                     simplicity, let‚Äôs assume each title is a Movie&nbsp;object.Production: Each Movie is associated
                     with a Studio Production. A Production object tracks everything needed to make a Movie
                     including shooting location, vendors, and&nbsp;more.Talent: the people working on a Movie
                     are the Talent, including actors, directors, and so&nbsp;on.Sample GraphQL&nbsp;SchemaOnce entities
                     like the above are available in the graph, it‚Äôs very common for folks to want to query
                     for a particular entity based on attributes of related entities, e.g. give me all
                     movies that are currently in photography with Ryan Reynolds as an&nbsp;actor.In a federated
                     graph architecture, how can we answer such a query given that each entity is served
                     by its own service? The Movie service would need to provide an endpoint that accepts
                     a query and filters that may apply to data the service does not own, and use those
                     to identify the appropriate Movie entities to&nbsp;return.In fact, every entity owning
                     service could be required to do this&nbsp;work.This common problem of making a federated
                     graph searchable led to the creation of Studio&nbsp;Search.The Studio Search platform was
                     designed to take a portion of the federated graph, a subgraph rooted at an entity
                     of interest, and make it searchable. The entities of the subgraph can be queried with
                     text input, filtered, ranked, and faceted. In the next section, we‚Äôll discuss how
                     we made this possible.Introducing Studio&nbsp;SearchWhen hearing that we want to enable
                     teams to search something, your mind likely goes to building an index of some kind.
                     Ours did too! So we need to build an index of a portion of the federated graph.How
                     do our users tell us which portion and, even more critically, given that the portion
                     of the graph of interest will almost definitely span data exposed by many services,
                     how do we keep the index current with all these various services?We chose Elasticsearch
                     as the underlying technology for our index and determined that there were three main
                     pieces of information required to build out an indexing pipeline:A definition of their
                     subgraph of interest rooted at the entity they primarily will be searching forEvents
                     to notify the platform of changes to entities in the&nbsp;subgraphIndex specific configuration
                     such as whether a field should be used for full text queries or whether a sub-document
                     is&nbsp;nestedIn short, our solution was to build an index for the subgraphs of interest.
                     This index needs to be kept up-to-date with the data exposed by the various services
                     in the federated graph in near-real time.GraphQL gives us a straightforward way to
                     define the subgraph‚Ää‚Äî‚Ääa single templated GraphQL query that pulls all of the data
                     the user is interested in using in their searches.Here‚Äôs an example GraphQL query
                     template. It‚Äôs pulling data for Movies and their related Productions and&nbsp;Talent.Sample
                     GraphQL&nbsp;queryTo keep the index up to date, events are used to trigger a reindexing
                     operation for individual entities when they change. Change Data Capture (CDC) events
                     are the preferred events for triggering these operations‚Ää‚Äî‚Äämost teams produce them
                     using Netflix‚Äôs CDC connectors‚Ää‚Äî‚Äähowever, application events are also supported when
                     necessary.All data to be indexed is being fetched from the federated graph so all
                     that is needed in the events is an entity id; the id can be substituted into the GraphQL
                     query template to fetch the entity and any related&nbsp;data.Using the type information
                     present in the GraphQL query template and the user specified index configuration we
                     were able to create an index template with a set of custom Elasticsearch text analyzers
                     that generalized well across&nbsp;domains.Given these inputs, a Data Mesh pipeline can
                     be created that consists of the user provided CDC event source, a processor to enrich
                     those events using the user provided GraphQL query and a sink to Elasticsearch.ArchitecturePutting
                     this all together, below you can see a simplified view of the architecture.Studio
                     Search Indexing ArchitectureStudio applications produce events to schematized Kafka
                     streams within Data&nbsp;Mesh.a. By transacting with a database which is monitored by a
                     CDC connector that creates events,&nbsp;orb. By directly creating events using a Data Mesh&nbsp;client.2.
                     The schematized events are consumed by Data Mesh processors implemented in the Apache
                     Flink framework. Some entities have multiple events for their changes so we leverage
                     union processors to combine data from multiple Kafka&nbsp;streams.a. A GraphQL processor
                     executes the user provided GraphQL query to fetch documents from the federated gateway.b.
                     The federated gateway, in turn, fetches data from the Studio applications.3. The documents
                     fetched from the federated gateway are put onto another schematized Kafka topic before
                     being processed by an Elasticsearch sink in Data Mesh that indexes them into Elasticsearch
                     index configured with an indexing template created specifically for the fields and
                     types present in the document.Reverse lookupsYou may have noticed something missing
                     in the above explanation. If the index is being populated based on Movie id events,
                     how does it stay up to date when a Production or Talent changes? Our solution to this
                     is a reverse lookup‚Ää‚Äî‚Ääwhen a change to a related entity is made, we need to look up
                     all of the primary entities that could be affected and trigger events for those. We
                     do this by consulting the index itself and querying for all primary entities related
                     to the entity that has&nbsp;changed.For instance if our index has a document that looks
                     like&nbsp;this:Sample Elasticsearch documentAnd the pipeline observes a change to the Production
                     with ptpId ‚Äúabc‚Äù, we can query the index for all documents with production.ptpId ==
                     ‚Äúabc‚Äù and extract the movieId. Then, we can pass that movieId down into the rest of
                     the indexing pipeline.Scaling the&nbsp;SolutionThe solution we came up with worked quite
                     well. Teams were easily able to share the requirements for their subgraph‚Äôs index
                     via a GraphQL query template and could use existing tooling to generate the events
                     to enable the index to be kept up to date in near real-time. Reusing the index itself
                     to power reverse lookups enabled us to keep all the logic for handling related entities
                     contained within our systems and shield our users from that complexity. In fact it
                     worked so well that we became inundated with requests to integrate with Studio Search‚Ää‚Äî‚Ääit
                     began to power a significant portion of the user experience for many applications
                     within Content Engineering.Early on, we did integrations by hand but as adoption of
                     Studio Search took off this did not scale. We needed to build tools to help us automate
                     as much of the provisioning of the pipelines as possible. In order to get there we
                     identified four main problems we needed to&nbsp;solve:How to collect all the required configuration
                     for the pipeline from&nbsp;users.Data Mesh streams are schematized with Avro. In the previous
                     architecture diagram, in 3) there is a stream carrying the results of the GraphQL
                     query to the Elasticsearch sink. The response from GraphQL can contain 10s of fields,
                     often nested. Writing an Avro schema for such a document is time consuming and error
                     prone to do by hand. We needed to make this step much&nbsp;easier.Similarly the generation
                     of the Elasticsearch template was time consuming and error prone. We needed to determine
                     how to generate one based on the users‚Äô configuration.Finally, creating Data Mesh
                     pipelines manually was time consuming and error prone as well due to the volume of
                     configuration required.ConfigurationFor collecting the indexing pipeline configuration
                     from users we defined a single configuration file that enabled users to provide a
                     high level description of their pipeline that we can use to programmatically create
                     the indexing pipeline in Data Mesh. By using this high-level description we were able
                     to greatly simplify the pipeline creation process for users by filling in common yet
                     required configuration for the Data Mesh pipeline.Sample&nbsp;.yaml configurationAvro schema
                     &amp;amp; Elasticsearch index template generationThe approach for both schema and index
                     template generation was very similar. Essentially it required taking the user provided
                     GraphQL query template and generating JSON from it. This was done using graphql-java.
                     The steps required are enumerated below:Introspect the federated graph‚Äôs schema and
                     use the response to build a GraphQLSchema objectParse and validate the user provided
                     GraphQL query template against the&nbsp;schemaVisit the nodes of the query using utilities
                     provided by graphql-java and collect the results into a JSON object‚Ää‚Äî‚Ääthis generated
                     object is the schema/templateDeploymentThe previous steps centralized all the configuration
                     in a single file and provided tools to generate additional configuration for the pipeline‚Äôs
                     dependencies. Now all that was required was an entry point for users to provide their
                     configuration file for orchestrating the provisioning of the indexing pipeline. Given
                     our user base was other engineers we decided to provide a command line interface (CLI)
                     written in Python. Using Python we were able to get the first version of the CLI to
                     our users quickly. Netflix provides tooling that makes the CLI auto-update which makes
                     the CLI easy to iterate on. The CLI performs the following tasks:Validates the provided
                     configuration fileCalls a service to generate the Avro schema &amp;amp; Elasticsearch
                     index&nbsp;templateAssembles the logical plan for the Data Mesh pipeline and creates it
                     using Data Mesh&nbsp;APIsA CLI is just a step towards a better self-service deployment
                     process. We‚Äôre currently exploring options for treating these indices and their pipelines
                     as declarative infrastructure managed within the application that consumes&nbsp;them.Current
                     ChallengesUsing the federated graph to provide the documents for indexing simplifies
                     much of the indexing process but it also creates its own set of challenges. If the
                     challenges below sound exciting to you, come join&nbsp;us!BackfillBootstrapping a new index
                     for the addition or removal of attributes or refreshing an established index both
                     add considerable additional and spiky load to the federated gateway and the component
                     DGSes. Depending on the cardinality of the index and the complexity of its query we
                     may need to coordinate with service owners and/or run backfills off peak. We continue
                     to manage tradeoffs between reindexing speed and&nbsp;load.Reverse LookupsReverse lookups,
                     while convenient, are not particularly user friendly. They introduce a circular dependency
                     in the pipeline‚Ää‚Äî‚Ääyou can‚Äôt create the indexing pipeline without reverse lookups and
                     reverse lookups need the index to function‚Ää‚Äî‚Ääwhich we‚Äôve mitigated although it still
                     creates some confusion. They also require the definer of the index to have detailed
                     knowledge of the eventing for related entities they want to include and that may cover
                     many different domains depending on the index‚Ää‚Äî‚Ääwe have one index covering eight&nbsp;domains.Index
                     consistencyAs an index becomes more complex it is likely to depend on more DGSes and
                     the likelihood of errors increases when fetching the required documents from the federated
                     graph. These errors can lead to documents in the index being out of date or even missing
                     altogether. The owner of the index is often required to follow up with other domain
                     teams regarding errors in related entities and be in the unenviable position of not
                     being able to do much to resolve the issues independently. When the errors are resolved,
                     the process of replaying the failed events is manual and there can be a lag when the
                     service is again successfully returning data but the index does not match&nbsp;it.Stay
                     TunedIn this post, we described how our indexing infrastructure moves data for any
                     given subgraph of the Netflix Content federated graph to Elasticsearch and keeps that
                     data in sync with the source of truth. In an upcoming post, we‚Äôll describe how this
                     data can be queried without actually needing to know anything about Elasticsearch.CreditsThanks
                     to Anoop Panicker, Bo Lei, Charles Zhao, Chris Dhanaraj, Hemamalini Kannan, Jim Isaacs,
                     Johnny Chang, Kasturi Chatterjee, Kishore Banala, Kevin Zhu, Tom Lee, Tongliang Liu,
                     Utkarsh Shrivastava, Vince Bello, Vinod Viswanathan, Yucheng&nbsp;ZengHow Netflix Content
                     Engineering makes a federated graph searchable was originally published in Netflix
                     TechBlog on Medium, where people are continuing the conversation by highlighting and
                     responding to this story.</span></summary><time datetime="2022-04-12T19:16:41+02:00">Tue, 12 Apr 2022 17:16</time><article><p>By <a href="https://www.linkedin.com/in/ahutter/">Alex Hutter</a>, <a href="https://www.linkedin.com/in/falgunijhaveri/">Falguni Jhaveri</a> and <a href="https://www.linkedin.com/in/senthilsayeebaba/">Senthil Sayeebaba</a></p><p>Over the past few years <a href="https://netflixtechblog.com/netflix-studio-engineering-overview-ed60afcfa0ce">Content Engineering</a> at Netflix has been transitioning many of its services to use a federated GraphQL platform. GraphQL federation enables domain teams to independently build and operate their own <a href="https://netflixtechblog.com/open-sourcing-the-netflix-domain-graph-service-framework-graphql-for-spring-boot-92b9dcecda18">Domain Graph Services</a> (DGS) and, at the same time, connect their domain with other domains in a unified GraphQL schema exposed by a <a href="https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2">federated gateway</a>.</p><p>As an example, let‚Äôs examine three core entities of the graph, each owned by separate engineering teams:</p><ol><li><strong>Movie</strong>: At Netflix, we make titles (shows, films, shorts etc.). For simplicity, let‚Äôs assume each title is a <strong>Movie</strong>¬†object.</li><li><strong>Production</strong>: Each Movie is associated with a Studio Production. A <strong>Production</strong> object tracks everything needed to make a <strong>Movie</strong> including shooting location, vendors, and¬†more.</li><li><strong>Talent</strong>: the people working on a <strong>Movie</strong> are the <strong>Talent</strong>, including actors, directors, and so¬†on.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zWqzYt1KvSpkVSSL" /><figcaption>Sample GraphQL¬†Schema</figcaption></figure><p>Once entities like the above are available in the graph, it‚Äôs very common for folks to want to query for a particular entity based on attributes of related entities, e.g. give me all movies that are currently in photography with Ryan Reynolds as an¬†actor.</p><p>In a federated graph architecture, how can we answer such a query given that each entity is served by its own service? The <strong>Movie</strong> service would need to provide an endpoint that accepts a query and filters that may <strong><em>apply to data the service does not own,</em></strong> and use those to identify the appropriate Movie entities to¬†return.</p><p>In fact, <strong><em>every entity owning service could be required to do this¬†work</em>.</strong></p><p>This common problem of making a federated graph searchable led to the creation of <strong>Studio¬†Search</strong>.</p><p>The Studio Search platform was designed to take a portion of the federated graph, a subgraph rooted at an entity of interest, and make it searchable. The entities of the subgraph can be queried with text input, filtered, ranked, and faceted. In the next section, we‚Äôll discuss how we made this possible.</p><h3>Introducing Studio¬†Search</h3><p>When hearing that we want to enable teams to search <strong><em>something</em></strong><em>,</em> your mind likely goes to building an index of some kind. Ours did too! So we need to build an index of a portion of the federated graph.</p><p>How do our users tell us which portion and, even more critically, given that the portion of the graph of interest will almost definitely span data exposed by many services, how do we keep the index current with all these various services?</p><p>We chose <strong>Elasticsearch</strong> as the underlying technology for our index and determined that there were three main pieces of information required to build out an indexing pipeline:</p><ul><li>A definition of their subgraph of interest rooted at the entity they primarily will be searching for</li><li>Events to notify the platform of changes to entities in the¬†subgraph</li><li>Index specific configuration such as whether a field should be used for full text queries or whether a sub-document is¬†nested</li></ul><p>In short, our solution was to build an index for the subgraphs of interest. This index needs to be kept up-to-date with the data exposed by the various services in the federated graph in near-real time.</p><p>GraphQL gives us a straightforward way to define the subgraph‚Ää‚Äî‚Ääa single templated GraphQL query that pulls all of the data the user is interested in using in their searches.</p><p>Here‚Äôs an example GraphQL query template. It‚Äôs pulling data for Movies and their related Productions and¬†Talent.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/706/1*9FhJnH2MiYQvFHysmSQpuA.png" /><figcaption><em>Sample GraphQL¬†query</em></figcaption></figure><p>To keep the index up to date, events are used to trigger a reindexing operation for individual entities when they change. <strong><em>Change Data Capture</em></strong> (CDC) events are the preferred events for triggering these operations‚Ää‚Äî‚Äämost teams produce them using Netflix‚Äôs <a href="https://netflixtechblog.com/dblog-a-generic-change-data-capture-framework-69351fb9099b">CDC connectors</a>‚Ää‚Äî‚Äähowever, <strong><em>application events</em></strong> are also supported when necessary.</p><p>All data to be indexed is being fetched from the federated graph so all that is needed in the events is an entity id; the id can be substituted into the GraphQL query template to fetch the entity and any related¬†data.</p><p>Using the type information present in the GraphQL query template and the user specified index configuration we were able to create an index template with a set of custom Elasticsearch text analyzers that generalized well across¬†domains.</p><p>Given these inputs, a <a href="https://netflixtechblog.com/data-movement-in-netflix-studio-via-data-mesh-3fddcceb1059">Data Mesh</a> pipeline can be created that consists of the user provided CDC event source, a processor to enrich those events using the user provided GraphQL query and a sink to Elasticsearch.</p><h3>Architecture</h3><p>Putting this all together, below you can see a simplified view of the architecture.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*LfYcAWOrhjbp9mA7" /><figcaption><em>Studio Search Indexing Architecture</em></figcaption></figure><ol><li>Studio applications produce events to schematized <strong>Kafka</strong> streams within Data¬†Mesh.</li></ol><p>a. By transacting with a database which is monitored by a CDC connector that creates events,¬†or</p><p>b. By directly creating events using a Data Mesh¬†client.</p><p>2. The schematized events are consumed by Data Mesh processors implemented in the <strong>Apache<em> </em>Flink</strong> framework. Some entities have multiple events for their changes so we leverage union processors to combine data from multiple Kafka¬†streams.</p><p>a. A GraphQL processor executes the user provided GraphQL query to fetch documents from the federated gateway.</p><p>b. The federated gateway, in turn, fetches data from the Studio applications.</p><p>3. The documents fetched from the federated gateway are put onto another schematized Kafka topic before being processed by an <strong>Elasticsearch</strong> sink in Data Mesh that indexes them into Elasticsearch index configured with an indexing template created specifically for the fields and types present in the document.</p><h3>Reverse lookups</h3><p>You may have noticed something missing in the above explanation. If the index is being populated based on <strong>Movie</strong> id events, how does it stay up to date when a <strong>Production</strong> or <strong>Talent</strong> changes? Our solution to this is a reverse lookup‚Ää‚Äî‚Ääwhen a change to a related entity is made, we need to look up all of the primary entities that could be affected and trigger events for those. We do this by consulting the index itself and querying for all primary entities related to the entity that has¬†changed.</p><p>For instance if our index has a document that looks like¬†this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qBiCX79OcsHNvXSh" /><figcaption><em>Sample Elasticsearch document</em></figcaption></figure><p>And the pipeline observes a change to the <strong>Production</strong> with ptpId ‚Äúabc‚Äù, we can query the index for all documents with production.ptpId == ‚Äúabc‚Äù and extract the movieId. Then, we can pass that movieId down into the rest of the indexing pipeline.</p><h3>Scaling the¬†Solution</h3><p>The solution we came up with worked quite well. Teams were easily able to share the requirements for their subgraph‚Äôs index via a GraphQL query template and could use existing tooling to generate the events to enable the index to be kept up to date in near real-time. Reusing the index itself to power reverse lookups enabled us to keep all the logic for handling related entities contained within our systems and shield our users from that complexity. In fact it worked so well that we became inundated with requests to integrate with Studio Search‚Ää‚Äî‚Ääit began to power a significant portion of the user experience for many applications within Content Engineering.</p><p>Early on, we did integrations by hand but as adoption of Studio Search took off this did not scale. We needed to build tools to help us <strong><em>automate</em></strong> as much of the <strong><em>provisioning</em></strong> of the pipelines as possible. In order to get there we identified four main problems we needed to¬†solve:</p><ul><li>How to collect all the required configuration for the pipeline from¬†users.</li><li>Data Mesh streams are schematized with Avro. In the previous architecture diagram, in <strong>3)</strong> there is a stream carrying the results of the GraphQL query to the Elasticsearch sink. The response from GraphQL can contain 10s of fields, often nested. Writing an Avro schema for such a document is time consuming and error prone to do by hand. We needed to make this step much¬†easier.</li><li>Similarly the generation of the Elasticsearch template was time consuming and error prone. We needed to determine how to generate one based on the users‚Äô configuration.</li><li>Finally, creating Data Mesh pipelines manually was time consuming and error prone as well due to the volume of configuration required.</li></ul><h3>Configuration</h3><p>For collecting the indexing pipeline configuration from users we defined a single configuration file that enabled users to provide a high level description of their pipeline that we can use to programmatically create the indexing pipeline in Data Mesh. By using this high-level description we were able to greatly simplify the pipeline creation process for users by filling in common yet required configuration for the Data Mesh pipeline.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/487/1*jw8US5Z96Sspa9vffxjphA.png" /><figcaption><em>Sample¬†.yaml configuration</em></figcaption></figure><h3>Avro schema &amp; Elasticsearch index template generation</h3><p>The approach for both schema and index template generation was very similar. Essentially it required taking the user provided GraphQL query template and generating JSON from it. This was done using <a href="https://www.graphql-java.com/">graphql-java</a>. The steps required are enumerated below:</p><ul><li>Introspect the federated graph‚Äôs schema and use the response to build a GraphQLSchema object</li><li>Parse and validate the user provided GraphQL query template against the¬†schema</li><li>Visit the nodes of the query using utilities provided by graphql-java and collect the results into a JSON object‚Ää‚Äî‚Ääthis generated object is the schema/template</li></ul><h3>Deployment</h3><p>The previous steps centralized all the configuration in a single file and provided tools to generate additional configuration for the pipeline‚Äôs dependencies. Now all that was required was an entry point for users to provide their configuration file for orchestrating the provisioning of the indexing pipeline. Given our user base was other engineers we decided to provide a command line interface (CLI) written in Python. Using Python we were able to get the first version of the CLI to our users quickly. Netflix provides tooling that makes the CLI auto-update which makes the CLI easy to iterate on. The CLI performs the following tasks:</p><ul><li>Validates the provided configuration file</li><li>Calls a service to generate the Avro schema &amp; Elasticsearch index¬†template</li><li>Assembles the logical plan for the Data Mesh pipeline and creates it using Data Mesh¬†APIs</li></ul><p>A CLI is just a step towards a better self-service deployment process. We‚Äôre currently exploring options for treating these indices and their pipelines as <strong><em>declarative infrastructure </em></strong><em>managed</em> within the application that consumes¬†them.</p><h3>Current Challenges</h3><p>Using the federated graph to provide the documents for indexing simplifies much of the indexing process but it also creates its own set of challenges. If the challenges below sound exciting to you, come <a href="https://jobs.netflix.com/">join¬†us</a>!</p><h3>Backfill</h3><p>Bootstrapping a new index for the addition or removal of attributes or refreshing an established index both add considerable additional and spiky load to the federated gateway and the component DGSes. Depending on the cardinality of the index and the complexity of its query we may need to coordinate with service owners and/or run backfills off peak. We continue to manage tradeoffs between reindexing speed and¬†load.</p><h3>Reverse Lookups</h3><p>Reverse lookups, while convenient, are not particularly user friendly. They introduce a circular dependency in the pipeline‚Ää‚Äî‚Ääyou can‚Äôt create the indexing pipeline without reverse lookups and reverse lookups need the index to function‚Ää‚Äî‚Ääwhich we‚Äôve mitigated although it still creates some confusion. They also require the definer of the index to have detailed knowledge of the eventing for related entities they want to include and that may cover many different domains depending on the index‚Ää‚Äî‚Ääwe have one index covering eight¬†domains.</p><h3>Index consistency</h3><p>As an index becomes more complex it is likely to depend on more DGSes and the likelihood of errors increases when fetching the required documents from the federated graph. These errors can lead to documents in the index being <strong><em>out of date</em></strong> or even <strong><em>missing</em></strong> altogether. The owner of the index is often required to follow up with other domain teams regarding errors in related entities and be in the unenviable position of not being able to do much to resolve the issues independently. When the errors are resolved, the process of replaying the failed events is manual and there can be a lag when the service is again successfully returning data but the index does not match¬†it.</p><h3>Stay Tuned</h3><p>In this post, we described how our indexing infrastructure moves data for any given subgraph of the Netflix Content federated graph to Elasticsearch and keeps that data in sync with the source of truth. In an upcoming post, we‚Äôll describe how this data can be queried without actually needing to know anything about Elasticsearch.</p><h3>Credits</h3><p>Thanks to <a href="https://www.linkedin.com/in/anoop-panicker/">Anoop Panicker</a>, <a href="https://www.linkedin.com/in/bolei1007/">Bo Lei</a>, <a href="https://www.linkedin.com/in/czhao/">Charles Zhao</a>, <a href="https://www.linkedin.com/in/chrisdhanaraj/">Chris Dhanaraj</a>, <a href="https://www.linkedin.com/in/hemamalinikannan/">Hemamalini Kannan</a>,<a href="https://www.linkedin.com/in/jimpisaacs/"> Jim Isaacs</a>, <a href="https://www.linkedin.com/in/johnnycc321/">Johnny Chang</a>, <a href="https://www.linkedin.com/in/kasturi-chatterjee-a900715/">Kasturi Chatterjee</a>, <a href="https://www.linkedin.com/in/kishore-banala/">Kishore Banala</a>, <a href="https://www.linkedin.com/in/kevinzhu/">Kevin Zhu</a>, <a href="https://www.linkedin.com/in/thomaslee4/">Tom Lee</a>, <a href="https://www.linkedin.com/in/tonylxc/">Tongliang Liu</a>, <a href="https://www.linkedin.com/in/utkarshshrivastava/">Utkarsh Shrivastava</a>, <a href="https://www.linkedin.com/in/vincentbello/">Vince Bello</a>, <a href="https://www.linkedin.com/in/vinodvish/">Vinod Viswanathan</a>, <a href="https://www.linkedin.com/in/yuchengzeng/">Yucheng¬†Zeng</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5c0c1c7d7eaf" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf">How Netflix Content Engineering makes a federated graph searchable</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://netflixtechblog.com/rapid-event-notification-system-at-netflix-6deb1d2b57d1?source=rss----2615bd06b42e---4">Rapid Event Notification System at Netflix</a> <span>By: Ankush Gulati, David GevorkyanAdditional credits: Michael Clark, Gokhan&nbsp;OzerIntroNetflix
                     has more than 220 million active members who perform a variety of actions throughout
                     each session, ranging from renaming a profile to watching a title. Reacting to these
                     actions in near real-time to keep the experience consistent across devices is critical
                     for ensuring an optimal member experience. This is not an easy task, considering the
                     wide variety of supported devices and the sheer volume of actions our members perform.
                     To this end, we developed a Rapid Event Notification System (RENO) to support use
                     cases that require server initiated communication with devices in a scalable and extensible
                     manner.In this blog post, we will give an overview of the Rapid Event Notification
                     System at Netflix and share some of the learnings we gained along the&nbsp;way.MotivationWith
                     the rapid growth in Netflix member base and the increasing complexity of our systems,
                     our architecture has evolved into an asynchronous one that enables both online and
                     offline computation. Providing a seamless and consistent Netflix experience across
                     various platforms (iOS, Android, smart TVs, Roku, Amazon FireStick, web browser) and
                     various device types (mobile phones, tablets, televisions, computers, set top boxes)
                     requires more than the traditional request-response model. Over time, we‚Äôve seen an
                     increase in use cases where backend systems need to initiate communication with devices
                     to notify them of member-driven changes or experience updates quickly and consistently.Use
                     casesViewing ActivityWhen a member begins to watch a show, their ‚ÄúContinue Watching‚Äù
                     list should be updated across all of their devices to reflect that&nbsp;viewing.Personalized
                     Experience RefreshNetflix Recommendation engine continuously refreshes recommendations
                     for every member. The updated recommendations need to be delivered to the device timely
                     for an optimal member experience.Membership Plan ChangesMembers often change their
                     plan types, leading to a change in their experience that must be immediately reflected
                     across all of their&nbsp;devices.Member ‚ÄúMy List‚Äù UpdatesWhen members update their ‚ÄúMy
                     List‚Äù by adding or removing titles, the changes should be reflected across all of
                     their&nbsp;devices.Member Profile ChangesWhen members update their account settings like
                     add/delete/rename profiles or change their preferred maturity level for content, these
                     updates must be reflected across all of their&nbsp;devices.System Diagnostic SignalsIn
                     special scenarios, we need to send diagnostic signals to the Netflix app on devices
                     to help troubleshoot problems and enable tracing capabilities.Design DecisionsIn designing
                     the system, we made a few key decisions that helped shape the architecture of&nbsp;RENO:Single
                     Events&nbsp;SourceEvent PrioritizationHybrid Communication ModelTargeted DeliveryManaging
                     High&nbsp;RPSSingle Events&nbsp;SourceThe use cases we wanted to support originate from various
                     internal systems and member actions, so we needed to listen for events from several
                     different microservices. At Netflix, our near-real-time event flow is managed by an
                     internal distributed computation framework called Manhattan (you can learn more about
                     it here). We leveraged Manhattan‚Äôs event management framework to create a level of
                     indirection serving as the single source of events for&nbsp;RENO.Event PrioritizationConsidering
                     the use cases were wide ranging both in terms of their sources and their importance,
                     we built segmentation into the event processing. For example, a member-triggered event
                     such as ‚Äúchange in a profile‚Äôs maturity level‚Äù should have a much higher priority
                     than a ‚Äúsystem diagnostic signal‚Äù. We thus assigned a priority to each use case and
                     sharded event traffic by routing to priority-specific queues and the corresponding
                     event processing clusters. This separation allows us to tune system configuration
                     and scaling policies independently for different event priorities and traffic patterns.Hybrid
                     Communication ModelAs mentioned earlier in this post, one key challenge for a service
                     like RENO is supporting multiple platforms. While a mobile device is almost always
                     connected to the internet and reachable, a smart TV is only online while in use. This
                     network connection heterogeneity made choosing a single delivery model difficult.
                     For example, entirely relying on a Pull model wherein the device frequently calls
                     home for updates would result in chatty mobile apps. That in turn will be triggering
                     the per-app communication limits that iOS and Android platforms enforce (we also need
                     to be considerate of low bandwidth connections). On the other hand, using only a Push
                     mechanism would lead smart TVs to miss notifications while they are powered off during
                     most of the day. We therefore chose a hybrid Push AND Pull communication model wherein
                     the server tries to deliver notifications to all devices immediately using Push notifications,
                     and devices call home at various stages of the application lifecycle.Using a Push-and-Pull
                     delivery model combination also supports devices limited to a single communication
                     model. This includes older, legacy devices that do not support Push Notifications.Targeted
                     DeliveryConsidering the use cases were wide ranging in terms of both sources and target
                     device types, we built support for device specific notification delivery. This capability
                     allows notifying specific device categories as per the use case. When an actionable
                     event arrives, RENO applies the use case specific business logic, gathers the list
                     of devices eligible to receive this notification and attempts delivery. This helps
                     limit the outgoing traffic footprint considerably.Managing High&nbsp;RPSWith over 220 million
                     members, we were conscious of the fact that a service like RENO needs to process many
                     events per member during a viewing session. At peak times, RENO serves about 150k
                     events per second. Such a high RPS during specific times of the day can create a thundering
                     herd problem and put strain on internal and external downstream services. We therefore
                     implemented a few optimizations:Event AgeMany events that need to be notified to the
                     devices are time sensitive, and they are of no or little value unless sent almost
                     immediately. To avoid processing old events, a staleness filter is applied as a gating
                     check. If an event age is older than a configurable threshold, it is not processed.
                     This filter weeds out events that have no value to the devices early in the processing
                     phase and protects the queues from being flooded due to stale upstream events that
                     may have been backed&nbsp;up.Online DevicesTo reduce the ongoing traffic footprint, notifications
                     are sent only to devices that are currently online by leveraging an existing registry
                     that is kept up-to-date by Zuul (learn more about it&nbsp;here).Scaling PoliciesTo address
                     the thundering herd problem and to keep latencies under acceptable thresholds, the
                     cluster scale-up policies are configured to be more aggressive than the scale-down
                     policies. This approach enables the computing power to catch up quickly when the queues&nbsp;grow.Event
                     DeduplicationBoth iOS and Android platforms aggressively restrict the level of activity
                     generated by backgrounded apps, hence the reason why incoming events are deduplicated
                     in RENO. Duplicate events can occur in case of high RPS, and they are merged together
                     when it does not cause any loss of context for the&nbsp;device.Bulkheaded DeliveryMultiple
                     downstream services are used to send push notifications to different device platforms
                     including external ones like Apple Push Notification Service (APNS) for Apple devices
                     and Google‚Äôs Firebase Cloud Messaging (FCM) for Android. To safeguard against a downstream
                     service bringing down the entire notification service, the event delivery is parallelized
                     across different platforms, making it best-effort per platform. If a downstream service
                     or platform fails to deliver the notification, the other devices are not blocked from
                     receiving push notifications.ArchitectureAs shown in the diagram above, the RENO service
                     can be broken down into the following components.Event TriggersMember actions and
                     system-driven updates that require refreshing the experience on members‚Äô&nbsp;devices.Event
                     Management EngineThe near-real-time event flow management framework at Netflix referred
                     to as Manhattan can be configured to listen to specific events and forward events
                     to different queues.Event Priority Based&nbsp;QueuesAmazon SQS queues that are populated
                     by priority-based event forwarding rules are set up in Manhattan to allow priority
                     based sharding of&nbsp;traffic.Event Priority Based&nbsp;ClustersAWS Instance Clusters that
                     subscribe to the corresponding queues with the same priority. They process all the
                     events arriving on those queues and generate actionable notifications for&nbsp;devices.Outbound
                     Messaging SystemThe Netflix messaging system that sends in-app push notifications
                     to members is used to send RENO-produced notifications on the last mile to mobile
                     devices. This messaging system is described in this blog&nbsp;post.For notifications to
                     web, TV &amp;amp; other streaming devices, we use a homegrown push notification solution
                     ‚Äã‚Äãcalled Zuul Push that provides ‚Äúalways-on‚Äù persistent connections with online devices.
                     To learn more about the Zuul Push solution, listen to this talk from a Netflix colleague.Persistent
                     StoreA Cassandra database that stores all the notifications emitted by RENO for each
                     device to allow those devices to poll for their messages at their own&nbsp;cadence.ObservabilityAt
                     Netflix, we put a strong emphasis on building robust monitoring into our systems to
                     provide a clear view of system health. For a high RPS service like RENO that relies
                     on several upstream systems as its traffic source and simultaneously produces heavy
                     traffic for different internal and external downstream systems, it is important to
                     have a strong combination of metrics, alerting and logging in place. For alerting,
                     in addition to the standard system health metrics such as CPU, memory, and performance,
                     we added a number of ‚Äúedge-of-the-service‚Äù metrics and logging to capture any aberrations
                     from upstream or downstream systems. Furthermore, in addition to real-time alerting,
                     we added trend analysis for important metrics to help catch longer term degradations.
                     We instrumented RENO with a real time stream processing application called Mantis
                     (you can learn more about it here). It allowed us to track events in real-time over
                     the wire at device specific granularity thus making debugging easier. Finally, we
                     found it useful to have platform-specific alerting (for iOS, Android, etc.) in finding
                     the root causes of issues&nbsp;faster.WinsCan easily support new use&nbsp;casesScales horizontally
                     with higher throughputWhen we set out to build RENO the goal was limited to the ‚ÄúPersonalized
                     Experience Refresh‚Äù use case of the product. As the design of RENO evolved, support
                     for new use cases became possible and RENO was quickly positioned as the centralized
                     rapid notification service for all product areas at&nbsp;Netflix.The design decisions we
                     made early on paid off, such as making addition of new use cases a ‚Äúplug-and-play‚Äù
                     solution and providing a hybrid delivery model across all platforms. We were able
                     to onboard additional product use cases at a fast pace thus unblocking a lot of innovation.An
                     important learning in building this platform was ensuring that RENO could scale horizontally
                     as more types of events and higher throughput was needed over time. This ability was
                     primarily achieved by allowing sharding based on either event type or priority, along
                     with using an asynchronous event driven processing model that can be scaled by simply
                     adding more machines for event processing.Looking AheadAs Netflix‚Äôs member base continues
                     to grow at a rapid pace, it is increasingly beneficial to have a service like RENO
                     that helps give our members the best and most up to date Netflix experience. From
                     membership related updates to contextual personalization, and more‚Ää‚Äî‚Ääwe are continually
                     evolving our notifications portfolio as we continue to innovate on our member experience.
                     Architecturally, we are evaluating opportunities to build in more features such as
                     guaranteed message delivery and message batching that can open up more use cases and
                     help reduce the communication footprint of&nbsp;RENO.Building Great Things&nbsp;TogetherWe are
                     just getting started on this journey to build impactful systems that help propel our
                     business forward. The core to bringing these engineering solutions to life is our
                     direct collaboration with our colleagues and using the most impactful tools and technologies
                     available. If this is something that excites you, we‚Äôd love for you to join&nbsp;us.Rapid
                     Event Notification System at Netflix was originally published in Netflix TechBlog
                     on Medium, where people are continuing the conversation by highlighting and responding
                     to this story.</span></summary><time datetime="2022-02-18T17:30:24+02:00">Fri, 18 Feb 2022 15:30</time><article><p>By: <a href="https://www.linkedin.com/in/gulatiankush/">Ankush Gulati</a>, <a href="https://www.linkedin.com/in/davidgevorkyan/">David Gevorkyan</a><br>Additional credits: <a href="https://www.linkedin.com/in/michael-clark-621880/">Michael Clark</a>, <a href="https://www.linkedin.com/in/gokhan-ozer-94407822/">Gokhan¬†Ozer</a></p><h3>Intro</h3><p>Netflix has more than 220 million active members who perform a variety of actions throughout each session, ranging from renaming a profile to watching a title. Reacting to these actions in near real-time to keep the experience consistent across devices is critical for ensuring an optimal member experience. This is not an easy task, considering the wide variety of supported devices and the sheer volume of actions our members perform. To this end, we developed a <strong>Rapid Event Notification System</strong> (RENO) to support use cases that require server initiated communication with devices in a scalable and extensible manner.</p><p>In this blog post, we will give an overview of the Rapid Event Notification System at Netflix and share some of the learnings we gained along the¬†way.</p><h3>Motivation</h3><p>With the rapid growth in Netflix member base and the increasing complexity of our systems, our architecture has evolved into an asynchronous one that enables both online and offline computation. Providing a seamless and consistent Netflix experience across various platforms (iOS, Android, smart TVs, Roku, Amazon FireStick, web browser) and various device types (mobile phones, tablets, televisions, computers, set top boxes) requires more than the traditional request-response model. Over time, we‚Äôve seen an increase in use cases where backend systems need to initiate communication with devices to notify them of member-driven changes or experience updates quickly and consistently.</p><h3>Use cases</h3><ul><li><strong>Viewing Activity<br></strong>When a member begins to watch a show, their ‚ÄúContinue Watching‚Äù list should be updated across all of their devices to reflect that¬†viewing.</li><li><strong>Personalized Experience Refresh<br></strong>Netflix Recommendation engine continuously refreshes recommendations for every member. The updated recommendations need to be delivered to the device timely for an optimal member experience.</li><li><strong>Membership Plan Changes<br></strong>Members often change their plan types, leading to a change in their experience that must be immediately reflected across all of their¬†devices.</li><li><strong>Member ‚ÄúMy List‚Äù Updates</strong><br>When members update their ‚ÄúMy List‚Äù by adding or removing titles, the changes should be reflected across all of their¬†devices.</li><li><strong>Member Profile Changes<br></strong>When members update their account settings like add/delete/rename profiles or change their preferred maturity level for content, these updates must be reflected across all of their¬†devices.</li><li><strong>System Diagnostic Signals<br></strong>In special scenarios, we need to send diagnostic signals to the Netflix app on devices to help troubleshoot problems and enable tracing capabilities.</li></ul><h3>Design Decisions</h3><p>In designing the system, we made a few key decisions that helped shape the architecture of¬†RENO:</p><ol><li>Single Events¬†Source</li><li>Event Prioritization</li><li>Hybrid Communication Model</li><li>Targeted Delivery</li><li>Managing High¬†RPS</li></ol><h4>Single Events¬†Source</h4><p>The use cases we wanted to support originate from various internal systems and member actions, so we needed to listen for events from several different microservices. At Netflix, our near-real-time event flow is managed by an internal distributed computation framework called Manhattan (you can learn more about it <a href="https://netflixtechblog.com/system-architectures-for-personalization-and-recommendation-e081aa94b5d8">here</a>). We leveraged Manhattan‚Äôs event management framework to create a level of indirection serving as the single source of events for¬†RENO.</p><h4>Event Prioritization</h4><p>Considering the use cases were wide ranging both in terms of their sources and their importance, we built segmentation into the event processing. For example, a member-triggered event such as ‚Äú<em>change in a profile‚Äôs maturity level‚Äù</em> should have a much higher priority than a ‚Äú<em>system diagnostic signal‚Äù.</em> We thus assigned a priority to each use case and sharded event traffic by routing to priority-specific queues and the corresponding event processing clusters. This separation allows us to tune system configuration and scaling policies independently for different event priorities and traffic patterns.</p><h4>Hybrid Communication Model</h4><p>As mentioned earlier in this post, one key challenge for a service like RENO is supporting multiple platforms. While a mobile device is almost always connected to the internet and reachable, a smart TV is only online while in use. This network connection heterogeneity made choosing a single delivery model difficult. For example, entirely relying on a Pull model wherein the device frequently calls home for updates would result in chatty mobile apps. That in turn will be triggering the per-app communication limits that iOS and Android platforms enforce (we also need to be considerate of low bandwidth connections). On the other hand, using only a Push mechanism would lead smart TVs to miss notifications while they are powered off during most of the day. We therefore chose a hybrid Push AND Pull communication model wherein the server tries to deliver notifications to all devices immediately using Push notifications, and devices call home at various stages of the application lifecycle.</p><p>Using a Push-and-Pull delivery model combination also supports devices limited to a single communication model. This includes older, legacy devices that do not support Push Notifications.</p><h4>Targeted Delivery</h4><p>Considering the use cases were wide ranging in terms of both sources and target device types, we built support for device specific notification delivery. This capability allows notifying specific device categories as per the use case. When an actionable event arrives, RENO applies the use case specific business logic, gathers the list of devices eligible to receive this notification and attempts delivery. This helps limit the outgoing traffic footprint considerably.</p><h4>Managing High¬†RPS</h4><p>With over 220 million members, we were conscious of the fact that a service like RENO needs to process many events per member during a viewing session. At peak times, RENO serves about 150k events per second. Such a high RPS during specific times of the day can create a <a href="https://en.wikipedia.org/wiki/Thundering_herd_problem">thundering herd problem</a> and put strain on internal and external downstream services. We therefore implemented a few optimizations:</p><ul><li><strong>Event Age<br></strong>Many events that need to be notified to the devices are time sensitive, and they are of no or little value unless sent almost immediately. To avoid processing old events, a staleness filter is applied as a gating check. If an event age is older than a configurable threshold, it is not processed. This filter weeds out events that have no value to the devices early in the processing phase and protects the queues from being flooded due to stale upstream events that may have been backed¬†up.</li><li><strong>Online Devices<br></strong>To reduce the ongoing traffic footprint, notifications are sent only to devices that are currently online by leveraging an existing registry that is kept up-to-date by Zuul (learn more about it¬†<a href="https://netflixtechblog.com/tagged/zuul">here</a>).</li><li><strong>Scaling Policies<br></strong>To address the thundering herd problem and to keep latencies under acceptable thresholds, the cluster scale-up policies are configured to be more aggressive than the scale-down policies. This approach enables the computing power to catch up quickly when the queues¬†grow.</li><li><strong>Event Deduplication<br></strong>Both iOS and Android platforms aggressively restrict the level of activity generated by backgrounded apps, hence the reason why incoming events are deduplicated in RENO. Duplicate events can occur in case of high RPS, and they are merged together when it does not cause any loss of context for the¬†device.</li><li><strong>Bulkheaded Delivery<br></strong>Multiple downstream services are used to send push notifications to different device platforms including external ones like Apple Push Notification Service (<a href="https://developer.apple.com/go/?id=push-notifications">APNS</a>) for Apple devices and Google‚Äôs Firebase Cloud Messaging (<a href="https://firebase.google.com/docs/cloud-messaging">FCM</a>) for Android. To safeguard against a downstream service bringing down the entire notification service, the event delivery is parallelized across different platforms, making it best-effort per platform. If a downstream service or platform fails to deliver the notification, the other devices are not blocked from receiving push notifications.</li></ul><h3>Architecture</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*i6o12AqlvqZDMcAj" /></figure><p>As shown in the diagram above, the RENO service can be broken down into the following components.</p><h4>Event Triggers</h4><p>Member actions and system-driven updates that require refreshing the experience on members‚Äô¬†devices.</p><h4>Event Management Engine</h4><p>The near-real-time event flow management framework at Netflix referred to as Manhattan can be configured to listen to specific events and forward events to different queues.</p><h4>Event Priority Based¬†Queues</h4><p>Amazon SQS queues that are populated by priority-based event forwarding rules are set up in Manhattan to allow priority based sharding of¬†traffic.</p><h4>Event Priority Based¬†Clusters</h4><p>AWS Instance Clusters that subscribe to the corresponding queues with the same priority. They process all the events arriving on those queues and generate actionable notifications for¬†devices.</p><h4>Outbound Messaging System</h4><p>The Netflix messaging system that sends in-app push notifications to members is used to send RENO-produced notifications on the last mile to mobile devices. This messaging system is described in <a href="https://netflixtechblog.com/building-a-cross-platform-in-app-messaging-orchestration-service-86ba614f92d8">this blog¬†post</a>.</p><p>For notifications to web, TV &amp; other streaming devices, we use a homegrown push notification solution ‚Äã‚Äãcalled Zuul Push that provides ‚Äúalways-on‚Äù persistent connections with online devices. To learn more about the Zuul Push solution, listen to <a href="https://qconnewyork.com/ny2018/presentation/architectures-youve-always-wondered-about-presentation">this talk</a> from a Netflix colleague.</p><h4>Persistent Store</h4><p>A Cassandra database that stores all the notifications emitted by RENO for each device to allow those devices to poll for their messages at their own¬†cadence.</p><h3>Observability</h3><p>At Netflix, we put a strong emphasis on building robust monitoring into our systems to provide a clear view of system health. For a high RPS service like RENO that relies on several upstream systems as its traffic source and simultaneously produces heavy traffic for different internal and external downstream systems, it is important to have a strong combination of metrics, alerting and logging in place. For alerting, in addition to the standard system health metrics such as CPU, memory, and performance, we added a number of ‚Äúedge-of-the-service‚Äù metrics and logging to capture any aberrations from upstream or downstream systems. Furthermore, in addition to real-time alerting, we added trend analysis for important metrics to help catch longer term degradations. We instrumented RENO with a real time stream processing application called Mantis (you can learn more about it <a href="https://netflixtechblog.com/open-sourcing-mantis-a-platform-for-building-cost-effective-realtime-operations-focused-5b8ff387813a">here</a>). It allowed us to track events in real-time over the wire at device specific granularity thus making debugging easier. Finally, we found it useful to have platform-specific alerting (for iOS, Android, etc.) in finding the root causes of issues¬†faster.</p><h3>Wins</h3><ul><li>Can easily support new use¬†cases</li><li>Scales horizontally with higher throughput</li></ul><p>When we set out to build RENO the goal was limited to the ‚ÄúPersonalized Experience Refresh‚Äù use case of the product. As the design of RENO evolved, support for new use cases became possible and RENO was quickly positioned as the centralized rapid notification service for all product areas at¬†Netflix.</p><p>The design decisions we made early on paid off, such as making addition of new use cases a ‚Äúplug-and-play‚Äù solution and providing a hybrid delivery model across all platforms. We were able to onboard additional product use cases at a fast pace thus unblocking a lot of innovation.</p><p>An important learning in building this platform was ensuring that RENO could scale horizontally as more types of events and higher throughput was needed over time. This ability was primarily achieved by allowing sharding based on either event type or priority, along with using an asynchronous event driven processing model that can be scaled by simply adding more machines for event processing.</p><h3>Looking Ahead</h3><p>As Netflix‚Äôs member base continues to grow at a rapid pace, it is increasingly beneficial to have a service like RENO that helps give our members the best and most up to date Netflix experience. From membership related updates to contextual personalization, and more‚Ää‚Äî‚Ääwe are continually evolving our notifications portfolio as we continue to innovate on our member experience. Architecturally, we are evaluating opportunities to build in more features such as guaranteed message delivery and message batching that can open up more use cases and help reduce the communication footprint of¬†RENO.</p><h3>Building Great Things¬†Together</h3><p>We are just getting started on this journey to build impactful systems that help propel our business forward. The core to bringing these engineering solutions to life is our direct collaboration with our colleagues and using the most impactful tools and technologies available. If this is something that excites you, we‚Äôd love for you to <a href="https://jobs.netflix.com/">join¬†us</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6deb1d2b57d1" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/rapid-event-notification-system-at-netflix-6deb1d2b57d1">Rapid Event Notification System at Netflix</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://netflixtechblog.com/data-pipeline-asset-management-with-dataflow-86525b3e21ca?source=rss----2615bd06b42e---4">Data pipeline asset management with Dataflow</a> <span>by Sam Setegne, Jai Balani, Olek&nbsp;GorajekGlossaryasset‚Ää‚Äî‚Ääany business logic code in
                     a raw (e.g. SQL) or compiled (e.g. JAR) form to be executed as part of the user defined
                     data pipeline.data pipeline‚Ää‚Äî‚Ääa set of tasks (or jobs) to be executed in a predefined
                     order (a.k.a. DAG) for the purpose of transforming data using some business&nbsp;logic.Dataflow‚Ää‚Äî‚ÄäNetflix
                     homegrown CLI tool for data pipeline management.job‚Ää‚Äî‚Ääa.k.a task, an atomic unit of
                     data transformation logic, a non-separable execution block in the workflow&nbsp;chain.namespace‚Ää‚Äî‚Ääunique
                     label, usually representing a business subject area, assigned to a workflow asset
                     to identify it across all other assets managed by Dataflow (e.g. security).workflow‚Ää‚Äî‚Ääsee
                     ‚Äúdata pipeline‚ÄùIntroThe problem of managing scheduled workflows and their assets is
                     as old as the use of cron daemon in early Unix operating systems. The design of a
                     cron job is simple, you take some system command, you pick the schedule to run it
                     on and you are done.&nbsp;Example:0 0 * * MON /home/alice/backup.shIn the above example
                     the system would wake up every Monday morning and execute the backup.sh script. Simple
                     right? But what if the script does not exist in the given path, or what if it existed
                     initially but then Alice let Bob access her home directory and he accidentally deleted
                     it? Or what if Alice wanted to add new backup functionality and she accidentally broke
                     existing code while updating&nbsp;it?The answers to these questions is something we would
                     like to address in this article and propose a clean solution to this&nbsp;problem.Let‚Äôs
                     define some requirements that we are interested in delivering to the Netflix data
                     engineers or anyone who would like to schedule a workflow with some external assets
                     in it. By external assets we simply mean some executable carrying the actual business
                     logic of the job. It could be a JAR compiled from Scala, a Python script or module,
                     or a simple SQL file. The important thing is that this business logic can be built
                     in a separate repository and maintained independently from the workflow definition.
                     Keeping all that in mind we would like to achieve the following properties for the
                     whole workflow deployment:Versioning: we want both the workflow definition and its
                     assets to be versioned and we want the versions to be tied together in a clear&nbsp;way.Transparency:
                     we want to know which version of an asset is running along with every workflow instance,
                     so if there are any issues we can easily identify which version caused the problem
                     and to which one we could revert, if necessary.ACID deployment: for every scheduler
                     workflow definition change, we would like to have all the workflow assets bundled
                     in an atomic, durable, isolated and consistent manner. This way, if necessary, all
                     we need to know is which version of the workflow to roll back to, and the rest would
                     be taken care of for&nbsp;us.While all the above goals are our North Star, we also don‚Äôt
                     want to negatively affect fast deployment, high availability and arbitrary life span
                     of any deployed&nbsp;asset.Previous solutionsThe basic approach to pulling down arbitrary
                     workflow resources during workflow execution has been known to mankind since the invention
                     of cron, and with the advent of ‚Äúinfinite‚Äù cloud storage systems like S3, this approach
                     has served us for many years. Its apparent flexibility and convenience can often fool
                     us into thinking that by simply replacing the asset in the S3 location we can, without
                     any hassle, introduce changes to our business logic. This method often proves very
                     troublesome especially if there is more than one engineer working on the same pipeline
                     and they are not all aware of the other folks‚Äô ‚Äúdeployment process‚Äù.The slightly improved
                     approach is shown on the diagram&nbsp;below.Figure 1. Manually constructed continuous delivery&nbsp;system.In
                     Figure 1, you can see an illustration of a typical deployment pipeline manually constructed
                     by a user for an individual project. The continuous deployment tool submits a workflow
                     definition with pointers to assets in fixed S3 locations. These assets are then separately
                     deployed to these fixed locations. At runtime, the assets are retrieved from the defined
                     locations in S3 and executed in the runtime container. Despite requiring users to
                     construct the deployment pipeline manually, often by writing their own scripts from
                     scratch, this design works and has been successfully used by many teams for years.
                     That being said, it does have some drawbacks that are revealed as you try to add any
                     amount of complexity to your deployment logic. Let‚Äôs discuss a few of&nbsp;them.Does not
                     consider branch/PR deploymentsIn any production pipeline, you want the flexibility
                     of having a ‚Äúsafe‚Äù alternative deployment logic. For example, you may want to build
                     your Scala code and deploy it to an alternative location in S3 while pushing a sandbox
                     version of your workflow that points to this alternative location. Something this
                     simple gets very complicated very quickly and requires the user to consider a number
                     of things. Where should this alternative location be in S3? Is a single location enough?
                     How do you set up your deployment logic to know when to deploy the workflow to a test
                     or dev environment? Answers to these questions often end up being more custom logic
                     inside of the user‚Äôs deployment scripts.Cannot rollback to previous workflow&nbsp;versionsWhen
                     you deploy a workflow, you really want it to encapsulate an atomic and idempotent
                     unit of work. Part of the reason for that is the desire for the ability to rollback
                     to a previous workflow version and knowing that it will always behave as it did in
                     previous runs. There can be many reasons to rollback but the typical one is when you‚Äôve
                     recognized a regression in a recent deployment that was not caught during testing.
                     In the current design, reverting to a previous workflow definition in your scheduling
                     system is not enough! You have to rebuild your assets from source and move them to
                     your fixed S3 location that your workflow points to. To enable atomic rollbacks, you
                     can add more custom logic to your deployment scripts to always deploy your assets
                     to a new location and generate new pointers for your workflows to use, but that comes
                     with higher complexity that often just doesn‚Äôt feel worth it. More commonly, teams
                     will opt to do more testing to try and catch regressions before deploying to production
                     and will accept the extra burden of rebuilding all of their workflow dependencies
                     in the event of a regression.Runtime dependency on user-managed cloud storage locationsAt
                     runtime, the container must reach out to a user-defined storage location to retrieve
                     the assets required. This causes the user-managed storage system to be a critical
                     runtime dependency. If we zoom out to look at an entire workflow management system,
                     the runtime dependencies can become unwieldy if it relies on various storage systems
                     that are arbitrarily defined by the workflow developers!Dataflow deployment with asset
                     managementIn the attempt to deliver a simple and robust solution to the managed workflow
                     deployments we created a command line utility called Dataflow. It is a Python based
                     CLI + library that can be installed anywhere inside the Netflix environment. This
                     utility can build and configure workflow definitions and their assets during testing
                     and deployment. See below&nbsp;diagram:Figure 2. Dataflow asset management system.In Figure
                     2, we show a variation of the typical manually constructed deployment pipeline. Every
                     asset deployment is released to some newly calculated UUID. The workflow definition
                     can then identify a specific asset by its UUID. Deploying the workflow to the scheduling
                     system produces a ‚ÄúDeployment Bundle‚Äù. The bundle includes all of the assets that
                     have been referenced by the workflow definition and the entire bundle is deployed
                     to the scheduling system. At every scheduled runtime, the scheduling system can create
                     an instance of your workflow without having to gather runtime dependencies from external&nbsp;systems.The
                     asset management system that we‚Äôve created for Dataflow provides a strong abstraction
                     over this deployment design. Deploying the asset, generating the UUID, and building
                     the deployment bundle is all handled automatically by the Dataflow build logic. The
                     user does not need to be aware of anything that‚Äôs happening on S3, nor that S3 is
                     being used at all! Instead, the user is given a flexible UUID referencing system that‚Äôs
                     layered on top of our scheduling system‚Äôs workflow DSL. Later in the article we‚Äôll
                     cover this referencing system in some detail. But first, let‚Äôs look at an example
                     of deploying an asset and a workflow.Deployment of an&nbsp;assetLet‚Äôs walk through an example
                     of a workflow asset build and deployment. Let‚Äôs assume we have a repository called
                     stranger-data with the following structure:.‚îú‚îÄ‚îÄ dataflow.yaml‚îú‚îÄ‚îÄ pyspark-workflow‚îÇ
                     ‚îú‚îÄ‚îÄ main.sch.yaml‚îÇ ‚îî‚îÄ‚îÄ hello_world‚îÇ     ‚îú‚îÄ‚îÄ ...‚îÇ     ‚îî‚îÄ‚îÄ setup.py‚îî‚îÄ‚îÄ scala-workflow
                     ‚îú‚îÄ‚îÄ build.gradle    ‚îú‚îÄ‚îÄ main.sch.yaml    ‚îî‚îÄ‚îÄ src    ‚îú‚îÄ‚îÄ main    ‚îÇ   ‚îî‚îÄ‚îÄ ...    ‚îî‚îÄ‚îÄ
                     test        ‚îî‚îÄ‚îÄ ...Let‚Äôs now use Dataflow command to see what project components are&nbsp;visible:stranger-data$
                     dataflow project listPython Assets: -&amp;gt; ./pyspark-workflow/hello_world/setup.pySummary:
                     1 found.Gradle Assets: -&amp;gt; ./scala-workflow/build.gradleSummary: 1 found.Scheduler
                     Workflows: -&amp;gt; ./scala-workflow/main.sch.yaml -&amp;gt; ./pyspark-workflow/main.sch.yamlSummary:
                     2found.Before deploying the assets, and especially if we made any changes to them,
                     we can run unit tests to make sure that we didn‚Äôt break anything. In a typical Dataflow
                     configuration this manual testing is optional because Dataflow continuous integration
                     tests will do that for us on any pull-request.stranger-data$ dataflow project testTesting
                     Python Assets: -&amp;gt; ./pyspark-workflow/hello_world/setup.py... PASSEDSummary: 1 successful,
                     0 failed.Testing Gradle Assets: -&amp;gt; ./scala-workflow/build.gradle... PASSEDSummary:
                     1 successful, 0 failed.Building Scheduler Workflows: -&amp;gt; ./scala-workflow/main.sch.yaml...
                     CREATED ./.workflows/scala-workflow.main.sch.rendered.yaml -&amp;gt; ./pyspark-workflow/main.sch.yaml...
                     CREATED ./.workflows/pyspark-workflow.main.sch.rendered.yamlSummary: 2 successful,
                     0 failed.Testing Scheduler Workflows: -&amp;gt; ./scala-workflow/main.sch.yaml... PASSED
                     -&amp;gt; ./pyspark-workflow/main.sch.yaml... PASSEDSummary: 2 successful, 0 failed.Notice
                     that the test command we use above not only executes unit test suites defined in our
                     Scala and Python sub-projects, but it also renders and statically validates all the
                     workflow definitions in our repo, but more on that&nbsp;later‚Ä¶Assuming all tests passed,
                     let‚Äôs now use the Dataflow command to build and deploy a new version of the Scala
                     and Python assets into the Dataflow asset registry.stranger-data$ dataflow project
                     deployBuilding Python Assets: -&amp;gt; ./pyspark-workflow/hello_world/setup.py... CREATED
                     ./pyspark-workflow/hello_world/dist/hello_world-0.0.1-py3.7.eggSummary: 1 successful,
                     0 failed.Deploying Python Assets: -&amp;gt; ./pyspark-workflow/hello_world/setup.py...
                     DEPLOYED AS dataflow.egg.hello_world.user.stranger-data.master.39206ee8.3Summary:
                     1 successful, 0 failed.Building Gradle Assets: -&amp;gt; ./scala-workflow/build.gradle...
                     CREATED ./scala-workflow/build/libs/scala-workflow-all.jarSummary: 1 successful, 0
                     failed.Deploying Gradle Assets: -&amp;gt; ./scala-workflow/build.gradle... DEPLOYED AS
                     dataflow.jar.scala-workflow.user.stranger-data.master.39206ee8.11Summary: 1 successful,
                     0 failed....Notice that the above&nbsp;command:created a new version of the workflow&nbsp;assetsassigned
                     the asset a ‚ÄúUUID‚Äù (consisting of the ‚Äúdataflow‚Äù string, asset type, asset namespace,
                     git repo owner, git repo name, git branch name, commit hash and consecutive build&nbsp;number)and
                     deployed them to a Dataflow managed S3 location.We can check the existing assets of
                     any given type deployed to any given namespace using the following Dataflow&nbsp;command:stranger-data$
                     dataflow project list eggs --namespace hello_world --deployedProject namespaces with
                     deployed EGGS:hello_world -&amp;gt; dataflow.egg.hello_world.user.stranger-data.master.39206ee8.3
                     -&amp;gt; dataflow.egg.hello_world.user.stranger-data.master.39206ee8.2 -&amp;gt; dataflow.egg.hello_world.user.stranger-data.master.39206ee8.1The
                     above list could come in handy, for example if we needed to find and access an older
                     version of an asset deployed from a given branch and commit&nbsp;hash.Deployment of a&nbsp;workflowNow
                     let‚Äôs have a look at the build and deployment of the workflow definition which references
                     the above assets as part of its pipeline&nbsp;DAG.Let‚Äôs list the workflow definitions in
                     our repo&nbsp;again:stranger-data$ dataflow project list workflowsScheduler Workflows:
                     -&amp;gt; ./scala-workflow/main.sch.yaml -&amp;gt; ./pyspark-workflow/main.sch.yamlSummary:
                     2 found.And let‚Äôs look at part of the content of one of these workflows:stranger-data$
                     cat ./scala-workflow/main.sch.yaml...dag: - ddl -&amp;gt; write - write -&amp;gt; audit -
                     audit -&amp;gt; publishjobs: - ddl: ... - write:     spark:       script: ${dataflow.jar.scala-workflow}
                     class: com.netflix.spark.ExampleApp       conf: ...       params: ... - audit: ...
                     - publish: ......You can see from the above snippet that the write job wants to access
                     some version of the JAR from the scala-workflow namespace. A typical workflow definition,
                     written in YAML, does not need any compilation before it is shipped to the Scheduler
                     API, but Dataflow designates a special step called ‚Äúrendering‚Äù to substitute all of
                     the Dataflow variables and build the final&nbsp;version.The above expression ${dataflow.jar.scala-workflow}
                     means that the workflow will be rendered and deployed with the latest version of the
                     scala-workflow JAR available at the time of the workflow deployment. It is possible
                     that the JAR is built as part of the same repository in which case the new build of
                     the JAR and a new version of the workflow may be coming from the same deployment.
                     But the JAR may be built as part of a completely different project and in that case
                     the testing and deployment of the new workflow version can be completely decoupled.We
                     showed above how one would request the latest asset version available during deployment,
                     but with Dataflow asset management we can distinguish two more asset access patterns.
                     An obvious next one is to specify it by all its attributes: asset type, asset namespace,
                     git repo owner, git repo name, git branch name, commit hash and consecutive build
                     number. There is one more extra method for a middle ground solution to pick a specific
                     build for a given namespace and git branch, which can help during testing and development.
                     All of this is part of the user-interface for determining how the deployment bundle
                     will be created. See below diagram for a visual illustration.Figure 3. A closer at
                     the Deployment BundleIn short, using the above variables gives the user full flexibility
                     and allows them to pick any version of any asset in any workflow.An example of the
                     workflow deployment with the rendering step is shown&nbsp;below:stranger-data$ dataflow
                     project deploy...Building Scheduler Workflows: -&amp;gt; ./scala-workflow/main.sch.yaml...
                     CREATED ./.workflows/scala-workflow.main.sch.rendered.yaml -&amp;gt; ./pyspark-workflow/main.sch.yaml...
                     CREATED ./.workflows/pyspark-workflow.main.sch.rendered.yamlSummary: 2 successful,
                     0 failed.Deploying Scheduler Workflows: -&amp;gt; ./scala-workflow/main.sch.yaml‚Ä¶ DEPLOYED
                     AS https://hawkins.com/scheduler/sandbox:user.stranger-data.scala-workflow -&amp;gt; ./pyspark-workflow/main.sch.yaml‚Ä¶
                     DEPLOYED AS https://hawkins.com/scheduler/sandbox:user.stranger-data.pyspark-workflowSummary:
                     2 successful, 0 failed.And here you can see what the workflow definition looks like
                     before it is sent to the Scheduler API and registered as the latest version. Notice
                     the value of the script variable of the write job. In the original code says ${dataflow.jar.scala-workflow}
                     and in the rendered version it is translated to a specific file&nbsp;pointer:stranger-data$
                     cat ./scala-workflow/main.sch.yaml...dag: - ddl -&amp;gt; write - write -&amp;gt; audit -
                     audit -&amp;gt; publishjobs: - ddl: ... - write:     spark:       script: s3://dataflow/jars/scala-workflow/user/stranger-data/master/39206ee8/1.jar
                     class: com.netflix.spark.ExampleApp       conf: ...       params: ... - audit: ...
                     - publish: ......User perspectiveThe Infrastructure DSE team at Netflix is responsible
                     for providing insights into data that can help the Netflix platform and service scale
                     in a secure and effective way. Our team members partner with business units like Platform,
                     OpenConnect, InfoSec and engage in enterprise level initiatives on a regular&nbsp;basis.One
                     side effect of such wide engagement is that over the years our repository evolved
                     into a mono-repo with each module requiring a customized build, testing and deployment
                     strategy packaged into a single Jenkins job. This setup required constant upkeep and
                     also meant every time we had a build failure multiple people needed to spend a lot
                     of time in communication to ensure they did not step on each&nbsp;other.Last quarter we
                     decided to split the mono-repo into separate modules and adopt Dataflow as our asset
                     orchestration tool. Post deployment, the team relies on Dataflow for automated execution
                     of unit tests, management and deployment of workflow related&nbsp;assets.By the end of
                     the migration process our Jenkins configuration went&nbsp;from:Figure 4. Real example of
                     a deployment script.to:cd /dataflow_workspacedataflow project deployThe simplicity
                     of deployment enabled the team to focus on the problems they set out to solve while
                     the branch based customization gave us the flexibility to be our most effective at
                     solving&nbsp;them.ConclusionsThis new method available for Netflix data engineers makes
                     workflow management easier, more transparent and more reliable. And while it remains
                     fairly easy and safe to build your business logic code (in Scala, Python, etc) in
                     the same repository as the workflow definition that invokes it, the new Dataflow versioned
                     asset registry makes it easier yet to build that code completely independently and
                     then reference it safely inside data pipelines in any other Netflix repository, thus
                     enabling easy code sharing and&nbsp;reuse.One more aspect of data workflow development
                     that gets enabled by this functionality is what we call branch-driven deployment.
                     This approach enables multiple versions of your business logic and workflows to be
                     running at the same time in the scheduler ecosystem, and makes it easy, not only for
                     individual users to run isolated versions of the code during development, but also
                     to define isolated staging environments through which the code can pass before it
                     reaches the production stage. Obviously, in order for the workflows to be safely used
                     in that configuration they must comply with a few simple rules with regards to the
                     parametrization of their inputs and outputs, but let‚Äôs leave this subject for another
                     blog&nbsp;post.CreditsSpecial thanks to Peter Volpe, Harrington Joseph and Daniel Watson
                     for the initial design&nbsp;review.Data pipeline asset management with Dataflow was originally
                     published in Netflix TechBlog on Medium, where people are continuing the conversation
                     by highlighting and responding to this story.</span></summary><time datetime="2022-02-09T19:33:22+02:00">Wed, 9 Feb 2022 17:33</time><article><p>by Sam Setegne, Jai Balani, Olek¬†Gorajek</p><h3>Glossary</h3><ul><li><strong><em>asset</em></strong>‚Ää‚Äî‚Ääany business logic code in a raw (e.g. SQL) or compiled (e.g. JAR) form to be executed as part of the user defined data pipeline.</li><li><strong><em>data pipeline</em></strong>‚Ää‚Äî‚Ääa set of tasks (or jobs) to be executed in a predefined order (a.k.a. DAG) for the purpose of transforming data using some business¬†logic.</li><li><strong><em>Dataflow</em></strong>‚Ää‚Äî‚ÄäNetflix homegrown CLI tool for data pipeline management.</li><li><strong>job‚Ää</strong>‚Äî‚Ääa.k.a task, an atomic unit of data transformation logic, a non-separable execution block in the workflow¬†chain.</li><li><strong><em>namespace</em></strong>‚Ää‚Äî‚Ääunique label, usually representing a business subject area, assigned to a workflow asset to identify it across all other assets managed by Dataflow (e.g. security).</li><li><strong><em>workflow</em></strong>‚Ää‚Äî‚Ääsee ‚Äúdata pipeline‚Äù</li></ul><h3>Intro</h3><p>The problem of managing scheduled workflows and their assets is as old as the use of cron daemon in early Unix operating systems. The design of a cron job is simple, you take some system command, you pick the schedule to run it on and you are done.¬†Example:</p><pre>0 0 * * MON /home/alice/backup.sh</pre><p>In the above example the system would wake up every Monday morning and execute the backup.sh script. Simple right? But what if the script does not exist in the given path, or what if it existed initially but then Alice let Bob access her home directory and he accidentally deleted it? Or what if Alice wanted to add new backup functionality and she accidentally broke existing code while updating¬†it?</p><p>The answers to these questions is something we would like to address in this article and propose a clean solution to this¬†problem.</p><p>Let‚Äôs define some requirements that we are interested in delivering to the Netflix data engineers or anyone who would like to schedule a workflow with some external assets in it. By external assets we simply mean some executable carrying the actual business logic of the job. It could be a JAR compiled from Scala, a Python script or module, or a simple SQL file. The important thing is that this business logic can be built in a separate repository and maintained independently from the workflow definition. Keeping all that in mind we would like to achieve the following properties for the whole workflow deployment:</p><ol><li><strong>Versioning</strong>: we want both the workflow definition and its assets to be versioned and we want the versions to be tied together in a clear¬†way.</li><li><strong>Transparency</strong>: we want to know which version of an asset is running along with every workflow instance, so if there are any issues we can easily identify which version caused the problem and to which one we could revert, if necessary.</li><li><strong>ACID deployment</strong>: for every scheduler workflow definition change, we would like to have all the workflow assets bundled in an atomic, durable, isolated and consistent manner. This way, if necessary, all we need to know is which version of the workflow to roll back to, and the rest would be taken care of for¬†us.</li></ol><p>While all the above goals are our North Star, we also don‚Äôt want to negatively affect fast deployment, high availability and arbitrary life span of any deployed¬†asset.</p><h3>Previous solutions</h3><p>The basic approach to pulling down arbitrary workflow resources during workflow execution has been known to mankind since the invention of cron, and with the advent of ‚Äúinfinite‚Äù cloud storage systems like S3, this approach has served us for many years. Its apparent flexibility and convenience can often fool us into thinking that by simply replacing the asset in the S3 location we can, without any hassle, introduce changes to our business logic. This method often proves very troublesome especially if there is more than one engineer working on the same pipeline and they are not all aware of the other folks‚Äô ‚Äúdeployment process‚Äù.</p><p>The slightly improved approach is shown on the diagram¬†below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/976/0*DwP-Z_4MrdAz5UTG" /><figcaption><strong>Figure 1.</strong> Manually constructed continuous delivery¬†system.</figcaption></figure><p>In Figure 1, you can see an illustration of a typical deployment pipeline manually constructed by a user for an individual project. The continuous deployment tool submits a workflow definition with pointers to assets in fixed S3 locations. These assets are then separately deployed to these fixed locations. At runtime, the assets are retrieved from the defined locations in S3 and executed in the runtime container. Despite requiring users to construct the deployment pipeline manually, often by writing their own scripts from scratch, this design works and has been successfully used by many teams for years. That being said, it does have some drawbacks that are revealed as you try to add any amount of complexity to your deployment logic. Let‚Äôs discuss a few of¬†them.</p><h4><strong>Does not consider branch/PR deployments</strong></h4><p>In any production pipeline, you want the flexibility of having a ‚Äúsafe‚Äù alternative deployment logic. For example, you may want to build your Scala code and deploy it to an alternative location in S3 while pushing a sandbox version of your workflow that points to this alternative location. Something this simple gets very complicated very quickly and requires the user to consider a number of things. Where should this alternative location be in S3? Is a single location enough? How do you set up your deployment logic to know when to deploy the workflow to a test or dev environment? Answers to these questions often end up being more custom logic inside of the user‚Äôs deployment scripts.</p><h4><strong>Cannot rollback to previous workflow¬†versions</strong></h4><p>When you deploy a workflow, you really want it to encapsulate an atomic and idempotent unit of work. Part of the reason for that is the desire for the ability to rollback to a previous workflow version and knowing that it will always behave as it did in previous runs. There can be many reasons to rollback but the typical one is when you‚Äôve recognized a regression in a recent deployment that was not caught during testing. In the current design, reverting to a previous workflow definition in your scheduling system is not enough! You have to rebuild your assets from source and move them to your fixed S3 location that your workflow points to. To enable atomic rollbacks, you can add more custom logic to your deployment scripts to always deploy your assets to a new location and generate new pointers for your workflows to use, but that comes with higher complexity that often just doesn‚Äôt feel worth it. More commonly, teams will opt to do more testing to try and catch regressions before deploying to production and will accept the extra burden of rebuilding all of their workflow dependencies in the event of a regression.</p><h4><strong>Runtime dependency on user-managed cloud storage locations</strong></h4><p>At runtime, the container must reach out to a user-defined storage location to retrieve the assets required. This causes the user-managed storage system to be a critical runtime dependency. If we zoom out to look at an entire workflow management system, the runtime dependencies can become unwieldy if it relies on various storage systems that are arbitrarily defined by the workflow developers!</p><h3>Dataflow deployment with asset management</h3><p>In the attempt to deliver a simple and robust solution to the managed workflow deployments we created a command line utility called Dataflow. It is a Python based CLI + library that can be installed anywhere inside the Netflix environment. This utility can build and configure workflow definitions and their assets during testing and deployment. See below¬†diagram:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/968/0*l2pFeuXB6azOzen0" /><figcaption><strong>Figure 2</strong>. Dataflow asset management system.</figcaption></figure><p>In Figure 2, we show a variation of the typical manually constructed deployment pipeline. Every asset deployment is released to some newly calculated UUID. The workflow definition can then identify a specific asset by its UUID. Deploying the workflow to the scheduling system produces a ‚ÄúDeployment Bundle‚Äù. The bundle includes all of the assets that have been referenced by the workflow definition and the entire bundle is deployed to the scheduling system. At every scheduled runtime, the scheduling system can create an instance of your workflow without having to gather runtime dependencies from external¬†systems.</p><p>The asset management system that we‚Äôve created for Dataflow provides a strong abstraction over this deployment design. Deploying the asset, generating the UUID, and building the deployment bundle is all handled automatically by the Dataflow build logic. The user does not need to be aware of anything that‚Äôs happening on S3, nor that S3 is being used at all! Instead, the user is given a flexible UUID referencing system that‚Äôs layered on top of our scheduling system‚Äôs workflow DSL. Later in the article we‚Äôll cover this referencing system in some detail. But first, let‚Äôs look at an example of deploying an asset and a workflow.</p><h4>Deployment of an¬†asset</h4><p>Let‚Äôs walk through an example of a workflow asset build and deployment. Let‚Äôs assume we have a repository called <strong>stranger-data</strong> with the following structure:</p><pre>.<br>‚îú‚îÄ‚îÄ dataflow.yaml<br>‚îú‚îÄ‚îÄ pyspark-workflow<br>‚îÇ ‚îú‚îÄ‚îÄ main.sch.yaml<br>‚îÇ ‚îî‚îÄ‚îÄ hello_world<br>‚îÇ     ‚îú‚îÄ‚îÄ ...<br>‚îÇ     ‚îî‚îÄ‚îÄ setup.py<br>‚îî‚îÄ‚îÄ scala-workflow<br>    ‚îú‚îÄ‚îÄ build.gradle<br>    ‚îú‚îÄ‚îÄ main.sch.yaml<br>    ‚îî‚îÄ‚îÄ src<br>    ‚îú‚îÄ‚îÄ main<br>    ‚îÇ   ‚îî‚îÄ‚îÄ ...<br>    ‚îî‚îÄ‚îÄ test<br>        ‚îî‚îÄ‚îÄ ...</pre><p>Let‚Äôs now use Dataflow command to see what project components are¬†visible:</p><pre>stranger-data$ <strong>dataflow project list</strong></pre><pre>Python Assets:<br> -&gt; ./pyspark-workflow/hello_world/setup.py<br>Summary: 1 found.</pre><pre>Gradle Assets:<br> -&gt; ./scala-workflow/build.gradle<br>Summary: 1 found.</pre><pre>Scheduler Workflows:<br> -&gt; ./scala-workflow/main.sch.yaml<br> -&gt; ./pyspark-workflow/main.sch.yaml<br>Summary: 2found.</pre><p>Before deploying the assets, and especially if we made any changes to them, we can run unit tests to make sure that we didn‚Äôt break anything. In a typical Dataflow configuration this manual testing is optional because Dataflow continuous integration tests will do that for us on any pull-request.</p><pre>stranger-data$ <strong>dataflow project test</strong></pre><pre>Testing Python Assets:<br> -&gt; ./pyspark-workflow/hello_world/setup.py... PASSED<br>Summary: 1 successful, 0 failed.</pre><pre>Testing Gradle Assets:<br> -&gt; ./scala-workflow/build.gradle... PASSED<br>Summary: 1 successful, 0 failed.</pre><pre>Building Scheduler Workflows:<br> -&gt; ./scala-workflow/main.sch.yaml... CREATED ./.workflows/scala-workflow.main.sch.rendered.yaml<br> -&gt; ./pyspark-workflow/main.sch.yaml... CREATED ./.workflows/pyspark-workflow.main.sch.rendered.yaml<br>Summary: 2 successful, 0 failed.</pre><pre>Testing Scheduler Workflows:<br> -&gt; ./scala-workflow/main.sch.yaml... PASSED<br> -&gt; ./pyspark-workflow/main.sch.yaml... PASSED<br>Summary: 2 successful, 0 failed.</pre><p>Notice that the test command we use above not only executes unit test suites defined in our Scala and Python sub-projects, but it also renders and statically validates all the workflow definitions in our repo, but more on that¬†later‚Ä¶</p><p>Assuming all tests passed, let‚Äôs now use the Dataflow command to build and deploy a new version of the Scala and Python assets into the Dataflow asset registry.</p><pre>stranger-data$ <strong>dataflow project deploy</strong></pre><pre>Building Python Assets:<br> -&gt; ./pyspark-workflow/hello_world/setup.py... CREATED ./pyspark-workflow/hello_world/dist/hello_world-0.0.1-py3.7.egg<br>Summary: 1 successful, 0 failed.</pre><pre>Deploying Python Assets:<br> -&gt; ./pyspark-workflow/hello_world/setup.py... DEPLOYED AS <strong>dataflow.egg.hello_world.user.stranger-data.master.39206ee8.3<br></strong>Summary: 1 successful, 0 failed.</pre><pre>Building Gradle Assets:<br> -&gt; ./scala-workflow/build.gradle... CREATED ./scala-workflow/build/libs/scala-workflow-all.jar<br>Summary: 1 successful, 0 failed.</pre><pre>Deploying Gradle Assets:<br> -&gt; ./scala-workflow/build.gradle... DEPLOYED AS <strong>dataflow.jar.scala-workflow.user.stranger-data.master.39206ee8.11<br></strong>Summary: 1 successful, 0 failed.</pre><pre>...</pre><p>Notice that the above¬†command:</p><ul><li>created a new version of the workflow¬†assets</li><li>assigned the asset a ‚ÄúUUID‚Äù (consisting of the ‚Äúdataflow‚Äù string, asset type, asset namespace, git repo owner, git repo name, git branch name, commit hash and consecutive build¬†number)</li><li>and deployed them to a Dataflow managed S3 location.</li></ul><p>We can check the existing assets of any given type deployed to any given namespace using the following Dataflow¬†command:</p><pre>stranger-data$ <strong>dataflow project list eggs --namespace hello_world --deployed</strong></pre><pre>Project namespaces with deployed EGGS:</pre><pre>hello_world<br> -&gt; dataflow.egg.hello_world.user.stranger-data.master.39206ee8.3<br> -&gt; dataflow.egg.hello_world.user.stranger-data.master.39206ee8.2<br> -&gt; dataflow.egg.hello_world.user.stranger-data.master.39206ee8.1</pre><p>The above list could come in handy, for example if we needed to find and access an older version of an asset deployed from a given branch and commit¬†hash.</p><h4>Deployment of a¬†workflow</h4><p>Now let‚Äôs have a look at the build and deployment of the workflow definition which references the above assets as part of its pipeline¬†DAG.</p><p>Let‚Äôs list the workflow definitions in our repo¬†again:</p><pre>stranger-data$ <strong>dataflow project list workflows</strong></pre><pre>Scheduler Workflows:<br> -&gt; ./scala-workflow/main.sch.yaml<br> -&gt; ./pyspark-workflow/main.sch.yaml<br>Summary: 2 found.</pre><p>And let‚Äôs look at part of the content of one of these workflows:</p><pre>stranger-data$ <strong>cat ./scala-workflow/main.sch.yaml<br></strong>...<br>dag:<br> - ddl -&gt; write<br> - write -&gt; audit<br> - audit -&gt; publish<br>jobs:<br> - ddl: ...<br> - write:<br>     spark:<br>       script: <strong>${dataflow.jar.scala-workflow}<br>       </strong>class: com.netflix.spark.ExampleApp<br>       conf: ...<br>       params: ...<br> - audit: ...<br> - publish: ...<br>...</pre><p>You can see from the above snippet that the write job wants to access some version of the JAR from the scala-workflow namespace. A typical workflow definition, written in YAML, does not need any compilation before it is shipped to the Scheduler API, but Dataflow designates a special step called ‚Äúrendering‚Äù to substitute all of the Dataflow variables and build the final¬†version.</p><p>The above expression ${dataflow.jar.scala-workflow} means that the workflow will be rendered and deployed with the latest version of the scala-workflow JAR available at the time of the workflow deployment. It is possible that the JAR is built as part of the same repository in which case the new build of the JAR and a new version of the workflow may be coming from the same deployment. But the JAR may be built as part of a completely different project and in that case the testing and deployment of the new workflow version can be completely decoupled.</p><p>We showed above how one would request the latest asset version available during deployment, but with Dataflow asset management we can distinguish two more asset access patterns. An obvious next one is to specify it by all its attributes: asset type, asset namespace, git repo owner, git repo name, git branch name, commit hash and consecutive build number. There is one more extra method for a middle ground solution to pick a specific build for a given namespace and git branch, which can help during testing and development. All of this is part of the user-interface for determining how the deployment bundle will be created. See below diagram for a visual illustration.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*cC9kyLjonVmjSZdl" /><figcaption><strong>Figure 3</strong>. A closer at the Deployment Bundle</figcaption></figure><p>In short, using the above variables gives the user full flexibility and allows them to pick any <strong>version</strong> of any <strong>asset</strong> in any <strong>workflow.</strong></p><p>An example of the workflow deployment with the rendering step is shown¬†below:</p><pre>stranger-data$ <strong>dataflow project deploy</strong></pre><pre>...</pre><pre>Building Scheduler Workflows:<br> -&gt; ./scala-workflow/main.sch.yaml... CREATED ./.workflows/scala-workflow.main.sch.rendered.yaml<br> -&gt; ./pyspark-workflow/main.sch.yaml... CREATED ./.workflows/pyspark-workflow.main.sch.rendered.yaml<br>Summary: 2 successful, 0 failed.</pre><pre>Deploying Scheduler Workflows:<br> -&gt; ./scala-workflow/main.sch.yaml‚Ä¶ DEPLOYED AS <a href="https://hawkins.com/scheduler/sandbox:user.stranger-etl.scala-workflow">https://hawkins.com/scheduler/sandbox:user.stranger-data.scala-workflow</a><br> -&gt; ./pyspark-workflow/main.sch.yaml‚Ä¶ DEPLOYED AS <a href="https://hawkins.com/scheduler/sandbox:user.stranger-etl.pyspark-workflow">https://hawkins.com/scheduler/sandbox:user.stranger-data.pyspark-workflow</a><br>Summary: 2 successful, 0 failed.</pre><p>And here you can see what the workflow definition looks like before it is sent to the Scheduler API and registered as the latest version. Notice the value of the script variable of the write job. In the original code says ${dataflow.jar.scala-workflow} and in the rendered version it is translated to a specific file¬†pointer:</p><pre>stranger-data$ <strong>cat ./scala-workflow/main.sch.yaml<br></strong>...<br>dag:<br> - ddl -&gt; write<br> - write -&gt; audit<br> - audit -&gt; publish<br>jobs:<br> - ddl: ...<br> - write:<br>     spark:<br>       script: <strong>s3://dataflow/jars/scala-workflow/user/stranger-data/master/39206ee8/1.jar<br>       </strong>class: com.netflix.spark.ExampleApp<br>       conf: ...<br>       params: ...<br> - audit: ...<br> - publish: ...<br>...</pre><h3>User perspective</h3><p>The Infrastructure DSE team at Netflix is responsible for providing insights into data that can help the Netflix platform and service scale in a secure and effective way. Our team members partner with business units like Platform, OpenConnect, InfoSec and engage in enterprise level initiatives on a regular¬†basis.</p><p>One side effect of such wide engagement is that over the years our repository evolved into a mono-repo with each module requiring a customized build, testing and deployment strategy packaged into a single Jenkins job. This setup required constant upkeep and also meant every time we had a build failure multiple people needed to spend a lot of time in communication to ensure they did not step on each¬†other.</p><p>Last quarter we decided to split the mono-repo into separate modules and adopt Dataflow as our asset orchestration tool. Post deployment, the team relies on Dataflow for automated execution of unit tests, management and deployment of workflow related¬†assets.</p><p>By the end of the migration process our Jenkins configuration went¬†from:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*84Syq7sY9PniGsWs" /><figcaption>Figure 4. Real example of a deployment script.</figcaption></figure><p>to:</p><pre>cd /dataflow_workspace<br>dataflow project deploy</pre><p>The simplicity of deployment enabled the team to focus on the problems they set out to solve while the branch based customization gave us the flexibility to be our most effective at solving¬†them.</p><h3>Conclusions</h3><p>This new method available for Netflix data engineers makes workflow management easier, more transparent and more reliable. And while it remains fairly easy and safe to build your business logic code (in Scala, Python, etc) in the same repository as the workflow definition that invokes it, the new Dataflow versioned asset registry makes it easier yet to build that code completely independently and then reference it safely inside data pipelines in any other Netflix repository, thus enabling easy code sharing and¬†reuse.</p><p>One more aspect of data workflow development that gets enabled by this functionality is what we call branch-driven deployment. This approach enables multiple versions of your business logic and workflows to be running at the same time in the scheduler ecosystem, and makes it easy, not only for individual users to run isolated versions of the code during development, but also to define isolated staging environments through which the code can pass before it reaches the production stage. Obviously, in order for the workflows to be safely used in that configuration they must comply with a few simple rules with regards to the parametrization of their inputs and outputs, but let‚Äôs leave this subject for another blog¬†post.</p><h3>Credits</h3><p>Special thanks to Peter Volpe, Harrington Joseph and Daniel Watson for the initial design¬†review.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=86525b3e21ca" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/data-pipeline-asset-management-with-dataflow-86525b3e21ca">Data pipeline asset management with Dataflow</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://netflixtechblog.com/demystifying-interviewing-for-backend-engineers-netflix-aceb26a83495?source=rss----2615bd06b42e---4">Demystifying Interviewing for Backend Engineers @ Netflix</a> <span>By Karen Casella, Director of Engineering, Access &amp;amp; Identity ManagementHave you
                     ever experienced one of the following scenarios while looking for your next&nbsp;role?You
                     study and practice coding interview problems for hours/days/weeks/months, only to
                     be asked to merge two sorted&nbsp;lists.You apply for multiple roles at the same company
                     and proceed through the interview process with each hiring team separately, despite
                     the fact that there is tremendous overlap in the&nbsp;roles.You go through the interview
                     process, do really well, get really excited about the company and the people you meet,
                     and in the end, you are ‚Äúmatched‚Äù to a role that does not excite you, working with
                     a manager and team you have not even met during the interview process.Interviewing
                     can be a daunting endeavor and how companies, and teams, approach the process varies
                     greatly. We hope that by demystifying the process, you will feel more informed and
                     confident about your interview experience.Backend Engineering Interview LoopWhen you
                     apply for a backend engineering role at Netflix, or if one of our recruiters or hiring
                     managers find your LinkedIn profile interesting, a recruiter or hiring manager reviews
                     your technical background and experience to see if your experience is aligned with
                     our requirements. If so, we invite you to begin the interview process.Most backend
                     engineering teams follow a process very similar to what is shown below. While this
                     is a relatively stream-lined process, it is not as efficient if a candidate is interested
                     in or qualified for multiple roles within the organization.Following is a brief description
                     of each of these&nbsp;stages.Recruiter Phone Screen: A member of our talent team contacts
                     you to explain the process and to assess high-level qualifications&nbsp;. The recruiter
                     also reviews the relevant open roles to see if you have a strong affinity for one
                     or another. If your interests and experience align well with one or more of the roles,
                     they schedule a phone screen with one of the hiring managers.Manager Phone Screen:
                     The purpose of this discussion is to get a sense for your technical background, your
                     approach to problem solving, and how you work. It‚Äôs also a great opportunity for you
                     to learn more about the available roles, the technical challenges the teams are facing
                     and what it‚Äôs like to work on a backend engineering team at&nbsp;Netflix.Technical Screen:
                     The final screen before on-site interviews is used to assess your technical skills
                     and match for the team. For many roles, you will be given a choice between a take-home
                     coding exercise or a one-hour discussion with one of the engineers from the team.
                     The problems you are asked to solve are related to the work of the&nbsp;team.Round 1 Interviews:
                     If you are invited on-site, the first round interview is with four or five people
                     for 45 minutes each. The interview panel consists of two or three engineers, a hiring
                     manager and a recruiter. The engineers assess your technical skills by asking you
                     to solve various design and coding problems. These questions reflect actual challenges
                     that our teams&nbsp;face.Round 2 Interviews: You meet with two or three additional people,
                     for 45 minutes each. The interview panel comprises an engineering director, a partner
                     engineer or manager, and another engineering leader. The focus of this round is to
                     assess how well you partner with other teams and your non-technical skills.Decision
                     &amp;amp; Offer: After round 2, we review the feedback and decide whether or not we will
                     be offering you a role. If so, you will work with the recruiter to discuss compensation
                     expectations, answer any questions that remain for you, and discuss a start date with
                     your new&nbsp;team.Enter Centralized HiringSome Netflix backend engineering teams, seeking
                     stunning colleagues with similar backgrounds and talents, are joining forces and adopting
                     a centralized hiring model. Centralized hiring is an approach of making multiple hiring
                     decisions through one unified hiring process across multiple teams with shared needs
                     in skill, function and experience level.The interview approach does not vary much
                     from what is shown above, with one big exception: there are several potential ‚Äúpivot
                     points‚Äù where you and / or Netflix may decide to focus on a particular role based
                     on your experience and preference. At each stage of the process, we consider your
                     preference and skills and may focus your remaining interviews with a specific team
                     if we both consider it a strong match. It‚Äôs important to note that, even though your
                     experience may not be an exact match for one team, you might be more closely aligned
                     with another team. In that case, we would pivot you to another team rather than disqualify
                     you from the&nbsp;process.Interview TipsInterviewing can be intimidating and stressful!
                     Being prepared can help you minimize stress and anxiety. Following are a few quick
                     tips to help you&nbsp;prepare:Review your profile and make connections between your experience
                     and the job description.Think about your past work experiences and prepare some examples
                     of when you achieved something amazing, or had some tough challenges.We recommend
                     against interview coding practice puzzle-type exercises, as we don‚Äôt ask those types
                     of questions. If you want to practice, focus on medium-difficulty real-world problems
                     you might encounter in a software engineering role.Be sure to have questions prepared
                     to ask the interviewers. This is a conversation, not an inquisition!We are here to
                     accommodate any accessibility needs you may have, to ensure that you‚Äôre set up for
                     success during your interview. Let us know if you need any assistive technology or
                     other accommodations ahead of time, and we‚Äôll be sure to work with you to get it set&nbsp;up.We
                     want to see you at your best‚Ää‚Äî‚Ääwe are not trying to trick you or trip you up! Try
                     to relax, remember to breathe, and be honest and curious. Remember, this is not just
                     about whether Netflix thinks you are a fit for the role, it‚Äôs about you deciding that
                     Netflix and the role are right for&nbsp;you!Yes, We Are&nbsp;Hiring!Several of our backend engineering
                     teams are searching for our next stunning colleagues. Some of the areas for which
                     we are actively seeking backend engineers include Streaming &amp;amp; Gaming Technologies,
                     Product Innovation, Infrastructure, and Studio Technologies. If any of the high-level
                     descriptions below are of interest to you and seem like a good match for your experience
                     and career goals, we‚Äôd like to hear from you! Simply click on the job description
                     link and submit your application through our jobs&nbsp;site.Streaming &amp;amp; Gaming Technologies(https://jobs.netflix.com/jobs/175726412)You
                     are a distributed systems engineer working on product backend systems that support
                     streaming video and/or mobile &amp;amp; cloud&nbsp;games.You‚Äôre passionate about resilience,
                     scalability, availability, and observability. Passion for large data sets, APIs, access
                     &amp;amp; identity management, or delivering backend systems that enable mobile and cloud
                     gaming is a big&nbsp;plus.Your work centers around architecting, building and operating
                     fault-tolerant distributed systems at massive&nbsp;scale.Product Innovation(https://jobs.netflix.com/jobs/175728345)You
                     are a distributed systems engineer working on core backend services that support our
                     user journeys in signup, subscription, search, personalization and messaging.You‚Äôre
                     passionate about working at the intersection of business, product and technology at
                     large&nbsp;scale.Your work centers around building fault-tolerant backend systems and services
                     that make a direct impact on users and the business.Infrastructure(https://jobs.netflix.com/jobs/122163878)You
                     are a distributed systems engineer working on infrastructure and platforms that enable
                     or amplify the work of other engineering teams or&nbsp;systems.You‚Äôre passionate about
                     scalable and highly available complex distributed systems and have a deep understanding
                     of how they operate and&nbsp;fail.Your work centers around raising levels of abstraction
                     to improve development at scale and creating engineering efficiencies.Studio Technologies(https://jobs.netflix.com/jobs/175745345)You
                     are a software engineer that builds products and services used by creative partners
                     across the studio and external productions to produce and manage all of Netflix global
                     content. Our products enable the entire workflow of content acquisition, production,
                     promotion and financing from script to screen. We create innovative solutions that
                     develop and manage entertainment at scale while helping entertain the world as members
                     find joy in the shows and movies they&nbsp;love.You‚Äôre passionate about innovation, scalability,
                     functionality, shipping high-value features quickly and are committed to delivering
                     exceptional backend systems for our consumers. You‚Äôre humble, curious, and looking
                     to deliver results with other stunning colleagues.Your work centers around building
                     products and services targeting creative partners producing/managing global&nbsp;content.ConclusionNetflix
                     has a Freedom &amp;amp; Responsibility culture in which every Netflix employee has the
                     freedom to do their best work and the responsibility to achieve excellence. We value
                     strong judgment, communication, impact, curiosity, innovation, courage, passion, integrity,
                     selflessness, inclusion, and diversity. For more information on the culture, see http://jobs.netflix.com/culture.Karen
                     Casella is the Director of Engineering for Access &amp;amp; Identity Management technologies
                     for Netflix streaming and gaming products. Connect with Karen on LinkedIn or&nbsp;Twitter.Demystifying
                     Interviewing for Backend Engineers @ Netflix was originally published in Netflix TechBlog
                     on Medium, where people are continuing the conversation by highlighting and responding
                     to this story.</span></summary><time datetime="2022-02-01T16:24:18+02:00">Tue, 1 Feb 2022 14:24</time><article><p><em>By Karen Casella, Director of Engineering, Access &amp; Identity Management</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7zWnpCdZ_OTjyo-6" /></figure><p>Have you ever experienced one of the following scenarios while looking for your next¬†role?</p><ul><li>You study and practice coding interview problems for hours/days/weeks/months, only to be asked to merge two sorted¬†lists.</li><li>You apply for multiple roles at the same company and proceed through the interview process with each hiring team separately, despite the fact that there is tremendous overlap in the¬†roles.</li><li>You go through the interview process, do really well, get really excited about the company and the people you meet, and in the end, you are ‚Äúmatched‚Äù to a role that does not excite you, working with a manager and team you have not even met during the interview process.</li></ul><p>Interviewing can be a daunting endeavor and how companies, and teams, approach the process varies greatly. We hope that by demystifying the process, you will feel more informed and confident about your interview experience.</p><h3>Backend Engineering Interview Loop</h3><p>When you apply for a backend engineering role at Netflix, or if one of our recruiters or hiring managers find your LinkedIn profile interesting, a recruiter or hiring manager reviews your technical background and experience to see if your experience is aligned with our requirements. If so, we invite you to begin the interview process.</p><p>Most backend engineering teams follow a process very similar to what is shown below. While this is a relatively stream-lined process, it is not as efficient if a candidate is interested in or qualified for multiple roles within the organization.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/940/0*p0bYR_ImiRxFBYep" /></figure><p>Following is a brief description of each of these¬†stages.</p><p><strong>Recruiter Phone Screen: </strong>A member of our talent team contacts you to explain the process and to assess high-level qualifications¬†. The recruiter also reviews the relevant open roles to see if you have a strong affinity for one or another. If your interests and experience align well with one or more of the roles, they schedule a phone screen with one of the hiring managers.</p><p><strong>Manager Phone Screen: </strong>The purpose of this discussion is to get a sense for your technical background, your approach to problem solving, and how you work. It‚Äôs also a great opportunity for you to learn more about the available roles, the technical challenges the teams are facing and what it‚Äôs like to work on a backend engineering team at¬†Netflix.</p><p><strong>Technical Screen: </strong>The final screen before on-site interviews is used to assess your technical skills and match for the team. For many roles, you will be given a choice between a take-home coding exercise or a one-hour discussion with one of the engineers from the team. The problems you are asked to solve are related to the work of the¬†team.</p><p><strong>Round 1 Interviews: </strong>If you are invited on-site, the first round interview is with four or five people for 45 minutes each. The interview panel consists of two or three engineers, a hiring manager and a recruiter. The engineers assess your technical skills by asking you to solve various design and coding problems. These questions reflect actual challenges that our teams¬†face.</p><p><strong>Round 2 Interviews: </strong>You meet with two or three additional people, for 45 minutes each. The interview panel comprises an engineering director, a partner engineer or manager, and another engineering leader. The focus of this round is to assess how well you partner with other teams and your non-technical skills.</p><p><strong>Decision &amp; Offer: </strong>After round 2, we review the feedback and decide whether or not we will be offering you a role. If so, you will work with the recruiter to discuss compensation expectations, answer any questions that remain for you, and discuss a start date with your new¬†team.</p><h3>Enter Centralized Hiring</h3><p>Some Netflix backend engineering teams, seeking <a href="https://jobs.netflix.com/culture">stunning colleagues</a> with similar backgrounds and talents, are joining forces and adopting a centralized hiring model. <strong>Centralized hiring is an approach of making multiple hiring decisions through one unified hiring process across multiple teams with shared needs in skill, function and experience level.</strong></p><p>The interview approach does not vary much from what is shown above, with one big exception: <strong><em>there are several potential ‚Äúpivot points‚Äù where you and / or Netflix may decide to focus on a particular role based on your experience and preference. </em></strong>At each stage of the process, we consider your preference and skills and may focus your remaining interviews with a specific team if we both consider it a strong match. It‚Äôs important to note that, even though your experience may not be an exact match for one team, you might be more closely aligned with another team. In that case, we would pivot you to another team rather than disqualify you from the¬†process.</p><h3>Interview Tips</h3><p>Interviewing can be intimidating and stressful! Being prepared can help you minimize stress and anxiety. Following are a few quick tips to help you¬†prepare:</p><ul><li>Review your profile and make connections between your experience and the job description.</li><li>Think about your past work experiences and prepare some examples of when you achieved something amazing, or had some tough challenges.</li><li>We recommend against interview coding practice puzzle-type exercises, as we don‚Äôt ask those types of questions. If you want to practice, focus on medium-difficulty real-world problems you might encounter in a software engineering role.</li><li>Be sure to have questions prepared to ask the interviewers. This is a conversation, not an inquisition!</li></ul><p>We are here to accommodate any accessibility needs you may have, to ensure that you‚Äôre set up for success during your interview. Let us know if you need any assistive technology or other accommodations ahead of time, and we‚Äôll be sure to work with you to get it set¬†up.</p><p>We want to see you at your best‚Ää‚Äî‚Ääwe are not trying to trick you or trip you up! Try to relax, remember to breathe, and be honest and curious. <strong><em>Remember, this is not just about whether Netflix thinks you are a fit for the role, it‚Äôs about you deciding that Netflix and the role are right for¬†you!</em></strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-3TCETA_eVvvl6ce" /></figure><h3>Yes, We Are¬†Hiring!</h3><p>Several of our backend engineering teams are searching for our next stunning colleagues. Some of the areas for which we are actively seeking backend engineers include Streaming &amp; Gaming Technologies, Product Innovation, Infrastructure, and Studio Technologies. If any of the high-level descriptions below are of interest to you and seem like a good match for your experience and career goals, we‚Äôd like to hear from you! Simply click on the job description link and submit your application through our jobs¬†site.</p><h3>Streaming &amp; Gaming Technologies</h3><p>(<a href="https://jobs.netflix.com/jobs/175726412">https://jobs.netflix.com/jobs/175726412</a>)</p><ul><li>You are a <strong>distributed systems engineer </strong>working on product backend systems that support streaming video and/or mobile &amp; cloud¬†games.</li><li>You‚Äôre passionate about <strong>resilience, scalability, availability, and observability</strong>. Passion for large data sets, APIs, access &amp; identity management, or delivering backend systems that enable mobile and cloud gaming is a big¬†plus.</li><li>Your work centers around <strong>architecting, building and operating fault-tolerant distributed systems at massive¬†scale</strong>.</li></ul><h3>Product Innovation</h3><p>(<a href="https://jobs.netflix.com/jobs/175728345">https://jobs.netflix.com/jobs/175728345</a>)</p><ul><li>You are a <strong>distributed systems engineer</strong> working on core backend services that support our user journeys in signup, subscription, search, personalization and messaging.</li><li>You‚Äôre passionate about working at the <strong>intersection of business, product and technology</strong> at large¬†scale.</li><li>Your work centers around <strong>building fault-tolerant backend systems and services</strong> that make a direct impact on users and the business.</li></ul><h3>Infrastructure</h3><p>(<a href="https://jobs.netflix.com/jobs/122163878">https://jobs.netflix.com/jobs/122163878</a>)</p><ul><li>You are a <strong>distributed systems engineer</strong> working on infrastructure and platforms that enable or amplify the work of other engineering teams or¬†systems.</li><li>You‚Äôre passionate about <strong>scalable and highly available complex distributed systems</strong> and have a deep understanding of how they operate and¬†fail.</li><li>Your work centers around <strong>raising levels of abstraction to improve development at scale</strong> and creating engineering efficiencies.</li></ul><h3>Studio Technologies</h3><p>(<a href="https://jobs.netflix.com/jobs/175745345">https://jobs.netflix.com/jobs/175745345</a>)</p><ul><li>You are a <strong>software engineer that builds products and services used by creative partners across the studio and external productions to produce and manage all of Netflix global content. </strong>Our products enable the entire workflow of content acquisition, production, promotion and financing from script to screen. We create innovative solutions that develop and manage entertainment at scale while helping entertain the world as members find joy in the shows and movies they¬†love.</li><li>You‚Äôre <strong>passionate about innovation, scalability, functionality, shipping high-value features </strong>quickly and are committed to delivering exceptional backend systems for our consumers. You‚Äôre humble, curious, and looking to deliver results with other stunning colleagues.</li><li>Your work centers around <strong>building products and services targeting creative partners producing/managing global¬†content.</strong></li></ul><h3>Conclusion</h3><p>Netflix has a Freedom &amp; Responsibility culture in which every Netflix employee has the freedom to do their best work and the responsibility to achieve excellence. We value strong judgment, communication, impact, curiosity, innovation, courage, passion, integrity, selflessness, inclusion, and diversity. For more information on the culture, see <a href="http://jobs.netflix.com/culture">http://jobs.netflix.com/culture</a>.</p><p><strong>Karen Casella</strong> is the Director of Engineering for Access &amp; Identity Management technologies for Netflix streaming and gaming products. Connect with Karen on <a href="http://www.linkedin.com/in/kcasella">LinkedIn</a> or¬†<a href="http://twitter.com/kcasella">Twitter</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=aceb26a83495" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/demystifying-interviewing-for-backend-engineers-netflix-aceb26a83495">Demystifying Interviewing for Backend Engineers @ Netflix</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://netflixtechblog.com/netflix-a-culture-of-learning-394bc7d0f94c?source=rss----2615bd06b42e---4">Netflix: A Culture of Learning</a> <span>Martin Tingley with Wenjing Zheng, Simon Ejdemyr, Stephanie Lane, Colin McFarland,
                     Mihir Tendulkar, and Travis&nbsp;BrooksThis is the last post in an overview series on experimentation
                     at Netflix. Need to catch up? Earlier posts covered the basics of A/B tests (Part
                     1 and Part 2 ), core statistical concepts (Part 3 and Part 4), how to build confidence
                     in a decision (Part 5), and the the role of Experimentation and A/B testing within
                     the larger Data Science and Engineering organization at Netflix (Part&nbsp;6).Earlier posts
                     in this series covered the why, what and how of A/B testing, all of which are necessary
                     to reap the benefits of experimentation for product development. But without a little
                     magic, these basics are still not&nbsp;enough.The secret sauce that turns the raw ingredients
                     of experimentation into supercharged product innovation is culture. There are never
                     any shortcuts when developing and growing culture, and fostering a culture of experimentation
                     is no exception. Building leadership buy-in for an approach to learning that emphasizes
                     A/B testing, building trust in the results of tests, and building the technical capabilities
                     to execute experiments at scale all take time‚Ää‚Äî‚Ääparticularly within an organization
                     that‚Äôs new to these ideas. But the pay-offs of using experimentation and the virtuous
                     cycle of product development via the scientific method are well worth the effort.
                     Our colleagues at Microsoft have shared thoughtful publications on how to Kickstart
                     the Experimentation Flywheel and build a culture of experimentation, while their ‚ÄúCrawl,
                     Walk, Run, Fly‚Äù model is a great tool for assessing the maturity of an experimentation
                     practice.At Netflix, we‚Äôve been leveraging experimentation and the scientific method
                     for decades, and are fortunate to have a mature experimentation culture. There is
                     broad buy-in across the company, including from the C-Suite, that, whenever possible,
                     results from A/B tests or other causal inference approaches are near-requirements
                     for decision making. We‚Äôve also invested in education programs to up-level company-wide
                     understanding of how we use A/B tests as a framework for product development. In fact,
                     most of the material from this blog series has been adapted from our internal Experimentation
                     101 and 201 classes, which are open to anyone at&nbsp;Netflix.Netflix is organized to&nbsp;learnAs
                     a company, Netflix is organized to emphasize the importance of learning from data,
                     including from A/B tests. Our Data and Insights organization has teams that partner
                     with all corners of the company to deliver a better experience to our members, from
                     understanding content preferences around the globe to delivering a seamless customer
                     support experience. We use qualitative and quantitative consumer research, analytics,
                     experimentation, predictive modeling, and other tools to develop a deep understanding
                     of our members. And we own the data pipelines that power everything from executive-oriented
                     dashboards to the personalization systems that help connect each Netflix member with
                     content that will spark joy for them. This data-driven mindset is ubiquitous at all
                     levels of the company, and the Data and Insights organization is represented at the
                     highest echelon of Netflix Leadership.As discussed in Part 6, there are experimentation
                     and causal inference focussed data scientists who collaborate with product innovation
                     teams across Netflix. These data scientists design and execute tests to support learning
                     agendas and contribute to decision making. By diving deep into the details of single
                     test results, looking for patterns across tests, and exploring other data sources,
                     these Netflix data scientists build up domain expertise about aspects of the Netflix
                     experience and become valued partners to product managers and engineering leaders.
                     Data scientists help shape the evolution of the Netflix product through opportunity
                     sizing and identifying areas ripe for innovation, and frequently propose hypotheses
                     that are subsequently tested.We‚Äôve also invested in a broad and flexible experimentation
                     platform that allows our experimentation program to scale with the ambitions of the
                     company to learn more and better serve Netflix members. Just as the Netflix product
                     itself has evolved over the years, our approach to developing technologies to support
                     experimentation at scale continues to evolve. In fact, we‚Äôve been working to improve
                     experimentation platform solutions at Netflix for more than 20 years‚Ää‚Äî‚Ääour first investments
                     in tooling to support A/B tests came way back in&nbsp;2001.Early experimentation tooling
                     developed by Stan Lanning at Netflix, in&nbsp;2001.Learning and experimentation are ubiquitous
                     across&nbsp;NetflixNetflix has a unique internal culture that reinforces the use of experimentation
                     and the scientific method as a means to deliver more joy to all of our current and
                     future members. As a company, we aim to be curious, and to truly and honestly understand
                     our members around the world, and how we can better entertain them. We are also open
                     minded, knowing that great ideas can come from unlikely sources. There‚Äôs no better
                     way to learn and make great decisions than to confirm or falsify ideas and hypotheses
                     using the power of rigorous testing. Openly and candidly sharing test results allows
                     everyone at Netflix to develop intuition about our members and ideas for how we can
                     deliver an ever better experience to them‚Ää‚Äî‚Ääand then the virtuous cycle starts&nbsp;again.In
                     fact, Netflix has so many tests running on the product at any given time that a member
                     may be simultaneously allocated to several tests. There is not one Netflix product:
                     at any given time, we are testing out a large number of product variants, always seeking
                     to learn more about how we can deliver more joy to our current members and attract
                     new members. Some tests, such as the Top 10 list, are easy for users to notice, while
                     others, such as changes to the personalization and search systems or how Netflix encodes
                     and delivers streaming video, are less&nbsp;obvious.At Netflix, we are not afraid to test
                     boldly, and to challenge fundamental or long-held assumptions. The Top 10 list is
                     a great example of both: it‚Äôs a large and noticeable change that surfaces a new type
                     of evidence on the Netflix product. Large tests like this can open up whole new areas
                     for innovation, and are actively socialized and debated within the company (see below).
                     On the other end of the spectrum, we also run tests on much smaller scales in order
                     to optimize every aspect of the product. A great example is the testing we do to find
                     just the right text copy for every aspect of the product. By the numbers, we run far
                     more of these smaller and less noticeable tests, and we invest in end-to-end infrastructure
                     that simplifies their execution, allowing product teams to rapidly go from hypothesis
                     to test to roll out of the winning experience. As an example, the Shakespeare project
                     provides an end-to-end solution for rapid text copy testing that integrates with the
                     centralized Netflix experimentation platform. More generally, we are always on the
                     lookout for new areas that can benefit from experimentation, or areas where additional
                     methodology or tooling can produce new or faster learnings.Debating tests and the
                     importance of&nbsp;humilityNetflix has mature operating mechanisms to debate, make, and
                     socialize product decisions. Netflix does not make decisions by committee or by seeking
                     consensus. Instead, for every significant decision there is a single ‚ÄúInformed Captain‚Äù
                     who is ultimately responsible for making a judgment call after digesting relevant
                     data and input from colleagues (including dissenting perspectives). Wherever possible,
                     A/B test results or causal inference studies are an expected input to this decision
                     making&nbsp;process.In fact, not only are test results expected for product decisions‚Ää‚Äî‚Ääit‚Äôs
                     expected that decisions on investment areas for innovation and testing, test plans
                     for major innovations, and results of major tests are all summarized in memos, socialized
                     broadly, and actively debated. The forums where these debates take place are broadly
                     accessible, ensuring a diverse set of viewpoints provide feedback on test designs
                     and results, and weigh in on decisions. Invites for these forums are open to anyone
                     who is interested, and the price of admission is reading the memo. Despite strong
                     executive attendance, there‚Äôs a notable lack of hierarchy in these forums, as we all
                     seek to be led by the&nbsp;data.Netflix data scientists are active and valued participants
                     in these forums. Data scientists are expected to speak for the data, both what can
                     and what cannot be concluded from experimental results, the pros and cons of different
                     experimental designs, and so forth. Although they are not informed captains on product
                     decisions, data scientists, as interpreters of the data, are active contributors to
                     key product decisions.Product evolution via experimentation can be a humbling experience.
                     At Netflix, we have experts in every discipline required to develop and evolve the
                     Netflix service (product managers, UI/UX designers, data scientists, engineers of
                     all types, experts in recommendation systems and streaming video optimization‚Ää‚Äî‚Ääthe
                     list goes on), who are constantly coming up with novel hypotheses for how to improve
                     Netflix. But only a small percentage of our ideas turn out to be winners in A/B tests.
                     That‚Äôs right: despite our broad expertise, our members let us know, through their
                     actions in A/B tests, that most of our ideas do not improve the service. We build
                     and test hundreds of product variants each year, but only a small percentage end up
                     in production and rolled out to the more than 200 million Netflix members around the&nbsp;world.The
                     low win rate in our experimentation program is both humbling and empowering. It‚Äôs
                     hard to maintain a big ego when anyone at the company can look at the data and see
                     all the big ideas and investments that have ultimately not panned out. But nothing
                     proves the value of decision making through experimentation like seeing ideas that
                     all the experts were bullish on voted down by member actions in A/B tests‚Ää‚Äî‚Ääand seeing
                     a minor tweak to a sign up flow turn out to be a massive revenue generator.At Netflix,
                     we do not view tests that do not produce winning experience as ‚Äúfailures.‚Äù When our
                     members vote down new product experiences with their actions, we still learn a lot
                     about their preferences, what works (and does not work!) for different member cohorts,
                     and where there may, or may not be, opportunities for innovation. Combining learnings
                     from tests in a given innovation area, such as the Mobile UI experience, helps us
                     paint a more complete picture of the types of experiences that do and do not resonate
                     with our members, leading to new hypotheses, new tests, and, ultimately, a more joyful
                     experience for our members. And as our member base continues to grow globally, and
                     as consumer preferences and expectations continue to evolve, we also revisit ideas
                     that were unsuccessful when originally tested. Sometimes there are signals from the
                     original analysis that suggest now is a better time for that idea, or that it will
                     provide value to some of our newer member&nbsp;cohorts.Because Netflix tests all ideas,
                     and because most ideas are not winners, our culture of experimentation democratizes
                     ideation. Product managers are always hungry for ideas, and are open to innovative
                     suggestions coming from anyone in the company, regardless of seniority or expertise.
                     After all, we‚Äôll test anything before rolling it out to the member base, and even
                     the experts have low success rates! We‚Äôve seen time and time again at Netflix that
                     product suggestions large and small that arise from engineers, data scientists, even
                     our executives, can result in unexpected wins.(Left) Very few of our ideas are winners.
                     (Right) Experimentation democratizes ideation. Because we test all ideas, and because
                     most do not win, there‚Äôs an openness to product ideas coming from all corners of the
                     business: anyone can raise their hand and make a suggestion.A culture of experimentation
                     allows more voices to contribute to ideation, and far, far more voices to help inform
                     decision making. It‚Äôs a way to get the best ideas from everyone working on the product,
                     and to ensure that the innovations that are rolled out are vetted and approved by&nbsp;members.A
                     better product for our members and an internal culture that is humble and values ideas
                     and evidence: experimentation is a win-win proposition for&nbsp;Netflix.Emerging research&nbsp;areasAlthough
                     Netflix has been running experiments for decades, we‚Äôve only scratched the surface
                     relative to what we want to learn and the capabilities we need to build to support
                     those learning ambitions. There are open challenges and opportunities across experimentation
                     and causal inference at Netflix: exploring and implementing new methodologies that
                     allow us to learn faster and better; developing software solutions that support research;
                     evolving our internal experimentation platform to better serve a growing user community
                     and ever increasing size and throughput of experiments. And there‚Äôs a continuous focus
                     on evolving and growing our experimentation culture through internal events and education
                     programs, as well as external contributions. Here are a few themes that are on our&nbsp;radar:Increasing
                     velocity: beyond fixed time horizon experimentation.This series has focused on fixed
                     time horizon tests: sample sizes, the proportion of traffic allocated to each treatment
                     experience, and the test duration are all fixed in advance. In principle, the data
                     are examined only once, at the conclusion of the test. This ensures that the false
                     positive rate (see Part 3) is not increased by peeking at the data numerous times.
                     In practice, we‚Äôd like to be able to call tests early, or to adapt how incoming traffic
                     is allocated as we learn incrementally about which treatments are successful and which
                     are not, in a way that preserves the statistical properties described earlier in this
                     series. To enable these benefits, Netflix is investing in sequential experimentation
                     that permits for valid decision making at any time, versus waiting until a fixed time
                     has passed. These methods are already being used to ensure safe deployment of Netflix
                     client applications. We are also investing in support for experimental designs that
                     adaptively allocate traffic throughout the test towards promising treatments. The
                     goal of both these efforts is the same: more rapid identification of experiences that
                     benefit&nbsp;members.Scaling support for quasi experimentation and causal inference.Netflix
                     has learned an enormous amount, and dramatically improved almost every aspect of the
                     product, using the classic online A/B tests, or randomized controlled trials, that
                     have been the focus of this series. But not every business question is amenable to
                     A/B testing, whether due to an inability to randomize at the individual level, or
                     due to factors, such as spillover effects, that may violate key assumptions for valid
                     causal inference. In these instances, we often rely on the rigorous evaluation of
                     quasi-experiments, where units are not assigned to a treatment or control condition
                     by a random process. But the term ‚Äúquasi-experimentation‚Äù itself covers a broad category
                     of experimental design and methodological approaches that differ between the myriad
                     academic backgrounds represented by the Netflix data science community. How can we
                     synthesize best practices across domains and scale our approach to enable more colleagues
                     to leverage quasi-experimentation?Our early successes in this space have been driven
                     by investments in knowledge sharing across business verticals, education, and enablement
                     via tooling. Because quasi-experiment use cases span many domains at Netflix, identifying
                     common patterns has been a powerful driver in developing shared libraries that scientists
                     can use to evaluate individual quasi-experiments. And to support our continued scale,
                     we‚Äôve built internal tooling that coalesces data retrieval, design evaluation, analysis,
                     and reproducible reporting, all with the goal to enable our scientists.We expect our
                     investments in research, tooling, and education for quasi-experiments to grow over
                     time. In success, we will enable both scientists and their cross functional partners
                     to learn more about how to deliver more joy to current and future Netflix&nbsp;members.Experimentation
                     Platform as a&nbsp;Product.We treat the Netflix Experimentation Platform as an internal
                     product, complete with its own product manager and innovation roadmap. We aim to provide
                     an end-to-end paved path for configuring, allocating, monitoring, reporting, storing
                     and analyzing A/B tests, focusing on experimentation use cases that are optimized
                     for simplicity and testing velocity. Our goal is to make experimentation a simple
                     and integrated part of the product lifecycle, with little effort required on the part
                     of engineers, data scientists, or PMs to create, analyze, and act on tests, with automation
                     available wherever the test owner wants&nbsp;it.However, if the platform‚Äôs default paths
                     don‚Äôt work for a specific use case, experimenters can leverage our democratized contribution
                     model, or reuse pieces of the platform, to build out their own solutions. As experimenters
                     innovate on the boundaries of what‚Äôs possible in measurement methodology, experimental
                     design, and automation, the Experimentation Platform team partners to commoditize
                     these innovations and make them available to the broader organization.Three core principles
                     guide product development for our experimentation platform:Complexities and nuances
                     of testing such as allocations and methodologies should, typically, be abstracted
                     away from the process of running a single test, with emphasis instead placed on opinionated
                     defaults that are sensible for a set of use cases or testing&nbsp;areas.Manual intervention
                     at specific steps in the test execution should, typically, be optional, with emphasis
                     instead on test owners being able to invest their attention where they feel it adds
                     value and leave other areas to automation.Designing, executing, reporting, deciding,
                     and learning are all different phases of the experiment lifecycle that have differing
                     needs and users, and each stage benefits from purpose built tooling for each&nbsp;use.ConclusionNetflix
                     has a strong culture of experimentation, and results from A/B tests, or other applications
                     of the scientific method, are generally expected to inform decisions about how to
                     improve our product and deliver more joy to members. To support the current and future
                     scale of experimentation required by the growing Netflix member base and the increasing
                     complexity of our business, Netflix has invested in culture, people, infrastructure,
                     and internal education to make A/B testing broadly accessible across the&nbsp;company.And
                     we are continuing to evolve our culture of learning and experimentation to deliver
                     more joy to Netflix members around the world. As our member base and business grows,
                     smaller differences between treatment and control experiences become materially important.
                     That‚Äôs also true for subsets of the population: with a growing member base, we can
                     become more targeted and look to deliver positive experiences to cohorts of users,
                     defined by geographical region, device type, etc. As our business grows and expands,
                     we are looking for new places that could benefit from experimentation, ways to run
                     more experiments and learn more with each, and ways to accelerate our experimentation
                     program while making experimentation accessible to more of our colleagues.But the
                     biggest opportunity is to deliver more joy to our members through the virtuous cycle
                     of experimentation.Interested in learning more? Explore our research&nbsp;site.Interested
                     in joining us? Explore our open&nbsp;roles.Netflix: A Culture of Learning was originally
                     published in Netflix TechBlog on Medium, where people are continuing the conversation
                     by highlighting and responding to this story.</span></summary><time datetime="2022-01-25T18:23:45+02:00">Tue, 25 Jan 2022 16:23</time><article><p><a href="https://www.linkedin.com/in/martintingley/"><em>Martin Tingley</em></a><em> with </em><a href="https://www.linkedin.com/in/wenjing-zheng/"><em>Wenjing Zheng</em></a><em>, </em><a href="https://www.linkedin.com/in/simon-ejdemyr-22b920123/"><em>Simon Ejdemyr</em></a><em>, </em><a href="https://www.linkedin.com/in/stephanielane1/"><em>Stephanie Lane</em></a><em>, </em><a href="https://www.linkedin.com/in/mcfrl/"><em>Colin McFarland</em></a><em>, </em><a href="https://www.linkedin.com/in/tendulkar/"><em>Mihir Tendulkar</em></a><em>, and </em><a href="https://www.linkedin.com/in/traviscb1998/"><em>Travis¬†Brooks</em></a></p><blockquote>This is the last post in an overview series on experimentation at Netflix. Need to catch up? Earlier posts covered the basics of A/B tests (<a href="https://netflixtechblog.com/decision-making-at-netflix-33065fa06481">Part 1</a> and <a href="https://netflixtechblog.com/what-is-an-a-b-test-b08cc1b57962">Part 2</a> ), core statistical concepts (<a href="https://netflixtechblog.com/interpreting-a-b-test-results-false-positives-and-statistical-significance-c1522d0db27a">Part 3</a> and <a href="https://netflixtechblog.com/interpreting-a-b-test-results-false-negatives-and-power-6943995cf3a8">Part 4</a>), how to build confidence in a decision (<a href="https://netflixtechblog.com/building-confidence-in-a-decision-8705834e6fd8">Part 5</a>), and the the role of Experimentation and A/B testing within the larger Data Science and Engineering organization at Netflix (<a href="https://netflixtechblog.com/experimentation-is-a-major-focus-of-data-science-across-netflix-f67923f8e985">Part¬†6</a>).</blockquote><p>Earlier posts in this series covered the why, what and how of A/B testing, all of which are necessary to reap the benefits of experimentation for product development. But without a little magic, these basics are still not¬†enough.</p><p>The secret sauce that turns the raw ingredients of experimentation into supercharged product innovation is <a href="https://jobs.netflix.com/culture">culture</a>. There are never any shortcuts when developing and growing culture, and fostering a culture of experimentation is no exception. Building leadership buy-in for an approach to learning that emphasizes A/B testing, building trust in the results of tests, and building the technical capabilities to execute experiments at scale all take time‚Ää‚Äî‚Ääparticularly within an organization that‚Äôs new to these ideas. But the pay-offs of using experimentation and the virtuous cycle of product development via the scientific method are well worth the effort. Our colleagues at <a href="https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/">Microsoft</a> have shared thoughtful publications on how to <a href="https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/it-takes-a-flywheel-to-fly-kickstarting-and-keeping-the-a-b-testing-momentum/">Kickstart the Experimentation Flywheel</a> and build a culture of experimentation, while their ‚Äú<a href="https://exp-platform.com/Documents/2017-05%20ICSE2017_EvolutionOfExP.pdf">Crawl, Walk, Run, Fly</a>‚Äù model is a great tool for assessing the maturity of an experimentation practice.</p><p>At Netflix, we‚Äôve been leveraging experimentation and the scientific method for decades, and are fortunate to have a mature experimentation culture. There is broad buy-in across the company, including from the C-Suite, that, whenever possible, results from A/B tests or other causal inference approaches are near-requirements for decision making. We‚Äôve also invested in education programs to up-level company-wide understanding of how we use A/B tests as a framework for product development. In fact, most of the material from this blog series has been adapted from our internal Experimentation 101 and 201 classes, which are open to anyone at¬†Netflix.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9RDYcxBOXJ0o04Xb" /></figure><h4>Netflix is organized to¬†learn</h4><p>As a company, Netflix is organized to emphasize the importance of learning from data, including from A/B tests. Our Data and Insights organization has teams that partner with all corners of the company to deliver a better experience to our members, from understanding content preferences around the globe to delivering a seamless customer support experience. We use qualitative and quantitative consumer research, analytics, experimentation, predictive modeling, and other tools to develop a deep understanding of our members. And we own the data pipelines that power everything from executive-oriented dashboards to the personalization systems that help connect each Netflix member with content that will spark joy for them. This data-driven mindset is ubiquitous at all levels of the company, and the Data and Insights organization is represented at the highest echelon of <a href="https://about.netflix.com/en/about-us">Netflix Leadership</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4yLRW1wClPf3pvLo" /></figure><p>As discussed in <a href="https://netflixtechblog.com/experimentation-is-a-major-focus-of-data-science-across-netflix-f67923f8e985">Part 6</a>, there are experimentation and causal inference focussed data scientists who collaborate with product innovation teams across Netflix. These data scientists design and execute tests to support learning agendas and contribute to decision making. By diving deep into the details of single test results, looking for patterns across tests, and exploring other data sources, these Netflix data scientists build up domain expertise about aspects of the Netflix experience and become valued partners to product managers and engineering leaders. Data scientists help shape the evolution of the Netflix product through opportunity sizing and identifying areas ripe for innovation, and frequently propose hypotheses that are subsequently tested.</p><p>We‚Äôve also invested in a broad and flexible experimentation platform that allows our experimentation program to scale with the ambitions of the company to learn more and better serve Netflix members. Just as the Netflix product itself has evolved over the years, our approach to developing technologies to support experimentation at scale continues to evolve. In fact, we‚Äôve been working to improve experimentation platform solutions at Netflix for more than 20 years‚Ää‚Äî‚Ääour first investments in tooling to support A/B tests came way back in¬†2001.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*01bdYWp3wc1lD5UK" /><figcaption>Early experimentation tooling developed by Stan Lanning at Netflix, in¬†2001.</figcaption></figure><h4>Learning and experimentation are ubiquitous across¬†Netflix</h4><p>Netflix has a unique internal <a href="https://jobs.netflix.com/culture">culture</a> that reinforces the use of experimentation and the scientific method as a means to deliver more joy to all of our current and future members. As a company, we aim to be curious, and to truly and honestly understand our members around the world, and how we can better entertain them. We are also open minded, knowing that great ideas can come from unlikely sources. There‚Äôs no better way to learn and make great decisions than to confirm or falsify ideas and hypotheses using the power of rigorous testing. Openly and candidly sharing test results allows everyone at Netflix to develop intuition about our members and ideas for how we can deliver an ever better experience to them‚Ää‚Äî‚Ääand then the virtuous cycle starts¬†again.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/593/0*f5PNP97wX4Ve3iva" /></figure><p>In fact, Netflix has so many tests running on the product at any given time that a member may be simultaneously allocated to several tests. There is not one Netflix product: at any given time, we are testing out a large number of product variants, always seeking to learn more about how we can deliver more joy to our current members and attract new members. Some tests, such as the Top 10 list, are easy for users to notice, while others, such as changes to the <a href="https://research.netflix.com/business-area/personalization-and-search">personalization and search systems</a> or how Netflix <a href="https://research.netflix.com/business-area/streaming">encodes and delivers streaming video</a>, are less¬†obvious.</p><p>At Netflix, we are not afraid to test boldly, and to challenge fundamental or long-held assumptions. The Top 10 list is a great example of both: it‚Äôs a large and noticeable change that surfaces a new type of evidence on the Netflix product. Large tests like this can open up whole new areas for innovation, and are actively socialized and debated within the company (see below). On the other end of the spectrum, we also run tests on much smaller scales in order to optimize every aspect of the product. A great example is the testing we do to find just the right text copy for every aspect of the product. By the numbers, we run far more of these smaller and less noticeable tests, and we invest in end-to-end infrastructure that simplifies their execution, allowing product teams to rapidly go from hypothesis to test to roll out of the winning experience. As an example, the <a href="https://netflixtechblog.medium.com/words-matter-testing-copy-with-shakespeare-5df48b38158a">Shakespeare project</a> provides an end-to-end solution for rapid text copy testing that integrates with the centralized Netflix experimentation platform. More generally, we are always on the lookout for new areas that can benefit from experimentation, or areas where additional methodology or tooling can produce new or faster learnings.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*jcFgZaWzxOd7rfaZ" /></figure><h4>Debating tests and the importance of¬†humility</h4><p>Netflix has mature operating mechanisms to debate, make, and socialize product decisions. Netflix does not make decisions by committee or by seeking consensus. Instead, for every significant decision there is a single ‚Äú<a href="https://jobs.netflix.com/culture">Informed Captain</a>‚Äù who is ultimately responsible for making a judgment call after digesting relevant data and input from colleagues (including dissenting perspectives). Wherever possible, A/B test results or causal inference studies are an expected input to this decision making¬†process.</p><p>In fact, not only are test results expected for product decisions‚Ää‚Äî‚Ääit‚Äôs expected that decisions on investment areas for innovation and testing, test plans for major innovations, and results of major tests are all summarized in memos, socialized broadly, and actively debated. The forums where these debates take place are broadly accessible, ensuring a diverse set of viewpoints provide feedback on test designs and results, and weigh in on decisions. Invites for these forums are open to anyone who is interested, and the price of admission is reading the memo. Despite strong executive attendance, there‚Äôs a notable lack of hierarchy in these forums, as we all seek to be led by the¬†data.</p><p>Netflix data scientists are active and valued participants in these forums. Data scientists are expected to speak for the data, both what can and what cannot be concluded from experimental results, the pros and cons of different experimental designs, and so forth. Although they are not informed captains on product decisions, data scientists, as interpreters of the data, are active contributors to key product decisions.</p><p>Product evolution via experimentation can be a humbling experience. At Netflix, we have experts in every discipline required to develop and evolve the Netflix service (product managers, UI/UX designers, data scientists, engineers of all types, experts in recommendation systems and streaming video optimization‚Ää‚Äî‚Ääthe list goes on), who are constantly coming up with novel hypotheses for how to improve Netflix. But only a small percentage of our ideas turn out to be winners in A/B tests. That‚Äôs right: despite our broad expertise, our members let us know, through their actions in A/B tests, that most of our ideas do not improve the service. We build and test hundreds of product variants each year, but only a small percentage end up in production and rolled out to the more than 200 million Netflix members around the¬†world.</p><p>The low win rate in our experimentation program is both humbling and empowering. It‚Äôs hard to maintain a big ego when anyone at the company can look at the data and see all the big ideas and investments that have ultimately not panned out. But nothing proves the value of decision making through experimentation like seeing ideas that all the experts were bullish on voted down by member actions in A/B tests‚Ää‚Äî‚Ääand seeing a minor tweak to a sign up flow turn out to be a massive revenue generator.</p><p>At Netflix, we do not view tests that do not produce winning experience as ‚Äúfailures.‚Äù When our members vote down new product experiences with their actions, we still learn a lot about their preferences, what works (and does not work!) for different member cohorts, and where there may, or may not be, opportunities for innovation. Combining learnings from tests in a given innovation area, such as the Mobile UI experience, helps us paint a more complete picture of the types of experiences that do and do not resonate with our members, leading to new hypotheses, new tests, and, ultimately, a more joyful experience for our members. And as our member base continues to grow globally, and as consumer preferences and expectations continue to evolve, we also revisit ideas that were unsuccessful when originally tested. Sometimes there are signals from the original analysis that suggest now is a better time for that idea, or that it will provide value to some of our newer member¬†cohorts.</p><p>Because Netflix tests all ideas, and because most ideas are not winners, our culture of experimentation democratizes ideation. Product managers are always hungry for ideas, and are open to innovative suggestions coming from anyone in the company, regardless of seniority or expertise. After all, we‚Äôll test anything before rolling it out to the member base, and even the experts have low success rates! We‚Äôve seen time and time again at Netflix that product suggestions large and small that arise from engineers, data scientists, even our executives, can result in unexpected wins.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*TTvsbBXmg38pxteb" /><figcaption>(Left) Very few of our ideas are winners. (Right) Experimentation democratizes ideation. Because we test all ideas, and because most do not win, there‚Äôs an openness to product ideas coming from all corners of the business: anyone can raise their hand and make a suggestion.</figcaption></figure><p>A culture of experimentation allows more voices to contribute to ideation, and far, far more voices to help inform decision making. It‚Äôs a way to get the best ideas from everyone working on the product, and to ensure that the innovations that are rolled out are vetted and approved by¬†members.</p><p>A better product for our members and an internal culture that is humble and values ideas and evidence: experimentation is a win-win proposition for¬†Netflix.</p><h4>Emerging research¬†areas</h4><p>Although Netflix has been running experiments for decades, we‚Äôve only scratched the surface relative to what we want to learn and the capabilities we need to build to support those learning ambitions. There are open challenges and opportunities across experimentation and causal inference at Netflix: exploring and implementing new methodologies that allow us to learn faster and better; developing software solutions that support research; evolving our internal experimentation platform to better serve a growing user community and ever increasing size and throughput of experiments. And there‚Äôs a continuous focus on evolving and growing our experimentation culture through internal events and education programs, as well as external contributions. Here are a few themes that are on our¬†radar:</p><p><strong>Increasing velocity: beyond fixed time horizon experimentation.</strong></p><p>This series has focused on fixed time horizon tests: sample sizes, the proportion of traffic allocated to each treatment experience, and the test duration are all fixed in advance. In principle, the data are examined only once, at the conclusion of the test. This ensures that the false positive rate (see <a href="https://netflixtechblog.com/interpreting-a-b-test-results-false-positives-and-statistical-significance-c1522d0db27a">Part 3</a>) is not increased by <a href="http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1517.pdf">peeking at the data numerous times</a>. In practice, we‚Äôd like to be able to call tests early, or to adapt how incoming traffic is allocated as we learn incrementally about which treatments are successful and which are not, in a way that preserves the statistical properties described earlier in this series. To enable these benefits, Netflix is investing in sequential experimentation that permits for valid decision making at any time, versus waiting until a fixed time has passed. These methods are already being used to ensure <a href="https://netflixtechblog.com/safe-updates-of-client-applications-at-netflix-1d01c71a930c">safe deployment of Netflix client applications</a>. We are also investing in support for experimental designs that adaptively allocate traffic throughout the test towards promising treatments. The goal of both these efforts is the same: more rapid identification of experiences that benefit¬†members.</p><p><strong>Scaling support for quasi experimentation and causal inference.</strong></p><p>Netflix has learned an enormous amount, and dramatically improved almost every aspect of the product, using the classic online A/B tests, or randomized controlled trials, that have been the focus of this series. But not every business question is amenable to A/B testing, whether due to an inability to randomize at the individual level, or due to factors, such as <a href="https://en.wikipedia.org/wiki/Spillover_(experiment)">spillover effects</a>, that may violate key assumptions for valid causal inference. In these instances, we often rely on the rigorous evaluation of quasi-experiments, where units are not assigned to a treatment or control condition by a random process. But the term ‚Äúquasi-experimentation‚Äù itself covers a broad category of experimental design and methodological approaches that differ between the myriad academic backgrounds represented by the Netflix data science community. How can we synthesize best practices across domains and scale our approach to enable more colleagues to leverage quasi-experimentation?</p><p>Our early successes in this space have been driven by investments in knowledge sharing across business verticals, education, and enablement via tooling. Because quasi-experiment use cases span many domains at Netflix, identifying common patterns has been a powerful driver in developing shared libraries that scientists can use to evaluate individual quasi-experiments. And to support our continued scale, we‚Äôve built internal tooling that coalesces data retrieval, design evaluation, analysis, and reproducible reporting, all with the goal to enable our scientists.</p><p>We expect our investments in research, tooling, and education for quasi-experiments to grow over time. In success, we will enable both scientists and their cross functional partners to learn more about how to deliver more joy to current and future Netflix¬†members.</p><p><strong>Experimentation Platform as a¬†Product</strong>.</p><p>We treat the Netflix Experimentation Platform as an internal product, complete with its own product manager and innovation roadmap. We aim to provide an end-to-end paved path for configuring, allocating, monitoring, reporting, storing and analyzing A/B tests, focusing on experimentation use cases that are optimized for simplicity and testing velocity. Our goal is to make experimentation a simple and integrated part of the product lifecycle, with little effort required on the part of engineers, data scientists, or PMs to create, analyze, and act on tests, with automation available wherever the test owner wants¬†it.</p><p>However, if the platform‚Äôs default paths don‚Äôt work for a specific use case, experimenters can leverage our <a href="https://netflixtechblog.com/reimagining-experimentation-analysis-at-netflix-71356393af21">democratized contribution model</a>, or reuse pieces of the platform, to build out their own solutions. As experimenters innovate on the boundaries of what‚Äôs possible in measurement methodology, experimental design, and automation, the Experimentation Platform team partners to commoditize these innovations and make them available to the broader organization.</p><p>Three core principles guide product development for our experimentation platform:</p><ul><li>Complexities and nuances of testing such as allocations and methodologies should, typically, be abstracted away from the process of running a single test, with emphasis instead placed on <strong>opinionated defaults that are sensible for a set of use cases or testing¬†areas.</strong></li><li>Manual intervention at specific steps in the test execution should, typically, be optional, with emphasis instead on test owners being able to<strong> invest their attention where they feel it adds value and leave other areas to automation</strong>.</li><li>Designing, executing, reporting, deciding, and learning are all different phases of the experiment lifecycle that have differing needs and users, and each stage benefits from <strong>purpose built tooling for each¬†use</strong>.</li></ul><h4>Conclusion</h4><p>Netflix has a strong culture of experimentation, and results from A/B tests, or other applications of the scientific method, are generally expected to inform decisions about how to improve our product and deliver more joy to members. To support the current and future scale of experimentation required by the growing Netflix member base and the increasing complexity of our business, Netflix has invested in culture, people, infrastructure, and internal education to make A/B testing broadly accessible across the¬†company.</p><p>And we are continuing to evolve our culture of learning and experimentation to deliver more joy to Netflix members around the world. As our member base and business grows, smaller differences between treatment and control experiences become materially important. That‚Äôs also true for subsets of the population: with a growing member base, we can become more targeted and look to deliver positive experiences to cohorts of users, defined by geographical region, device type, etc. As our business grows and expands, we are looking for new places that could benefit from experimentation, ways to run more experiments and learn more with each, and ways to accelerate our experimentation program while making experimentation accessible to more of our colleagues.</p><p>But the biggest opportunity is to deliver more joy to our members through the virtuous cycle of experimentation.</p><p>Interested in learning more? Explore our <a href="http://research.netflix.com/">research¬†site</a>.</p><p>Interested in joining us? Explore our <a href="https://jobs.netflix.com/">open¬†roles</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=394bc7d0f94c" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/netflix-a-culture-of-learning-394bc7d0f94c">Netflix: A Culture of Learning</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://netflixtechblog.com/fixing-performance-regressions-before-they-happen-eab2602b86fe?source=rss----2615bd06b42e---4">Fixing Performance Regressions Before they Happen</a> <span>Angus CrollNetflix is used by 222 million members and runs on over 1700 device types
                     ranging from state-of-the-art smart TVs to low-cost mobile&nbsp;devices.At Netflix we‚Äôre
                     proud of our reliability and we want to keep it that way. To that end, it‚Äôs important
                     that we prevent significant performance regressions from reaching the production app.
                     Sluggish scrolling or late rendering is frustrating and triggers accidental navigations.
                     Choppy playback makes watching a show less enjoyable. Any performance regression that
                     makes it into a product release will degrade user experience, so the challenge is
                     to detect and fix such regressions before they&nbsp;ship.This post describes how the Netflix
                     TVUI team implemented a robust strategy to quickly and easily detect performance anomalies
                     before they are released‚Ää‚Äî‚Ääand often before they are even committed to the codebase.What
                     do we mean by Performance?Technically, ‚Äúperformance‚Äù metrics are those relating to
                     the responsiveness or latency of the app, including start up&nbsp;time.But TV devices also
                     tend to be more memory constrained than other devices, and as such are more liable
                     to crash during a memory spike‚Ää‚Äî‚Ääso for Netflix TV we actually care about memory at
                     least as much as performance, maybe more&nbsp;so.At Netflix the term ‚Äúperformance‚Äù usually
                     encompasses both performance metrics (in the strict meaning) and memory metrics, and
                     that‚Äôs how we‚Äôre using the term&nbsp;here.Why do we run Performance Tests on&nbsp;commits?It‚Äôs
                     harder to reason about the performance profile of pre-production code since we can‚Äôt
                     gather real-time metrics for code that hasn‚Äôt yet shipped. We do cut a canary release
                     in advance of shipment which is dogfooded by Netflix employees and subject to the
                     same metrics collection as the production release. While the canary release is a useful
                     dry-run for pending shipments, it sometimes misses regressions because the canary
                     user base is a fraction of the production release. And in the event that regressions
                     are detected in the canary, it still necessitates an often messy and time consuming
                     revert or&nbsp;patch.By running performance tests against every commit (pre- and post-merge),
                     we can detect potentially regressive commits earlier. The sooner we detect such commits
                     the fewer subsequent builds are affected and the easier it is to revert. Ideally we
                     catch regressions before they even reach the main&nbsp;branch.What are the Performance
                     Tests?The goal of our TVUI Performance Tests is to gather memory and responsiveness
                     metrics while simulating the full range of member interactions with Netflix&nbsp;TV.There
                     are roughly 50 performance tests, each one designed to reproduce an aspect of member
                     engagement. The goal is to keep each test brief and focused on a specific, isolated
                     piece of functionality (startup, profile switching, scrolling through titles, selecting
                     an episode, playback etc.), while the test suite as a whole should cover the entire
                     member experience with minimal duplication. In this way we can run multiple tests
                     in parallel and the absence of long pole tests keeps the overall test time manageable
                     and allows for repeat test runs. Every test runs on a combination of devices (physical
                     and virtual) and platform versions (SDKs). We‚Äôll refer to each unique test/device/SDK
                     combination as a test variation.We run the full performance suite twice per Pull Request&nbsp;(PR):when
                     the PR is first submittedwhen the PR is merged to the destination branchMeasurementEach
                     performance test tracks either memory or responsiveness. Both of these metrics will
                     fluctuate over the course of a test, so we post metric values at regular intervals
                     throughout the test. To compare test runs we need a method to consolidate this range
                     of observed values into a single&nbsp;value.We made the following decisions:Memory Tests:
                     use the maximum memory value observed during the test run (because that‚Äôs the value
                     that determines whether a device could&nbsp;crash).Responsiveness Tests&nbsp;: use the median
                     value observed during the test run (based on the assumption that perceived slowness
                     is influenced by all responses, not just the worst response).What are the Challenges?When
                     Netflix is running in production, we capture real-time performance data which makes
                     it relatively easy to make assertions about the app‚Äôs performance. It‚Äôs much harder
                     to assess the performance of pre-production code (changes merged to the main branch
                     but not yet released) and harder still to get a performance signal for unmerged code
                     in a PR. Performance test metrics are inferior to real-time usage metrics for several&nbsp;reasons:Data
                     volume: In the Netflix app, the same steps are repeated billions of times, but developer
                     velocity and resource constraints dictate that performance tests can only run a handful
                     of times per&nbsp;build.Simulation: No matter how rigorous or creative our testing process
                     is, we can only ever approximate the experience of real life users, never replicate
                     it. Real users regularly use Netflix for hours at a time, and every user has different
                     preferences and&nbsp;habits.Noise: Ideally a given codebase running any given test variation
                     will always return identical results. In reality that just never happens: no two device
                     CPUs are identical, garbage collection is not entirely predictable, API request volume
                     and backend activity is variable‚Ää‚Äî‚Ääso are power levels and network bandwidth. For
                     every test there will be background noise that we need to somehow filter from our
                     analysis.Initial Approach: Static ThresholdsFor our first attempt at performance validation
                     we assigned maximum acceptable threshold values for memory metrics. There was a sound
                     rationale behind this approach‚Ää‚Äî‚Ääwhen a TV runs Netflix there is a hard limit for
                     memory footprint beyond which Netflix has the potential to&nbsp;crash.There were several
                     issues with the static thresholds approach:Custom preparation work per test: Since
                     each test variation has a unique memory profile, the appropriate static threshold
                     had to be researched and assigned on a case-by-case basis. This was difficult and
                     time consuming, so we only assigned thresholds to about 30% of test variations.Lack
                     of context: As a validation technique, static thresholds proved to be somewhat arbitrary.
                     Imagine a commit that increases memory usage by 10% but to a level which is just below
                     the threshold. The next commit might be a README change (zero memory impact) but due
                     to normal variations in device background noise, the metric could increase by just
                     enough to breach the threshold.Background variance is not filtered: Once the codebase
                     is bumping against the memory threshold, background device noise becomes the principal
                     factor determining which side of the threshold line the test result&nbsp;falls.Unreliable
                     regression signals with static Threshold techniquePost-alert adjustments: We found
                     ourselves repeatedly increasing the thresholds to move them clear of background noiseThe
                     Pivot: Anomaly and Changepoint DetectionIt became apparent we needed a technique for
                     performance validation that:Removes failure bias by giving equal weight to all test
                     runs, regardless of&nbsp;resultsDoesn‚Äôt treat performance data points in isolation, but
                     instead assesses the performance impact of a build in relation to previous&nbsp;builds.Can
                     be automatically applied to every test without the need for pre-hoc research, data
                     entry or ongoing manual interventionCould be equally applied to test data of any type:
                     memory, responsiveness, or any other non-boolean test&nbsp;dataMinimizes the impact of
                     background noise by prioritizing variance over absolute&nbsp;valuesImproves insight by
                     examining data points both at the time of creation and retroactivelyWe settled on
                     a two-pronged approach:Anomaly Detection immediately calls out potential performance
                     regressions by comparing with recent past&nbsp;dataChangepoint Detection identifies more
                     subtle performance inflections by examining past and future data&nbsp;clustersAnomaly DetectionWe
                     define an anomaly as any metric data point that is more than n standard deviations
                     above the recent mean, where recent mean and standard deviation are derived from the
                     previous m test runs. For Netflix TV performance tests we currently set n to 4 and
                     m to 40 but these values can be tweaked to maximize signal to noise ratio. When an
                     anomaly is detected the test status is set to failed and an alert is generated.Anomaly
                     detection works because thresholds are dynamic and derived from existing data. If
                     the data exhibits a lot of background variance then the anomaly threshold will increase
                     to account for the extra&nbsp;noise.ChangepointsChangepoints are data points at the boundary
                     of two distinct data distribution patterns. We use a technique called e-divisive to
                     analyze the 100 most recent test runs, using a Python implementation based on this
                     implementation.Since we‚Äôre only interested in performance regressions, we ignore changepoints
                     that trend lower. When a changepoint is detected for a test, we don‚Äôt fail the test
                     or generate an alert (we consider changepoints to be warnings of unusual patterns,
                     not full blown error assertions).As you can see, changepoints are a more subtle signal.
                     They don‚Äôt necessarily indicate a regression but they suggest builds that had an impact
                     on subsequent data distribution.Builds that generate changepoints across multiple
                     tests, warrant further investigation before they can be included in the release candidate.Changepoints
                     give us more confidence in regression detection because they disregard false positives
                     such as one time data spikes. Because changepoint detection requires after-the-fact
                     data, they are best suited to identifying potentially regressive code that is already
                     in the main branch but has not yet been&nbsp;shipped.Additional AdjustmentsRuns per&nbsp;TestTo
                     address failure bias, we decided to run all tests 3 times, regardless of the result.
                     We chose 3 iterations to provide enough data to eliminate most device noise (tests
                     are allocated to devices randomly) without creating a productivity bottleneck.Summarizing
                     across Test&nbsp;RunsNext we needed to decide on a methodology to compress the results
                     of each batch of 3 runs into a single value. The goal was to ignore outlier results
                     caused by erratic device behavior.Initially we took the average of those three runs,
                     but that led to an excess of false positives because the most irregular test runs
                     exerted too much influence on the result. Switching to the median eliminated some
                     of these false positives but we were still getting an unacceptable number of excess
                     alerts (because during periods of high device noise we would occasionally see outlier
                     results two times out of three). Finally, since we noticed that outlier results tended
                     to be higher than normal‚Ää‚Äî‚Äärarely lower‚Ää‚Äî‚Ääwe settled on using the minimum value across
                     the 3 runs and this proved to be the most effective at eliminating external&nbsp;noise.All
                     data points (3 runs per&nbsp;build)Selecting median value per&nbsp;buildSelecting minimum value
                     per&nbsp;buildWhat were the&nbsp;Results?After switching our performance validation to use anomaly
                     and changepoint detection we noticed several improvements.a) We are alerted for potential
                     performance regressions far less often, and when we do get alerted it‚Äôs much more
                     likely to indicate a genuine regression. Our workload is further reduced by no longer
                     having to manually increment static performance thresholds after each false positive.The
                     following table represents the alert summary for two distinct months last year. In
                     March 2021 we still used static thresholds for regression alerts. By October 2021
                     we had switched using anomaly detection for regression alerts. Alerts which were true
                     regressions is the number of alerted commits for which the suspected regression turned
                     out to be both significant and persistent.Note that since the March tests only validated
                     when a threshold was manually set, the total number of validating test runs in October
                     was much greater, and yet we still got only 10% of the&nbsp;alerts.b) We are not alerted
                     for subsequent innocuous builds that inherit regressive commits from preceding builds.
                     (Using the static threshold technique, all subsequent builds were alerted until the
                     regressive build was reverted.) This is because regressive builds increase both mean
                     and standard deviation and thus put subsequent non-regressing builds comfortably below
                     the alert threshold.Regressive build is above alert thresholdSubsequent build is easily
                     below alert thresholdc) Performance tests against PRs, which had been almost constantly
                     red (because the probability of at least one static threshold being breached was always
                     high), are now mostly green. When the performance tests are red we have a much higher
                     confidence that there is a genuine performance regression.d) Displaying the anomaly
                     and changepoint count per build provides a visual snapshot that quickly highlights
                     potentially problematic builds.What‚Äôs Next?Further WorkThere are still several things
                     we‚Äôd like to&nbsp;improveMake it easier to determine if regressions were due to external
                     agents: Often it turns out the detected regression, though real, was not a result
                     of the committed code but due to an external factor such as an upgrade to one of our
                     platform dependencies, or a feature flag that got switched on. It would be helpful
                     to summarize external changes in our alert summaries.Factor out resolved regressions
                     when determining baselines for validation: When generating recent mean and standard
                     deviation values, we could improve regression detection by filtering out data from
                     erstwhile regressions that have since been&nbsp;fixed.Improve Developer Velocity: We can
                     further reduce total test time by removing unnecessary iterations within tests, adding
                     more devices to ensure availability, and de-emphasizing testing for those parts of
                     the app where performance is less likely to be critical. We can also pre-build app
                     bundles (at least partially) so that the test suite is not delayed by waiting for
                     fresh&nbsp;builds.More closely mirror metrics gathered by the production app: In the deployed
                     Netflix TV app we collect additional metrics such as TTR (time to render) and empty
                     box rate (how frequently titles in the viewport are missing images). While test metrics
                     and metrics collected during real use do not lend themselves to direct comparison,
                     measuring the relative change in metrics in pre-production builds can help us to anticipate
                     regressions in production.Wider Adoption and New Use&nbsp;CasesAt this point Anomaly and
                     Changepoint detection is applied to every commit in the TVUI repo, and is in the process
                     of being deployed for commits to the TV Player repo (the layer that manages playback
                     operations). Other Netflix teams (outside of the TV platform) have also expressed
                     interest in these techniques and the ultimate goal is to standardize regression detection
                     across&nbsp;Netflix.Anomaly and changepoint detection are entirely framework independent‚Ää‚Äî‚Ääthe
                     only required inputs are a current value and an array of recent values to compare
                     it to. As such, their utility extends far beyond performance tests. For example, we
                     are considering using these techniques to monitor the reliability of non-performance-based
                     test suites‚Ää‚Äî‚Ääin this case the metric of interest is the percent of tests that ran
                     to completion.In the future we plan to decouple anomaly and changepoint logic from
                     our test infrastructure and offer it as a standalone open-source library.Wrap UpBy
                     using techniques that assess the performance impact of a build in relation to the
                     performance characteristics (magnitude, variance, trend) of adjacent builds, we can
                     more confidently distinguish genuine regressions from metrics that are elevated for
                     other reasons (e.g. inherited code, regressions in previous builds or one-off data
                     spikes due to test irregularities). We also spend less time chasing false negatives
                     and no longer need to manually assign a threshold to each result‚Ää‚Äî‚Ääthe data itself
                     now sets the thresholds dynamically.This improved efficiency and higher confidence
                     level helps us to quickly identify and fix regressions before they reach our&nbsp;members.The
                     anomaly and changepoint techniques discussed here can be used to identify regressions
                     (or progressions), unexpected values or inflection points in any chronologically sequenced,
                     quantitative data. Their utility extends well beyond performance analysis. For example
                     they could be used to identify inflection points in system reliability, customer satisfaction,
                     product usage, download volume or&nbsp;revenue.We encourage you to try these techniques
                     on your own data. We‚Äôd love to learn more about their success (or otherwise) in other
                     contexts!Fixing Performance Regressions Before they Happen was originally published
                     in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting
                     and responding to this story.</span></summary><time datetime="2022-01-24T23:49:43+02:00">Mon, 24 Jan 2022 21:49</time><article><p><a href="https://twitter.com/angustweets"><em>Angus Croll</em></a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*admHu1DuoaSrYF3Sx3_n9w.png" /></figure><p>Netflix is used by 222 million members and runs on over 1700 device types ranging from state-of-the-art smart TVs to low-cost mobile¬†devices.</p><p>At Netflix we‚Äôre <a href="https://www.theverge.com/22787426/netflix-cdn-open-connect">proud of our reliability</a> and we want to keep it that way. To that end, it‚Äôs important that we prevent significant performance regressions from reaching the production app. Sluggish scrolling or late rendering is frustrating and triggers accidental navigations. Choppy playback makes watching a show less enjoyable. Any performance regression that makes it into a product release will degrade user experience, so the challenge is to detect and fix such regressions before they¬†ship.</p><p>This post describes how the Netflix TVUI team implemented a robust strategy to quickly and easily detect performance anomalies before they are released‚Ää‚Äî‚Ääand often before they are even committed to the codebase.</p><h3>What do we mean by Performance?</h3><p>Technically, ‚Äúperformance‚Äù metrics are those relating to the responsiveness or latency of the app, including start up¬†time.</p><p>But TV devices also tend to be more memory constrained than other devices, and as such are more liable to crash during a memory spike‚Ää‚Äî‚Ääso for Netflix TV we actually care about memory at least as much as performance, maybe more¬†so.</p><p>At Netflix the term ‚Äúperformance‚Äù usually encompasses both performance metrics (in the strict meaning) and memory metrics, and that‚Äôs how we‚Äôre using the term¬†here.</p><h3>Why do we run Performance Tests on¬†commits?</h3><p>It‚Äôs harder to reason about the performance profile of pre-production code since we can‚Äôt gather real-time metrics for code that hasn‚Äôt yet shipped. We do cut a canary release in advance of shipment which is dogfooded by Netflix employees and subject to the same metrics collection as the production release. While the canary release is a useful dry-run for pending shipments, it sometimes misses regressions because the canary user base is a fraction of the production release. And in the event that regressions are detected in the canary, it still necessitates an often messy and time consuming revert or¬†patch.</p><p>By running performance tests against every commit (pre- and post-merge), we can detect potentially regressive commits earlier. The sooner we detect such commits the fewer subsequent builds are affected and the easier it is to revert. Ideally we catch regressions before they even reach the main¬†branch.</p><h3>What are the Performance Tests?</h3><p>The goal of our TVUI Performance Tests is to gather memory and responsiveness metrics while simulating the full range of member interactions with Netflix¬†TV.</p><p>There are roughly 50 performance tests, each one designed to reproduce an aspect of member engagement. The goal is to keep each test brief and focused on a specific, isolated piece of functionality (startup, profile switching, scrolling through titles, selecting an episode, playback etc.), while the test suite as a whole should cover the entire member experience with minimal duplication. In this way we can run multiple tests in parallel and the absence of long pole tests keeps the overall test time manageable and allows for repeat test runs. Every test runs on a combination of devices (physical and virtual) and platform versions (<em>SDKs</em>). We‚Äôll refer to each unique test/device/SDK combination as a <em>test variation</em>.</p><p>We run the full performance suite twice per Pull Request¬†(PR):</p><ul><li>when the PR is first submitted</li><li>when the PR is merged to the destination branch</li></ul><h4>Measurement</h4><p>Each performance test tracks either memory or responsiveness. Both of these metrics will fluctuate over the course of a test, so we post metric values at regular intervals throughout the test. To compare test runs we need a method to consolidate this range of observed values into a single¬†value.</p><p>We made the following decisions:</p><p><strong>Memory Tests:</strong> use the maximum memory value observed during the test run (because that‚Äôs the value that determines whether a device could¬†crash).</p><p><strong>Responsiveness Tests¬†:</strong> use the median value observed during the test run (based on the assumption that perceived slowness is influenced by all responses, not just the worst response).</p><h3>What are the Challenges?</h3><p>When Netflix is running in production, we capture real-time performance data which makes it relatively easy to make assertions about the app‚Äôs performance. It‚Äôs much harder to assess the performance of pre-production code (changes merged to the main branch but not yet released) and harder still to get a performance signal for unmerged code in a PR. Performance test metrics are inferior to real-time usage metrics for several¬†reasons:</p><ul><li><strong>Data volume:</strong> In the Netflix app, the same steps are repeated billions of times, but developer velocity and resource constraints dictate that performance tests can only run a handful of times per¬†build.</li><li><strong>Simulation: </strong>No matter how rigorous or creative our testing process is, we can only ever approximate the experience of real life users, never replicate it. Real users regularly use Netflix for hours at a time, and every user has different preferences and¬†habits.</li><li><strong>Noise: </strong>Ideally a given codebase running any given test variation will always return identical results. In reality that just never happens: no two device CPUs are identical, garbage collection is not entirely predictable, API request volume and backend activity is variable‚Ää‚Äî‚Ääso are power levels and network bandwidth. For every test there will be background noise that we need to somehow filter from our analysis.</li></ul><h3>Initial Approach: Static Thresholds</h3><p>For our first attempt at performance validation we assigned maximum acceptable threshold values for memory metrics. There was a sound rationale behind this approach‚Ää‚Äî‚Ääwhen a TV runs Netflix there is a hard limit for memory footprint beyond which Netflix has the potential to¬†crash.</p><p>There were several issues with the static thresholds approach:</p><ul><li><strong>Custom preparation work per test:</strong> Since each test variation has a unique memory profile, the appropriate static threshold had to be researched and assigned on a case-by-case basis. This was difficult and time consuming, so we only assigned thresholds to about 30% of test variations.</li><li><strong>Lack of context:</strong> As a validation technique, static thresholds proved to be somewhat arbitrary. Imagine a commit that increases memory usage by 10% but to a level which is just below the threshold. The next commit might be a README change (zero memory impact) but due to normal variations in device background noise, the metric could increase by just enough to breach the threshold.</li><li><strong>Background variance is not filtered:</strong> Once the codebase is bumping against the memory threshold, background device noise becomes the principal factor determining which side of the threshold line the test result¬†falls.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*jQil7x0BaYT7AP4u" /><figcaption>Unreliable regression signals with static Threshold technique</figcaption></figure><ul><li><strong>Post-alert adjustments:</strong> We found ourselves repeatedly increasing the thresholds to move them clear of background noise</li></ul><h3>The Pivot: Anomaly and Changepoint Detection</h3><p>It became apparent we needed a technique for performance validation that:</p><ul><li><strong>Removes failure bias</strong> by giving equal weight to all test runs, regardless of¬†results</li><li><strong>Doesn‚Äôt treat performance data points in isolation</strong>, but instead assesses the performance impact of a build in relation to previous¬†builds.</li><li><strong>Can be automatically applied to every test</strong> without the need for pre-hoc research, data entry or ongoing manual intervention</li><li><strong>Could be equally applied to test data of any type</strong>: memory, responsiveness, or any other non-boolean test¬†data</li><li><strong>Minimizes the impact of background noise</strong> by prioritizing variance over absolute¬†values</li><li><strong>Improves insight</strong> by examining data points both at the time of creation and retroactively</li></ul><p>We settled on a two-pronged approach:</p><ul><li><strong>Anomaly Detection</strong> immediately calls out potential performance regressions by comparing with recent past¬†data</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pzAyf5-vFdvq2zKBL0Alww.png" /></figure><ul><li><strong>Changepoint Detection</strong> identifies more subtle performance inflections by examining past and future data¬†clusters</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SjQL33XX-6kGvbwcduZ8vQ.png" /></figure><h4>Anomaly Detection</h4><p>We define an anomaly as any metric data point that is more than <em>n</em> standard deviations above the recent mean, where recent mean and standard deviation are derived from the previous <em>m</em> test runs. For Netflix TV performance tests we currently set <em>n</em> to 4 and <em>m</em> to 40 but these values can be tweaked to maximize signal to noise ratio. When an anomaly is detected the test status is set to <em>failed</em> and an alert is generated.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1014/0*tnM3kpWhVJIVGEDI" /></figure><p>Anomaly detection works because thresholds are dynamic and derived from existing data. If the data exhibits a lot of background variance then the anomaly threshold will increase to account for the extra¬†noise.</p><h4>Changepoints</h4><p>Changepoints are data points at the boundary of two distinct data distribution patterns. We use a technique called <a href="https://arxiv.org/pdf/1306.4933.pdf">e-divisive</a> to analyze the 100 most recent test runs, using a Python implementation based on <a href="https://github.com/mongodb/signal-processing-algorithms/blob/master/src/signal_processing_algorithms/energy_statistics/energy_statistics.py#L222">this implementation</a>.</p><p>Since we‚Äôre only interested in performance regressions, we ignore changepoints that trend lower. When a changepoint is detected for a test, we don‚Äôt fail the test or generate an alert (we consider changepoints to be warnings of unusual patterns, not full blown error assertions).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/974/0*qemS91n-I1HK1Jmx" /></figure><p>As you can see, changepoints are a more subtle signal. They don‚Äôt necessarily indicate a regression but they suggest builds that had an impact on subsequent data distribution.</p><p>Builds that generate changepoints across multiple tests, warrant further investigation before they can be included in the release candidate.</p><p>Changepoints give us more confidence in regression detection because they disregard false positives such as one time data spikes. Because changepoint detection requires after-the-fact data, they are best suited to identifying potentially regressive code that is already in the main branch but has not yet been¬†shipped.</p><h3>Additional Adjustments</h3><h4>Runs per¬†Test</h4><p>To address failure bias, we decided to run all tests 3 times, regardless of the result. We chose 3 iterations to provide enough data to eliminate most device noise (tests are allocated to devices randomly) without creating a productivity bottleneck.</p><h4>Summarizing across Test¬†Runs</h4><p>Next we needed to decide on a methodology to compress the results of each batch of 3 runs into a single value. The goal was to ignore outlier results caused by erratic device behavior.</p><p>Initially we took the average of those three runs, but that led to an excess of false positives because the most irregular test runs exerted too much influence on the result. Switching to the median eliminated some of these false positives but we were still getting an unacceptable number of excess alerts (because during periods of high device noise we would occasionally see outlier results two times out of three). Finally, since we noticed that outlier results tended to be higher than normal‚Ää‚Äî‚Äärarely lower‚Ää‚Äî‚Ääwe settled on using the minimum value across the 3 runs and this proved to be the most effective at eliminating external¬†noise.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*T1AC7tPkjiPL75XR" /><figcaption><em>All data points (3 runs per¬†build)</em></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*jtRkrq8wVNRBo8Ow" /><figcaption><em>Selecting median value per¬†build</em></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hoDXBhioBVBsu72Z" /><figcaption><em>Selecting minimum value per¬†build</em></figcaption></figure><h3>What were the¬†Results?</h3><p>After switching our performance validation to use anomaly and changepoint detection we noticed several improvements.</p><p>a) We are alerted for potential performance regressions far less often, and when we do get alerted it‚Äôs much more likely to indicate a genuine regression. Our workload is further reduced by no longer having to manually increment static performance thresholds after each false positive.</p><p>The following table represents the alert summary for two distinct months last year. In March 2021 we still used static thresholds for regression alerts. By October 2021 we had switched using anomaly detection for regression alerts. <em>Alerts which were true regressions</em> is the number of alerted commits for which the suspected regression turned out to be both significant and persistent.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XE2q95DThkwuVxuJqzlzNA.png" /></figure><p>Note that since the March tests only validated when a threshold was manually set, the total number of validating test runs in October was much greater, and yet we still got only 10% of the¬†alerts.</p><p>b) We are not alerted for subsequent innocuous builds that inherit regressive commits from preceding builds. (Using the static threshold technique, all subsequent builds were alerted until the regressive build was reverted.) This is because regressive builds increase both mean and standard deviation and thus put subsequent non-regressing builds comfortably below the alert threshold.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1kQNvfzf_S5YxZCyMtfoHw.png" /><figcaption>Regressive build is above alert threshold</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KbtqhnHNOjpDmD5N5WPDPg.png" /><figcaption>Subsequent build is easily below alert threshold</figcaption></figure><p>c) Performance tests against PRs, which had been almost constantly red (because the probability of at least one static threshold being breached was always high), are now mostly green. When the performance tests <em>are</em> red we have a much higher confidence that there is a genuine performance regression.</p><p>d) Displaying the anomaly and changepoint count per build provides a visual snapshot that quickly highlights potentially problematic builds.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bBj0ZH14Ffzz6sWL" /></figure><h3>What‚Äôs Next?</h3><h4>Further Work</h4><p>There are still several things we‚Äôd like to¬†improve</p><ul><li><strong>Make it easier to determine if regressions were due to external agents:</strong> Often it turns out the detected regression, though real, was not a result of the committed code but due to an external factor such as an upgrade to one of our platform dependencies, or a feature flag that got switched on. It would be helpful to summarize external changes in our alert summaries.</li><li><strong>Factor out resolved regressions when determining baselines for validation:</strong> <br>When generating recent mean and standard deviation values, we could improve regression detection by filtering out data from erstwhile regressions that have since been¬†fixed.</li><li><strong>Improve Developer Velocity:</strong> We can further reduce total test time by removing unnecessary iterations within tests, adding more devices to ensure availability, and de-emphasizing testing for those parts of the app where performance is less likely to be critical. We can also pre-build app bundles (at least partially) so that the test suite is not delayed by waiting for fresh¬†builds.</li><li><strong>More closely mirror metrics gathered by the production app:</strong> In the deployed Netflix TV app we collect additional metrics such as TTR (time to render) and empty box rate (how frequently titles in the viewport are missing images). While test metrics and metrics collected during real use do not lend themselves to direct comparison, measuring the relative change in metrics in pre-production builds can help us to anticipate regressions in production.</li></ul><h4>Wider Adoption and New Use¬†Cases</h4><p>At this point Anomaly and Changepoint detection is applied to every commit in the TVUI repo, and is in the process of being deployed for commits to the TV Player repo (the layer that manages playback operations). Other Netflix teams (outside of the TV platform) have also expressed interest in these techniques and the ultimate goal is to standardize regression detection across¬†Netflix.</p><p>Anomaly and changepoint detection are entirely framework independent‚Ää‚Äî‚Ääthe only required inputs are a current value and an array of recent values to compare it to. As such, their utility extends far beyond performance tests. For example, we are considering using these techniques to monitor the reliability of non-performance-based test suites‚Ää‚Äî‚Ääin this case the metric of interest is the percent of tests that ran to completion.</p><p>In the future we plan to decouple anomaly and changepoint logic from our test infrastructure and offer it as a standalone open-source library.</p><h3>Wrap Up</h3><p>By using techniques that assess the performance impact of a build in relation to the performance characteristics (magnitude, variance, trend) of adjacent builds, we can more confidently distinguish genuine regressions from metrics that are elevated for other reasons (e.g. inherited code, regressions in previous builds or one-off data spikes due to test irregularities). We also spend less time chasing false negatives and no longer need to manually assign a threshold to each result‚Ää‚Äî‚Ääthe data itself now sets the thresholds dynamically.</p><p>This improved efficiency and higher confidence level helps us to quickly identify and fix regressions before they reach our¬†members.</p><p>The anomaly and changepoint techniques discussed here can be used to identify regressions (or progressions), unexpected values or inflection points in any chronologically sequenced, quantitative data. Their utility extends well beyond performance analysis. For example they could be used to identify inflection points in system reliability, customer satisfaction, product usage, download volume or¬†revenue.</p><p>We encourage you to try these techniques on your own data. We‚Äôd love to learn more about their success (or otherwise) in other contexts!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=eab2602b86fe" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/fixing-performance-regressions-before-they-happen-eab2602b86fe">Fixing Performance Regressions Before they Happen</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></article>
            </details>
            <footer>&nbsp;<q>Netflix TechBlog - Medium&nbsp;</q></footer>
         </section>
         
         <section id="d7e9">
            <header>
               <h2 title="News For Open Source Professionals">Linux.com <a rel="noopener noreferrer" target="_blank" href="https://www.linux.com">ùìó</a><a rel="noopener noreferrer" target="_blank" href="https://www.linux.com/feed/">ùìï</a></h2>
            </header>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://www.linux.com/news/ospo-mind-map-2-0-release-is-out/">OSPO Mind Map 2.0 release is out!</a> <span>
                     
                     
                     TODO Group is proud to&nbsp;announce a new OSPO Mind Map version&nbsp;release. The mind map
                     shows a Open Source Program Office‚Äôs (OSPO) responsibilities, roles, behavior, and
                     team size within an organization. This post highlights the major improvements done
                     by the community in this new version of the OSPO Mind Map.
                     Updates on&nbsp;Responsibilities&nbsp;section
                     OSPO Mind Map&nbsp;Responsibilities&nbsp;section has new OSPO-specific topics and different
                     sub-sections defined, including:
                     Develop and Execute Open Source Strategy
                     Eliminate Friction from Using and Contributing to Open Source
                     Manage Open Source IT Infrastructure
                     Give Advice on Open Source
                     Grow and Retain Open Source Talent Inside the Organization
                     Implement InnerSource Practices
                     Track Performance Metrics
                     Collaborate with Open Source Organizations
                     Prioritize and Drive Open Source Upstream Development
                     Establish and Improve Open Source Policies and Processes
                     Oversee Open Source Compliance
                     Support Corporate Development Activities
                     Initial pull request with these changes can be found&nbsp;here . 
                     Welcoming Contributors 
                     The TODO Community welcomes more contributors to the OSPO mind Map to bring together
                     the various communities involved in OSPO-specific topics. This will help to improve
                     open source professionals‚Äô guidance across the OSPO ecosystem (e.g topics like ‚ÄúInnerSource‚Äù,
                     ‚ÄúOpen Source metrics‚Äù, ‚ÄúOpen Source Compliance‚Äù and more).
                     Updates on display
                     Initially, the OSPO Mind Map displayed all sections by default, showing a huge mind
                     map image. Now, when people access&nbsp;https://ospomindmap.todogroup.org/&nbsp;the display
                     view will only show the first 2 levels, so people can expand specific sections, avoiding
                     unnecessary information and focusing on what matters to them at that time.
                     Welcoming Contributors 
                     We are looking for tech contributors to work on a process to&nbsp;automatically deploy
                     new versions of OSPO mind map to the website&nbsp;. If you‚Äôd be interested to contribute,
                     please open a&nbsp;PR&nbsp;!
                     About OSPO Mind Map and OSPOlogy
                     This Mind Map is part of the TODO Group‚Äôs&nbsp;OSPOlogy repository&nbsp;which encapsulates a
                     set of open initiatives (including the OSPO Mind Map, virtual global &amp;amp; regional
                     meetings, an OSPO discussion forum, monthly OSPO News, and now, in-person workshops)
                     to work in collaboration and study the status of OSPOs.
                     Acknowledgments
                     Thanks to OSPO Mind Map‚Äôs v2.0 contributors and reviewers!
                     Thomas Steenbergen (EPAM)
                     Ana Jim√©nez (Linux Foundation)
                     Jari Koivisto
                     Josep Prat (Aiven)
                     Gergely Csatari (Nokia)
                     Special thanks to Ibrahim Haddad (Linux Foundation), we were inspired by the&nbsp;OSPO
                     responsibilities&nbsp;section in&nbsp;A Close Look at Open Source Program Offices: Structure,
                     Roles and Responsibilities&nbsp;.
                     
                     
                     The post OSPO Mind Map 2.0 release is out! appeared first on Linux Foundation.The
                     post OSPO Mind Map 2.0 release is out! appeared first on Linux.com.
                     </span></summary><time datetime="2022-06-18T07:16:27+02:00">Sat, 18 Jun 2022 05:16</time><article><img width="150" height="150" src="https://www.linux.com/wp-content/uploads/2022/06/mind-map-y0pcwd-150x150.gif" class="attachment-thumbnail size-thumbnail wp-post-image" alt="" loading="lazy" srcset="https://www.linux.com/wp-content/uploads/2022/06/mind-map-y0pcwd-150x150.gif 150w, https://www.linux.com/wp-content/uploads/2022/06/mind-map-y0pcwd-24x24.gif 24w, https://www.linux.com/wp-content/uploads/2022/06/mind-map-y0pcwd-48x48.gif 48w, https://www.linux.com/wp-content/uploads/2022/06/mind-map-y0pcwd-96x96.gif 96w, https://www.linux.com/wp-content/uploads/2022/06/mind-map-y0pcwd-300x300.gif 300w" sizes="(max-width: 150px) 100vw, 150px" /><div></div>
<div class="flex_column av-105y4ux-2a2050e787e8af56f7f732d3ab7f539c av_one_full  avia-builder-el-0  avia-builder-el-no-sibling  first flex_column_div ">
<div class="avia_textblock  ">
<p>TODO Group is proud to¬†<a href="https://ospomindmap.todogroup.org/" target="_blank" rel="noopener">announce a new OSPO Mind Map version</a>¬†release. The mind map shows a Open Source Program Office‚Äôs (OSPO) responsibilities, roles, behavior, and team size within an organization. This post highlights the major improvements done by the community in this new version of the OSPO Mind Map.</p>
<h2>Updates on¬†Responsibilities¬†section</h2>
<p>OSPO Mind Map¬†Responsibilities¬†section has new OSPO-specific topics and different sub-sections defined, including:</p>
<p> Develop and Execute Open Source Strategy<br />
 Eliminate Friction from Using and Contributing to Open Source<br />
 Manage Open Source IT Infrastructure<br />
 Give Advice on Open Source<br />
 Grow and Retain Open Source Talent Inside the Organization<br />
 Implement InnerSource Practices<br />
 Track Performance Metrics<br />
 Collaborate with Open Source Organizations<br />
 Prioritize and Drive Open Source Upstream Development<br />
 Establish and Improve Open Source Policies and Processes<br />
 Oversee Open Source Compliance<br />
 Support Corporate Development Activities</p>
<p>Initial pull request with these changes can be found¬†<a href="https://github.com/todogroup/ospology/pull/111" target="_blank" rel="noopener">here</a> . <a href="https://linuxfoundation.org/wp-content/uploads/mind-map.gif"></a></p>
<p><strong>Welcoming Contributors </strong></p>
<p>The TODO Community welcomes more contributors to the OSPO mind Map to bring together the various communities involved in OSPO-specific topics. This will help to improve open source professionals‚Äô guidance across the OSPO ecosystem (e.g topics like ‚ÄúInnerSource‚Äù, ‚ÄúOpen Source metrics‚Äù, ‚ÄúOpen Source Compliance‚Äù and more).</p>
<h2>Updates on display</h2>
<p>Initially, the OSPO Mind Map displayed all sections by default, showing a huge mind map image. Now, when people access¬†<a href="https://ospomindmap.todogroup.org/" target="_blank" rel="noopener">https://ospomindmap.todogroup.org/</a>¬†the display view will only show the first 2 levels, so people can expand specific sections, avoiding unnecessary information and focusing on what matters to them at that time.</p>
<p><strong>Welcoming Contributors </strong></p>
<p>We are looking for tech contributors to work on a process to¬†<a href="https://github.com/todogroup/ospology/issues/130" target="_blank" rel="noopener">automatically deploy new versions of OSPO mind map to the website</a>¬†. If you‚Äôd be interested to contribute, please open a¬†<a href="https://github.com/todogroup/ospology/pulls" target="_blank" rel="noopener">PR</a>¬†!</p>
<h2>About OSPO Mind Map and OSPOlogy</h2>
<p>This Mind Map is part of the TODO Group‚Äôs¬†<a href="https://github.com/todogroup/ospology" target="_blank" rel="noopener">OSPOlogy repository</a>¬†which encapsulates a set of open initiatives (including the OSPO Mind Map, virtual global &amp; regional meetings, an OSPO discussion forum, monthly OSPO News, and now, in-person workshops) to work in collaboration and study the status of OSPOs.</p>
<h2>Acknowledgments</h2>
<p>Thanks to OSPO Mind Map‚Äôs v2.0 contributors and reviewers!</p>
<p>Thomas Steenbergen (EPAM)<br />
Ana Jim√©nez (Linux Foundation)<br />
Jari Koivisto<br />
Josep Prat (Aiven)<br />
Gergely Csatari (Nokia)</p>
<p>Special thanks to Ibrahim Haddad (Linux Foundation), we were inspired by the¬†<em>OSPO responsibilities</em>¬†section in¬†<a href="https://github.com/ibrahimhaddad/publications/blob/master/Open%20Source%20Program%20Offices.pdf" target="_blank" rel="noopener">A Close Look at Open Source Program Offices: Structure, Roles and Responsibilities</a>¬†.</p>
</div>
</div>
<p>The post <a href="https://www.linuxfoundation.org/blog/ospo-mind-map-2-0-release-is-out/">OSPO Mind Map 2.0 release is out!</a> appeared first on <a href="https://www.linuxfoundation.org/">Linux Foundation</a>.</p><p>The post <a rel="nofollow" href="https://www.linux.com/news/ospo-mind-map-2-0-release-is-out/">OSPO Mind Map 2.0 release is out!</a> appeared first on <a rel="nofollow" href="https://www.linux.com">Linux.com</a>.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://www.linux.com/news/the-sogno-project-wins-prestigious-award-for-focus-on-modular-grid-automation/">The SOGNO Project Wins Prestigious Award for Focus on Modular Grid Automation</a> <span>
                     
                     
                     This post originally appeared on the LF Energy‚Äôs blog. LF Energy is a project at the
                     Linux Foundation that&nbsp;provides a neutral, collaborative community to build the shared
                     digital investments that will transform the world‚Äôs relationship to energy.
                     The energy sector is amid a huge transformation that will impact the entire world
                     and grid operators need new innovations to match those needs.
                     That‚Äôs why we‚Äôre especially excited to see the recognition awarded&nbsp;Antonello Monti,
                     Director of the  Institute for Automation of Complex Power Systems at RWTH Aachen
                     University and group Leader at Center for Digital Energy, Fraunhofer FIT, for his
                     leadership with SOGNO, the ‚ÄúService-based Open-source Grid automation platform for
                     Network Operation‚Äù of the future.
                     Monti received the second most prestigious award given by the German government, the&nbsp;innovation
                     prize of North Rhine-Westphalia. Awarded annually, this prize recognizes outstanding
                     achievements and excellent research.
                     We are so proud of the work Monti, who also serves at the Technical Advisory Committee
                     Chair for LF Energy, and&nbsp;Markus Mirz&nbsp;have undertaken. We also want to extend our congratulations
                     to the many individuals, companies, and the European Commission who funded the original
                     work for SOGNO (meaning ‚Äúdream‚Äù in Italian).
                     SOGNO is an LF Energy project that is creating plug-and-play, cloud-native, micro-services
                     to implement our next generation of data-driven monitoring and control systems. It
                     will simplify the life of distribution utilities by enabling them to optimize their
                     network operations through open source to deliver cost-effectively, and seamless,
                     secure power to customers.
                     A breakthrough innovation is that SOGNO introduces the idea of grid automation as
                     a modular system in which components can be added through time. This is in opposition
                     to classical monolithic solutions, which weren‚Äôt constructed with today‚Äôs energy landscape
                     in mind.
                     Today, as more renewables come onto the grid, the flow of energy moves from just one
                     way, which was true in the past, to both ways on and off the grid.In the future, power
                     system networks will be composed of assets whose profiles may shift between loads,
                     resources, and the ability to provide flexibility back to the grid.
                     Reinforcing the current system is not sufficient to deal with the increasing complexity
                     of distribution systems. Rather, we are at the cusp of needing deployment of advanced
                     distribution management systems that can be implemented as centralized but even better
                     as distributed architecture.
                     We reiterate our deep gratitude and support for this project, and the people and entities
                     who‚Äôre making it happen.
                     Read&nbsp;here&nbsp;for more information
                     
                     
                     The post The SOGNO Project Wins Prestigious Award for Focus on Modular Grid Automation
                     appeared first on Linux Foundation.The post The SOGNO Project Wins Prestigious Award
                     for Focus on Modular Grid Automation appeared first on Linux.com.
                     </span></summary><time datetime="2022-06-18T03:23:56+02:00">Sat, 18 Jun 2022 01:23</time><article><img width="150" height="150" src="https://www.linux.com/wp-content/uploads/2022/06/SOGNO-LF-Energy-eYBxU0-150x150.png" class="attachment-thumbnail size-thumbnail wp-post-image" alt="" loading="lazy" srcset="https://www.linux.com/wp-content/uploads/2022/06/SOGNO-LF-Energy-eYBxU0-150x150.png 150w, https://www.linux.com/wp-content/uploads/2022/06/SOGNO-LF-Energy-eYBxU0-24x24.png 24w, https://www.linux.com/wp-content/uploads/2022/06/SOGNO-LF-Energy-eYBxU0-48x48.png 48w, https://www.linux.com/wp-content/uploads/2022/06/SOGNO-LF-Energy-eYBxU0-96x96.png 96w, https://www.linux.com/wp-content/uploads/2022/06/SOGNO-LF-Energy-eYBxU0-300x300.png 300w" sizes="(max-width: 150px) 100vw, 150px" /><div></div>
<div class="flex_column av-14x6efc-68d378ed17e0a1a3dcf164675a680b2a av_one_full  avia-builder-el-0  avia-builder-el-no-sibling  first flex_column_div ">
<div class="avia_textblock  ">
<p><em>This post originally appeared on the LF Energy‚Äôs <a href="https://www.lfenergy.org/the-sogno-project-wins-prestigious-award-for-focus-on-modular-grid-automation/" target="_blank" rel="noopener">blog</a>. <a href="https://www.lfenergy.org/" target="_blank" rel="noopener">LF Energy</a> is a project at the Linux Foundation that¬†provides a neutral, collaborative community to build the shared digital investments that will transform the world‚Äôs relationship to energy.</em></p>
<p>The energy sector is amid a huge transformation that will impact the entire world and grid operators need new innovations to match those needs.</p>
<p>That‚Äôs why we‚Äôre especially excited to see the recognition awarded¬†<a href="https://www.lfenergy.org/leadership/" target="_blank" rel="noopener">Antonello Monti</a>, Director of the <a href="https://linuxfoundation.org/wp-content/uploads/sogno-horizontal-color.svg"></a> Institute for Automation of Complex Power Systems at RWTH Aachen University and group Leader at Center for Digital Energy, Fraunhofer FIT, for his leadership with <a href="https://github.com/sogno-platform" target="_blank" rel="noopener">SOGNO</a>, the ‚ÄúService-based Open-source Grid automation platform for Network Operation‚Äù of the future.</p>
<p>Monti received the second most prestigious award given by the German government, the¬†<a href="https://www.rwth-aachen.de/go/id/vhltf?lidx=1" target="_blank" rel="noopener">innovation prize of North Rhine-Westphalia</a>. Awarded annually, this prize recognizes outstanding achievements and excellent research.</p>
<p>We are so proud of the work Monti, who also serves at the Technical Advisory Committee Chair for LF Energy, and¬†<a href="https://www.linkedin.com/in/markus-mirz-14981571/" target="_blank" rel="noopener">Markus Mirz</a>¬†have undertaken. We also want to extend our congratulations to the many individuals, companies, and the European Commission who funded the original work for SOGNO (meaning ‚Äúdream‚Äù in Italian).</p>
<p>SOGNO is an LF Energy project that is creating plug-and-play, cloud-native, micro-services to implement our next generation of data-driven monitoring and control systems. It will simplify the life of distribution utilities by enabling them to optimize their network operations through open source to deliver cost-effectively, and seamless, secure power to customers.</p>
<p>A breakthrough innovation is that SOGNO introduces the idea of grid automation as a modular system in which components can be added through time. This is in opposition to classical monolithic solutions, which weren‚Äôt constructed with today‚Äôs energy landscape in mind.</p>
<p>Today, as more renewables come onto the grid, the flow of energy moves from just one way, which was true in the past, to both ways on and off the grid.In the future, power system networks will be composed of assets whose profiles may shift between loads, resources, and the ability to provide flexibility back to the grid.</p>
<p>Reinforcing the current system is not sufficient to deal with the increasing complexity of distribution systems. Rather, we are at the cusp of needing deployment of advanced distribution management systems that can be implemented as centralized but even better as distributed architecture.</p>
<p>We reiterate our deep gratitude and support for this project, and the people and entities who‚Äôre making it happen.</p>
<p>Read¬†<a href="https://www.lfenergy.org/projects/sogno/" target="_blank" rel="noopener">here</a>¬†for more information</p>
</div>
</div>
<p>The post <a href="https://www.linuxfoundation.org/blog/the-sogno-project-wins-prestigious-award-for-focus-on-modular-grid-automation/">The SOGNO Project Wins Prestigious Award for Focus on Modular Grid Automation</a> appeared first on <a href="https://www.linuxfoundation.org/">Linux Foundation</a>.</p><p>The post <a rel="nofollow" href="https://www.linux.com/news/the-sogno-project-wins-prestigious-award-for-focus-on-modular-grid-automation/">The SOGNO Project Wins Prestigious Award for Focus on Modular Grid Automation</a> appeared first on <a rel="nofollow" href="https://www.linux.com">Linux.com</a>.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://www.linux.com/news/one-place-to-manage-your-open-source-projects-and-communities/">One Place to Manage Your Open Source Projects and Communities</a> <span>
                     
                     
                     Open source communities are driven by a mutual interest in collaboration and sharing
                     around a common solution. They are filled with passion and energy. As a result, today‚Äôs
                     world is powered by open source software, powering the Internet, databases, programming
                     languages, and so much more. It is revolutionizing industries and tackling the toughest
                     challenges. Just check out the projects fostered here at the Linux Foundation for
                     a peek into what is possible.&nbsp;
                     What is the challenge?&nbsp;
                     As the communities and the projects they support grow and mature, active community
                     engagement to recruit, mentor, and enable an active community is critical. Organizations
                     are now recognizing this as they are more and more dependent on open source communities.
                     Yet, while the ethos of open source is transparency and collaboration, the tool chain
                     to automate, visualize, analyze, and manage open source software production remains
                     scattered, siloed, and of varying quality.
                     How do we address these challenges?
                     And now, involvement and engagement in open source communities goes beyond software
                     developers and extends to engineers, architects, documentation writers, designers,
                     Open Source Program Office professionals, lawyers, and more. To help everyone stay
                     coordinated and engaged, a centralized source of information about their activities,
                     tooling to simplify and streamline information from multiple sources, and a solution
                     to visualize and analyze key parameters and indicators is critical. It can help:&nbsp;
                     Organizations wishing to better understand how to coordinate internal participation
                     in open source and measure outcomes
                     CTOs and engineering leads looking to build a cohesive open source strategy&nbsp;
                     Project maintainers needing to wrangle the legal and operational sides of the project
                     Individual keeping track of their open source impacts
                     Enter the Linux Foundation‚Äôs LFX Platform ‚Äì LFX operationalizes this approach, providing
                     tools built to facilitate every aspect of open source development and empowers projects
                     to standardize, automate, analyze, and self-manage while preserving their choice of
                     tools and development workflows in a vendor-neutral platform.
                     LFX tools do not disrupt a project‚Äôs existing toolchain but rather integrate a project‚Äôs
                     community tools and ecosystem to provide a common control plane with APIs from numerous
                     distributed data sources and operations tools. It also adds intelligence to drive
                     outcome-driven KPIs and utilizes a best practices-driven, vendor-agnostic tools chain.
                     It is the place to go for active community engagement and open source activity, enabling
                     the already powerful open source movement to be even more successful.
                     How does it work?&nbsp;
                     Much of the data and information that makes up the open source universe is, not surprisingly,
                     open to see. For instance, GitHub and GitLab both offer APIs that allow third-parties
                     to track all activity on open projects. Social media and public chat channels, blog
                     posts, documentation, and conference talks are also easily captured. For projects
                     hosted at a foundation, such as the Linux Foundation, there is an opportunity to aggregate
                     the public and semi-private data into a privacy respecting, opt-in unified data layer.&nbsp;
                     More specifically to an organization or project, LFX is modular, extensible, and API-driven.
                     It is pluggable and can easily integrate the data sources and tools that are already
                     in use by organizations rather than force them to change their work processes. For
                     instance:
                     Source control software (e.g. Git, GitHub, or GitLab)
                     CI/CD platforms (e.g. Jenkins, CircleCI, Travis CI, and GitHub Actions)
                     Project management (e.g. Jira, GitHub Issues)
                     Registries&nbsp; (e.g. Docker Hub)
                     Documentation&nbsp; (e.g. Confluence Wiki)
                     Marketing automation (e.g. social media and blogging platforms)
                     Event management platforms (e.g. physical event attendance, speaking engagements,
                     sponsorships, webinar attendance, and webinar presentations)
                     This holistic and configurable view of projects, organizations, foundations, and more
                     make it much easier to understand what is happening in open source, from the most
                     granular to the universal.&nbsp;
                     What do real-world users think?&nbsp;
                     Part of LFX is a community forum to ask questions, share solutions, and more. Recently,
                     Jessica Wagantall shared about the Open Network Automation Platform (ONAP). She notes:
                     
                     ONAP is part of the LF Networking umbrella and consists of 30+ components working
                     together towards the same goal since 2017. Since then, we have faced situations where
                     we have to evaluate if the components are getting enough support during release schedules
                     and if we are identifying our key contributors to the project.
                     In this time, we have learned a lot as we grow, and we have had the chance to have
                     tools and resources that we can rely on every step of the way. One of these tools
                     is LFX Insights.
                     We rely on LFX Insights tools to guide the internal decisions and keep the project
                     growing and the contributions flowing.
                     LFX Insights has become a potent tool that gives us an overview of the project as
                     well as statistics of where our project stands and the changes that we have encountered
                     when we evaluate release content and contribution trends.
                     
                     Read Jessica‚Äôs full post for some specific examples of how LFX Insights helps her
                     and the whole team.&nbsp;
                     John Mertic is a seasoned open source project manager. One of his jobs currently is
                     helping to manage the Academy Software Foundation. John shares:&nbsp;
                     
                     The Academy Software Foundation was formed in 2018 in partnership with the Academy
                     of Motion Pictures Arts and Sciences to provide a vendor-neutral home for open source
                     software in the visual effects and motion picture industries.
                     A challenge this industry was having was that there were many key open source projects
                     used in the industry, such as OpenVDB, OpenColorIO, and OpenEXR, that were cornerstones
                     to production but lacked developers and resources to maintain them. These projects
                     were predominantly single vendor owned and led, and my experience with other open
                     source projects in other verticals and horizontal industries causes this situation,
                     which leads to sustainability concerns, security issues, and lack of future development
                     and innovation.
                     As the project hit its 3rd anniversary in 2021, the Governing Board was wanting to
                     assess the impact the foundation has had on increasing the sustainability of these
                     projects. There were three primary dimensions being assessed.
                     Contributor growth
                     Contribution growth
                     Contributor diversity
                     We at the LF know that seeing those metrics increasing is a good sign for a healthy,
                     sustainable project.
                     Academy Software Foundation projects use LFX Insights as a tool for measuring community
                     health. Using this tool enabled us to build some helpful charts which illustrated
                     the impacts of being a part of the Academy Software Foundation.
                     We took the approach of looking at before and after data on the contributor, contribution,
                     and contributor diversity.
                     
                     Here is one of the charts that John shared. You can view all of them on his post.&nbsp;
                     
                     
                     
                     
                     
                     
                     
                     
                     
                     Conclusion&nbsp;
                     LFX will improve communication and collaboration, simplify management, surface the
                     best projects and project leaders, and provide insightful guidance based on real data
                     captured at scale, across the widest variety of projects ever collected into a single
                     source of information. And it is available to you ‚Äì all Linux Foundation members and
                     projects have access to LFX.&nbsp;
                     To learn more about what it can do for you and your organization and project(s), read
                     our white paper (LINK), read posts in the LFX Community Forum, or just log in with
                     your free LFID and give it a spin. And check back here on the LF Blog for more articles
                     in the coming months on LFX ‚Äì digging in deeper.&nbsp;
                     If you would like to talk to someone at the Linux Foundation about LFX or membership,
                     reach out to Jen Shelby at jshelby@linuxfoundation.org.&nbsp;
                     
                     
                     The post One Place to Manage Your Open Source Projects and Communities appeared first
                     on Linux Foundation.The post One Place to Manage Your Open Source Projects and Communities
                     appeared first on Linux.com.
                     </span></summary><time datetime="2022-06-17T23:24:33+02:00">Fri, 17 Jun 2022 21:24</time><article><img width="150" height="150" src="https://www.linux.com/wp-content/uploads/2022/06/ASWF-LFX-example-bSE4p9-150x150.png" class="attachment-thumbnail size-thumbnail wp-post-image" alt="" loading="lazy" srcset="https://www.linux.com/wp-content/uploads/2022/06/ASWF-LFX-example-bSE4p9-150x150.png 150w, https://www.linux.com/wp-content/uploads/2022/06/ASWF-LFX-example-bSE4p9-24x24.png 24w, https://www.linux.com/wp-content/uploads/2022/06/ASWF-LFX-example-bSE4p9-48x48.png 48w, https://www.linux.com/wp-content/uploads/2022/06/ASWF-LFX-example-bSE4p9-96x96.png 96w" sizes="(max-width: 150px) 100vw, 150px" /><div></div>
<div class="flex_column av-4h7lr6-f8ffee1c01b329220fdaf021b5b5b5fb av_one_full  avia-builder-el-0  avia-builder-el-no-sibling  first flex_column_div ">
<div class="avia_textblock  ">
<p><span>Open source communities are driven by a mutual interest in collaboration and sharing around a common solution. They are filled with passion and energy. As a result, today‚Äôs world is powered by open source software, powering the Internet, databases, programming languages, and so much more. It is revolutionizing industries and tackling the toughest challenges. Just </span><a href="https://www.linuxfoundation.org/projects/"><span>check out the projects fostered here at the Linux Foundation</span></a><span> for a peek into what is possible.¬†</span></p>
<h2><span>What is the challenge?¬†</span></h2>
<p><span>As the communities and the projects they support grow and mature, active community engagement to recruit, mentor, and enable an active community is critical. Organizations are now recognizing this as they are more and more dependent on open source communities. Yet, while the ethos of open source is transparency and collaboration, the tool chain to automate, visualize, analyze, and manage open source software production remains scattered, siloed, and of varying quality.</span></p>
<h2><span>How do we address these challenges?</span></h2>
<p><span>And now, involvement and engagement in open source communities goes beyond software developers and extends to engineers, architects, documentation writers, designers, Open Source Program Office professionals, lawyers, and more. To help everyone stay coordinated and engaged, a centralized source of information about their activities, tooling to simplify and streamline information from multiple sources, and a solution to visualize and analyze key parameters and indicators is critical. It can help:¬†</span></p>
<p><span>Organizations wishing to better understand how to coordinate internal participation in open source and measure outcomes</span><br />
<span>CTOs and engineering leads looking to build a cohesive open source strategy¬†</span><br />
<span>Project maintainers needing to wrangle the legal and operational sides of the project</span><br />
<span>Individual keeping track of their open source impacts</span></p>
<p><span>Enter the Linux Foundation‚Äôs LFX Platform ‚Äì LFX operationalizes this approach, providing tools built to facilitate every aspect of open source development and empowers projects to standardize, automate, analyze, and self-manage while preserving their choice of tools and development workflows in a vendor-neutral platform.</span></p>
<p><span>LFX tools do not disrupt a project‚Äôs existing toolchain but rather integrate a project‚Äôs community tools and ecosystem to provide a common control plane with APIs from numerous distributed data sources and operations tools. It also adds intelligence to drive outcome-driven KPIs and utilizes a best practices-driven, vendor-agnostic tools chain. It is the place to go for active community engagement and open source activity, enabling the already powerful open source movement to be even more successful.</span></p>
<h2><span>How does it work?¬†</span></h2>
<p><span>Much of the data and information that makes up the open source universe is, not surprisingly, open to see. For instance, GitHub and GitLab both offer APIs that allow third-parties to track all activity on open projects. Social media and public chat channels, blog posts, documentation, and conference talks are also easily captured. For projects hosted at a foundation, such as the Linux Foundation, there is an opportunity to aggregate the public and semi-private data into a privacy respecting, opt-in unified data layer.¬†</span></p>
<p><span>More specifically to an organization or project, LFX is modular, extensible, and API-driven. It is pluggable and can easily integrate the data sources and tools that are already in use by organizations rather than force them to change their work processes. For instance:</span></p>
<p><span>Source control software (e.g. Git, GitHub, or GitLab)</span><br />
<span>CI/CD platforms (e.g. Jenkins, CircleCI, Travis CI, and GitHub Actions)</span><br />
<span>Project management (e.g. Jira, GitHub Issues)</span><br />
<span>Registries¬† (e.g. Docker Hub)</span><br />
<span>Documentation¬† (e.g. Confluence Wiki)</span><br />
<span>Marketing automation (e.g. social media and blogging platforms)</span><br />
<span>Event management platforms (e.g. physical event attendance, speaking engagements, sponsorships, webinar attendance, and webinar presentations)</span></p>
<p><span>This holistic and configurable view of projects, organizations, foundations, and more make it much easier to understand what is happening in open source, from the most granular to the universal.¬†</span></p>
<h2><span>What do real-world users think?¬†</span></h2>
<p><span>Part of LFX is a community forum to ask questions, share solutions, and more. Recently, Jessica Wagantall shared about the </span><a href="https://www.onap.org/"><span>Open Network Automation Platform (ONAP)</span></a><span>. She notes:</span></p>
<div>
<p><span>ONAP is part of the LF Networking umbrella and consists of 30+ components working together towards the same goal since 2017. Since then, we have faced s</span><span>ituations where we have to evaluate if the components are getting enough support during release schedules and if we are identifying our key contributors to the project.</span></p>
<p><span>In this time, we have learned a lot as we grow, and we have had the chance to have tools and resources that we can rely on every step of the way. One of these tools is </span><a href="https://insights.lfx.linuxfoundation.org/"><span>LFX Insights</span></a><span>.</span></p>
<p><span>We rely on LFX Insights tools to guide the internal decisions and keep the project growing and the contributions flowing.</span></p>
<p><span>LFX Insights has become a potent tool that gives us an overview of the project as well as statistics of where our project stands and the changes that we have encountered when we evaluate release content and contribution trends.</span></p>
</div>
<p><span>Read </span><a href="https://community.lfx.dev/t/my-experience-with-lfx-insights-as-a-release-engineer/917"><span>Jessica‚Äôs full post</span></a><span> for some specific examples of how LFX Insights helps her and the whole team.¬†</span></p>
<p><span>John Mertic is a seasoned open source project manager. One of his jobs currently is helping to manage the </span><a href="https://www.aswf.io/"><span>Academy Software Foundation</span></a><span>. John shares:¬†</span></p>
<div>
<p><span>The Academy Software Foundation was formed in 2018 in partnership with the Academy of Motion Pictures Arts and Sciences to provide a vendor-neutral home for open source software in the visual effects and motion picture industries.</span></p>
<p><span>A challenge this industry was having was that there were many key open source projects used in the industry, such as OpenVDB, OpenColorIO, and OpenEXR, that were cornerstones to production but lacked developers and resources to maintain them. These projects were predominantly single vendor owned and led, and my experience with other open source projects in other verticals and horizontal industries causes this situation, which leads to sustainability concerns, security issues, and lack of future development and innovation.</span></p>
<p><span>As the project hit its 3rd anniversary in 2021, the Governing Board was wanting to assess the impact the foundation has had on increasing the sustainability of these projects. There were three primary dimensions being assessed.</span></p>
<p><span>Contributor growth</span></p>
<p><span>Contribution growth</span></p>
<p><span>Contributor diversity</span></p>
<p><span>We at the LF know that seeing those metrics increasing is a good sign for a healthy, sustainable project.</span></p>
<p><span>Academy Software Foundation projects use LFX Insights as a tool for measuring community health. Using this tool enabled us to build some helpful charts which illustrated the impacts of being a part of the Academy Software Foundation.</span></p>
<p><span>We took the approach of looking at before and after data on the contributor, contribution, and contributor diversity.</span></p>
</div>
<p><span>Here is one of the charts that John shared. You can view all of them on his </span><a href="https://community.lfx.dev/t/showcase-the-value-of-a-foundation-with-lfx-insights/1004"><span>post</span></a><span>.¬†</span></p>
</div>
<p></p>
<div class="avia-image-container av-l4ijhe8x-49c22b374fb21496873bb00fd1a8f756 av-styling- av-hover-grow avia-align-center  avia-builder-el-2  el_after_av_textblock  el_before_av_textblock  ">
<div class="avia-image-container-inner">
<div class="avia-image-overlay-wrap"></div>
</div>
</div>
<p></p>
<div class="avia_textblock  ">
<h2><span>Conclusion¬†</span></h2>
<p><span>LFX will improve communication and collaboration, simplify management, surface the best projects and project leaders, and provide insightful guidance based on real data captured at scale, across the widest variety of projects ever collected into a single source of information. And it is available to you ‚Äì all Linux Foundation members and projects have access to LFX.¬†</span></p>
<p><span>To learn more about what it can do for you and your organization and project(s), read our </span><a href="https://www.linuxfoundation.org/tools/open-source-missing-data-management-layer/"><span>white paper</span></a><span> (LINK), read posts in the </span><a href="https://community.lfx.dev/"><span>LFX Community Forum</span></a><span>, or </span><a href="https://lfx.linuxfoundation.org/"><span>just log in </span><span>with your free LFID </span><span>and give it a spin</span></a><span>. And check back here on the LF Blog for more articles in the coming months on LFX ‚Äì digging in deeper.¬†</span></p>
<p><span>If you would like to talk to someone at the Linux Foundation about LFX or membership, reach out to Jen Shelby at <a href="mailto:jshelby@linuxfoundation.org">jshelby@linuxfoundation.org</a>.¬†</span></p>
</div>
</div>
<p>The post <a href="https://www.linuxfoundation.org/blog/one-place-to-manage-your-open-source-projects-and-communities/">One Place to Manage Your Open Source Projects and Communities</a> appeared first on <a href="https://www.linuxfoundation.org/">Linux Foundation</a>.</p><p>The post <a rel="nofollow" href="https://www.linux.com/news/one-place-to-manage-your-open-source-projects-and-communities/">One Place to Manage Your Open Source Projects and Communities</a> appeared first on <a rel="nofollow" href="https://www.linux.com">Linux.com</a>.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://www.linux.com/news/a-new-framework-for-in-person-ospo-workshops-todo-group-seeks-collaborators/">A New Framework for In-Person OSPO Workshops: TODO Group Seeks Collaborators</a> <span>
                     
                     
                     As more and more organizations adopt open source initiatives and/or seek to mature
                     their involvement in open source, they often face many challenges, such as educating
                     developers on good open source practices, building policies and infrastructure, ensuring
                     high-quality and frequent releases, engaging with developer communities, and contributing
                     back to other projects effectively. They recognize that open source is a complex ecosystem
                     that is a community of communities. It doesn‚Äôt follow traditional corporate rules,
                     so guidance is needed to overcome cultural change.&nbsp;
                     To help address these challenges and take advantage of the opportunities, organizations
                     are turning to open source program offices (OSPOs). An OSPO is designed to be the
                     center of competency for an organization‚Äôs open source operations and structure. This
                     can include setting code use, distribution, selection, auditing, and other policies,
                     as well as training developers, ensuring legal compliance, and promoting and building
                     community engagement that benefits the organization strategically.&nbsp;
                     The Linux Foundation‚Äôs TODO Group‚Äôs mission is to help foster the adoption and improvement
                     of OSPOs around the world. They are a tremendous resource, with extensive guides,
                     a new mind map, an online course, case studies, and more. Check out their resources,
                     community, and join their efforts.&nbsp;
                     Thanks in part to their efforts, the OSPO movement is expanding across industries
                     and regions of all types and sizes. However, due to the wide range of responsibilities
                     and ways to operate, OSPO professionals often find it difficult to implement OSPO
                     best practices, policies, processes, or tools for their open source management efforts.
                     To help people with these challenges, the TODO Group is introducing a new framework
                     for in-person OSPO workshops. The framework is publicly available in ospology. This
                     repo encapsulates a set of open initiatives (including an OSPO Mind Map 2.0, virtual
                     global &amp;amp; regional meetings, an OSPO discussion forum, monthly OSPO News, and now,
                     in-person workshops) to work in collaboration that aims to study and discuss the status
                     of OSPOs and, ultimately, make them even more effective.&nbsp;
                     TODO is piloting these in Europe first, and they are currently seeking collaborators
                     to bring together the various communities involved in OSPO-specific topics and help
                     organizations effectively implement OSPO Programs based on the specific needs for
                     the region.
                     Backing up a bit, let‚Äôs look at the OSPOlogy.live framework.&nbsp;
                     OSPOlogy.live framework in a nutshell
                     Follows an ‚Äúunconference style,‚Äù meaning it‚Äôs a participants-driven meeting
                     Adheres to the Chatham House Rule in order to share openly and learn from each other&nbsp;
                     Connects OSPOs with various open source communities involved in the open source activities
                     that matter to them (e.g. policies, tooling, standards, and community building)
                     Takes place over two days and is an in-person event
                     Consists of prepared presentations, hands-on workshops, and space for networking
                     Falls under the Linux Foundation‚Äôs policies and code of conduct
                     Held at a location provided by one of the participants for free
                     Each participant pays for their own food, travel, and lodging. Meals may be free if
                     workshop organizers find sponsors.
                     Participants can register their interest to receive an invite via Linux Foundation‚Äôs
                     community platform as seats are limited.
                     With that overview, let‚Äôs dig in a little on how the workshop is conducted.
                     Unconference style
                     Typically at an unconference, the agenda of the workshop portion is created by the
                     attendees at the beginning of the meeting. Anyone who wants to initiate a discussion
                     on a topic can claim a time and a space. OSPOlogy workshops are not fully an unconference
                     as the first day is a series of prepared presentations, so you know what the sessions
                     are before joining (1 or 2 will be chosen by the participants ahead of time). For
                     Day 2, the workshops follow the unconference model. Participants vote on topics to
                     be worked on that day. Participants may be asked to submit their topic before the
                     workshop to accelerate/simplify the voting process.
                     Suggested workshop sections
                     OSPO USE CASES Expert-led panels or talks to share experiences and case studies from
                     specific OSPOs
                     OSPO ACCELERATORS Presentation highlighting a specific activity within the specific
                     project, such as outcomes of recent community activities. The aim of the presentation
                     is to give people insights on various topics the communities are working on and get
                     their feedback / to ask for contributions.
                     SHARED CHALLENGES ASSESSMENT  Description: Identify OSPO shared challenges / pain
                     points on the OSPO Mind Map 2.0 and let the audience vote for the areas of interest
                     (working groups) for the workshop breakout groups. For instance, focus areas can be
                     specific activities within OSPO responsibilities.
                     BREAK OUT SESSIONS  Define goals and identify pain points. Each break out group aims
                     to capture their challenges for the selected focus and if possible document their
                     experiences/solutions.
                     NETWORKING
                     Interested in becoming a collaborator?
                     We can‚Äôt do this alone! If you are part of an open source community involved in OSPO-specific
                     topics or an organization willing to help with the workshop planning, schedule and/or
                     provide a space to kick off the first meet-up in Europe, we need your help! Please
                     contact:
                     Ana Jimenez ana@todogroup.org
                     Thomas Steenbergen opensource@steenbe.nl
                     And check out the FAQs below.&nbsp;
                     Don‚Äôt live in Europe? Pencil us in for when this is expanded.&nbsp;
                     Not involved in an OSPO yet? Take time to check out the TODO Group and join the community
                     to start your OSPOlogy journey.
                     Also, consider joining OSPONCon North America next week, June 21-24, 2022, either
                     in Austin, Texas during the Open Source Summit or virtually. Register here.
                     
                     
                     
                     
                     
                     
                     Frequently Asked Questions
                     What do we mean by communities involved in OSPO-specific topics?
                     OSPO-specific topics range from safely using open source to license compliance, sustainability,
                     contributing back to the community, and more. For the full list of OSPO topics please
                     see https://ospomindmap.todogroup.org/:
                     Develop and Execute Open Source Strategy
                     Oversee Open Source Compliance
                     Establish and Improve Open Source Policies and Processes
                     Prioritize and Drive Open Source Upstream Development
                     Collaborate with Open Source Organizations
                     Track Performance Metrics
                     Implement InnerSource Practices
                     Grow and Retain Open Source Talent Inside the Organization
                     Give Advice on Open Source
                     Manage Open Source IT Infrastructure
                     Some examples of OS communities highly involved in these topics are:
                     OpenChain
                     SPDX
                     CHAOSS
                     OpenSSF
                     InnerSource Commons
                     What are the necessary roles to set up an OSPOlogy.live workshop?
                     There are two ways in which you can play your part in OSPOlogy.live set up: (1) the
                     hosting party who makes available a meeting room; and, (2) the workshop organizer/facilitator
                     in charge of workshop activities and planning. (1) and (2) may be the same entity/individual.
                     Further details can be found in the framework documentation.&nbsp;
                     Where can I register for the next OSPOlogy.live?
                     Efforts are already on the way to organize the OSPOlogy workshops in different European
                     countries each quarter. Once collaborators and days are confirmed, registration details
                     and schedules will be published via the OSPOlogy community platform.
                     For further updates, please subscribe to OSPONewsletter and join the TODO community.
                     
                     
                     The post A New Framework for In-Person OSPO Workshops: TODO Group Seeks Collaborators
                     appeared first on Linux Foundation.The post A New Framework for In-Person OSPO Workshops:
                     TODO Group Seeks Collaborators appeared first on Linux.com.
                     </span></summary><time datetime="2022-06-17T22:30:30+02:00">Fri, 17 Jun 2022 20:30</time><article><img width="150" height="150" src="https://www.linux.com/wp-content/uploads/2022/06/OSPOlogy-live-workshops-9eXEBa-150x150.png" class="attachment-thumbnail size-thumbnail wp-post-image" alt="" loading="lazy" srcset="https://www.linux.com/wp-content/uploads/2022/06/OSPOlogy-live-workshops-9eXEBa-150x150.png 150w, https://www.linux.com/wp-content/uploads/2022/06/OSPOlogy-live-workshops-9eXEBa-24x24.png 24w, https://www.linux.com/wp-content/uploads/2022/06/OSPOlogy-live-workshops-9eXEBa-48x48.png 48w, https://www.linux.com/wp-content/uploads/2022/06/OSPOlogy-live-workshops-9eXEBa-96x96.png 96w, https://www.linux.com/wp-content/uploads/2022/06/OSPOlogy-live-workshops-9eXEBa-300x300.png 300w" sizes="(max-width: 150px) 100vw, 150px" /><div></div>
<div class="flex_column av-15axigi-b77bdd8a7337ad9318222594e6ac93f3 av_one_full  avia-builder-el-0  avia-builder-el-no-sibling  first flex_column_div ">
<div class="avia_textblock  ">
<p><span>As more and more organizations adopt open source initiatives and/or seek to mature their involvement in open source, they often face many challenges, such as educating developers on good open source practices, building policies and infrastructure, ensuring high-quality and frequent releases, engaging with developer communities, and contributing back to other projects effectively. They recognize that open source is a complex ecosystem that is a community of communities. It doesn‚Äôt follow traditional corporate rules, so guidance is needed to overcome cultural change.¬†</span></p>
<p><span>To help address these challenges and take advantage of the opportunities, organizations are turning to open source program offices (OSPOs). An </span><a href="https://github.com/todogroup/ospodefinition.org"><span>OSPO</span></a><span> is designed to be the center of competency for an organization‚Äôs open source operations and structure. This can include setting code use, distribution, selection, auditing, and other policies, as well as training developers, ensuring legal compliance, and promoting and building community engagement that benefits the organization strategically.¬†</span></p>
<p><span>The Linux Foundation‚Äôs </span><a href="https://todogroup.org/about/"><span>TODO Group</span></a><span>‚Äôs mission is to help foster the adoption and improvement of OSPOs around the world. They are a tremendous resource, with extensive guides, a new mind map, an online course, case studies, and more. Check out their </span><a href="https://todogroup.org/guides/"><span>resources</span></a><span>, </span><a href="https://todogroup.org/community/"><span>community</span></a><span>, and </span><a href="https://todogroup.org/join/"><span>join their efforts</span></a><span>.¬†</span></p>
<p><span>Thanks in part to their efforts, the OSPO movement is expanding across industries and regions of all types and sizes. However, due to the wide range of responsibilities and ways to operate, OSPO professionals often find it difficult to implement OSPO best practices, policies, processes, or tools for their open source management efforts.</span></p>
<p><span>To help people with these challenges, the TODO Group is introducing a new framework for in-person OSPO workshops. The framework is </span><a href="https://github.com/todogroup/ospology/blob/main/ospology-live/framework.md"><span>publicly available in ospology</span></a><span>. This repo encapsulates a set of open initiatives (including an </span><a href="https://todogroup.org/blog/ospo-mind-map-2-release/"><span>OSPO Mind Map 2.0</span></a><span>, virtual global &amp; regional meetings, an OSPO discussion forum, monthly OSPO News, and now, in-person workshops) to work in collaboration that aims to study and discuss the status of OSPOs and, ultimately, make them even more effective.¬†</span></p>
<p><span>TODO is piloting these in Europe first, and they are currently seeking collaborators to bring together the various communities involved in OSPO-specific topics and help organizations effectively implement OSPO Programs based on the specific needs for the region.</span></p>
<p><span>Backing up a bit, let‚Äôs look at the OSPOlogy.live framework.¬†</span></p>
<h2><span>OSPOlogy.live framework in a nutshell</span></h2>
<p><span>Follows an ‚Äúunconference style,‚Äù meaning it‚Äôs a participants-driven meeting</span><br />
<span>Adheres to the </span><a href="https://www.chathamhouse.org/about-us/chatham-house-rule#:~:text=The%20Rule%20reads%20as%20follows,other%20participant%2C%20may%20be%20revealed."><span>Chatham House Rule</span></a><span> in order to share openly and learn from each other¬†</span><br />
<span>Connects OSPOs with various open source communities involved in the open source activities that matter to them (e.g. policies, tooling, standards, and community building)</span><br />
<span>Takes place over two days and is an in-person event</span><br />
<span>Consists of prepared presentations, hands-on workshops, and space for networking</span><br />
<span>Falls under the Linux Foundation‚Äôs policies and code of conduct</span><br />
<span>Held at a location provided by one of the participants for free</span><br />
<span>Each participant pays for their own food, travel, and lodging. Meals may be free if workshop organizers find sponsors.</span><br />
<span>Participants can register their interest to receive an invite via Linux Foundation‚Äôs community platform as seats are limited.</span></p>
<p><span>With that overview, let‚Äôs dig in a little on how the workshop is conducted.</span></p>
<h2><span>Unconference style</span></h2>
<p><span>Typically at an unconference, the agenda of the workshop portion is created by the attendees at the beginning of the meeting. Anyone who wants to initiate a discussion on a topic can claim a time and a space. OSPOlogy workshops are not fully an unconference as the first day is a series of prepared presentations, so you know what the sessions are before joining (1 or 2 will be chosen by the participants ahead of time). For Day 2, the workshops follow the unconference model. Participants vote on topics to be worked on that day. Participants may be asked to submit their topic before the workshop to accelerate/simplify the voting process.</span></p>
<h2><span>Suggested workshop sections</span></h2>
<p><span>OSPO USE CASES Expert-led panels or talks to share experiences and case studies from specific OSPOs</span><br />
<span>OSPO ACCELERATORS Presentation highlighting a specific activity within the specific project, such as outcomes of recent community activities. The aim of the presentation is to give people insights on various topics the communities are working on and get their feedback / to ask for contributions.</span><br />
<span>SHARED CHALLENGES ASSESSMENT  Description: Identify OSPO shared challenges / pain points on the </span><a href="https://todogroup.org/blog/ospo-mind-map-2-release/"><span>OSPO Mind Map 2.0</span></a><span> and let the audience vote for the areas of interest (working groups) for the workshop breakout groups. For instance, focus areas can be specific activities within OSPO responsibilities.</span><br />
<span>BREAK OUT SESSIONS  Define goals and identify pain points. Each break out group aims to capture their challenges for the selected focus and if possible document their experiences/solutions.</span><br />
<span>NETWORKING</span></p>
<h2><span>Interested in becoming a collaborator?</span></h2>
<p><span>We can‚Äôt do this alone! If you are part of an open source community involved in OSPO-specific topics or an organization willing to help with the workshop planning, schedule and/or provide a space to kick off the first meet-up in Europe, we need your help! Please contact:</span></p>
<p><span>Ana Jimenez <a href="mailto:ana@todogroup.org">ana@todogroup.org</a></span><br />
<span>Thomas Steenbergen </span><a href="mailto:opensource@steenbe.nl"><span>opensource@steenbe.nl</span></a></p>
<p><span>And check out the FAQs below.¬†</span></p>
<p><span>Don‚Äôt live in Europe? Pencil us in for when this is expanded.¬†</span></p>
<p><span>Not involved in an OSPO yet? Take time to check out the TODO Group and join the community to start your OSPOlogy journey.</span></p>
<p><span>Also, consider joining </span><a href="https://events.linuxfoundation.org/open-source-summit-north-america/about/ospocon/"><span>OSPONCon North America</span></a><span> next week, June 21-24, 2022, either in Austin, Texas during the Open Source Summit or virtually. Register </span><a href="https://events.linuxfoundation.org/open-source-summit-north-america/about/ospocon/"><span>here</span></a><span>.</span></p>
</div>
<p>
</p>
<div class="hr av-vm1hyq-2d77e6ebb99eb9ead1505afb05f303b1 hr-default  avia-builder-el-2  el_after_av_textblock  el_before_av_textblock  "><span class="hr-inner "><span class="hr-inner-style"></span></span></div>
<p></p>
<div class="avia_textblock  ">
<h2>Frequently Asked Questions</h2>
<h3>What do we mean by communities involved in OSPO-specific topics?</h3>
<p>OSPO-specific topics range from safely using open source to license compliance, sustainability, contributing back to the community, and more. For the full list of OSPO topics please see <a href="https://ospomindmap.todogroup.org/"><span>https://ospomindmap.todogroup.org/</span></a><span>:</span></p>
<p><span>Develop and Execute Open Source Strategy</span><br />
<span>Oversee Open Source Compliance</span><br />
<span>Establish and Improve Open Source Policies and Processes</span><br />
<span>Prioritize and Drive Open Source Upstream Development</span><br />
<span>Collaborate with Open Source Organizations</span><br />
<span>Track Performance Metrics</span><br />
<span>Implement InnerSource Practices</span><br />
<span>Grow and Retain Open Source Talent Inside the Organization</span><br />
<span>Give Advice on Open Source</span><br />
<span>Manage Open Source IT Infrastructure</span></p>
<p><span>Some examples of OS communities highly involved in these topics are:</span></p>
<p><a href="https://www.openchainproject.org/"><span>OpenChain</span></a><br />
<a href="https://spdx.dev/"><span>SPDX</span></a><br />
<a href="https://chaoss.community/"><span>CHAOSS</span></a><br />
<a href="https://openssf.org/"><span>OpenSSF</span></a><br />
<a href="https://innersourcecommons.org/"><span>InnerSource Commons</span></a></p>
<h3>What are the necessary roles to set up an OSPOlogy.live workshop?</h3>
<p><span>There are two ways in which you can play your part in OSPOlogy.live set up: (1) the hosting party who makes available a meeting room; and, (2) the workshop organizer/facilitator in charge of workshop activities and planning. (1) and (2) may be the same entity/individual. Further details can be found in the framework </span><a href="https://github.com/todogroup/ospology/blob/main/ospology-live/framework.md#-launching-ospologylive-ospology-workshops-piloting-in-europe-first-"><span>documentation</span></a><span>.¬†</span></p>
<h3>Where can I register for the next OSPOlogy.live?</h3>
<p><span>Efforts are already on the way to organize the OSPOlogy workshops in different European countries each quarter. Once collaborators and days are confirmed, registration details and schedules will be published via the </span><a href="https://todogroup.org/community/"><span>OSPOlogy community platform</span></a><span>.</span></p>
<p><span>For further updates, please subscribe to </span><a href="https://www.getrevue.co/profile/osponews"><span>OSPONewsletter</span></a><span> and join the </span><a href="https://todogroup.org/community/"><span>TODO community.</span></a></p>
</div>
</div>
<p>The post <a href="https://www.linuxfoundation.org/blog/a-new-framework-for-in-person-ospo-workshops-todo-group-seeks-collaborators/">A New Framework for In-Person OSPO Workshops: TODO Group Seeks Collaborators</a> appeared first on <a href="https://www.linuxfoundation.org/">Linux Foundation</a>.</p><p>The post <a rel="nofollow" href="https://www.linux.com/news/a-new-framework-for-in-person-ospo-workshops-todo-group-seeks-collaborators/">A New Framework for In-Person OSPO Workshops: TODO Group Seeks Collaborators</a> appeared first on <a rel="nofollow" href="https://www.linux.com">Linux.com</a>.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://www.linux.com/news/podman-compose-or-docker-compose-which-should-you-use-in-podman/">Podman Compose or Docker Compose: Which should you use in Podman?</a> <span>Both projects let you run multiple Podman containers on a single machine. But their
                     differences might make one more appealing than the other.
                     Read More at Enable SysadminThe post Podman Compose or Docker Compose: Which should
                     you use in Podman? appeared first on Linux.com.
                     </span></summary><time datetime="2022-06-17T18:02:32+02:00">Fri, 17 Jun 2022 16:02</time><article><p>Both projects let you run multiple Podman containers on a single machine. But their differences might make one more appealing than the other.</p>
<p><a href="https://www.redhat.com/sysadmin/podman-compose-docker-compose" target="_blank" class="feedzy-rss-link-icon" rel="noopener">Read More</a> at Enable Sysadmin</p><p>The post <a rel="nofollow" href="https://www.linux.com/news/podman-compose-or-docker-compose-which-should-you-use-in-podman/">Podman Compose or Docker Compose: Which should you use in Podman?</a> appeared first on <a rel="nofollow" href="https://www.linux.com">Linux.com</a>.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://www.linux.com/news/sbom-sb-doesnt-stand-for-silver-bullet/">SBOM ‚Äì SB Doesn‚Äôt Stand for Silver Bullet</a> <span>
                     
                     
                     Software Bill of Materials (SBOMs) are like ingredient labels on food. They are critical
                     to keep consumers safe and healthy, they are somewhat standardized, but it is a lot
                     more exciting to grow or make the food rather than the label.&nbsp;
                     What is an SBOM?
                     What is an SBOM? In short, it is a way to tell another party all of the software that
                     is used in the stack that makes up an application. One benefit of having a SBOM is
                     you know what is in there when a vulnerability comes up. You can easily determine
                     if you are vulnerable and where.&nbsp;
                     As modern software is built utilizing a base of software already written (no sense
                     in recreating the wheel), it is important that all of the components don‚Äôt get lost
                     in the shuffle. It isn‚Äôt readily apparent what a particular piece of software utilizes.
                     So, if a vulnerability for Software A arises, you need to know, do I have that piece
                     of software somewhere in my ecosystem, and, if so, where. Then you can remediate if
                     you need to.
                     I can‚Äôt take credit for the food label analogy used in my introduction. I heard it
                     from Allan Friedman, a Senior Advisor and Strategist at the U.S. Cybersecurity and
                     Infrastructure Security Agency (CISA) and a key SBOM advocate, when he presented about
                     SBOMs at the RSA Conference 2022 with Kate Stewart, the VP of Dependable Embedded
                     Systems here at the Linux Foundation. Allan made the point that food labels only provide
                     information. The consumer needs to read and understand them and take appropriate action.
                     For instance, if they are allergic to peanuts, they can look at an ingredient label
                     and determine if they can safely eat the food.&nbsp;
                     SBOMs are similar ‚Äì they tell a person what software is used as an ‚Äúingredient‚Äù so
                     someone can determine if they need to take action if a vulnerability arises. It isn‚Äôt
                     a silver bullet, but it is a vital tool. Without SBOMs no one can track what component
                     ‚Äúingredients‚Äù are in their software applications.
                     SBOMs and the Software Supply Chain
                     Supply chains are impacting our lives more than just restricting availability of consumer
                     goods. Software supply chains are immensely more complicated now as software is built
                     with pre-existing components. This makes software better, more effective, more powerful,
                     etc. But it also introduces risk as more and more parties touch a particular piece
                     of software. Much like our world has become so interdependent, so has our software.&nbsp;
                     Understanding what is in the supply chain for our software helps us effectively secure
                     it. When a new risk emerges, we know what we need to do.&nbsp;
                     SBOMs and Software Security
                     SBOMs are increasingly being recognized as an important pillar in any comprehensive
                     software security plan. A global survey conducted in 2021 Q3 by the Linux Foundation
                     found that 78% of organizations responding plan to use SBOMs in 2022. Additionally,
                     the recently published Open Source Software Security Mobilization Plan recommends
                     SBOMs be universal and the U.S. Executive Order on Improving the Nation‚Äôs Cybersecurity
                     requires SBOMs be provided for software purchased by the U.S. government. And, as
                     Allan points out in his talk, ‚ÄúWe buy everything.‚Äù The E.O. actually lays out a nice
                     summary of SBOMs and their benefits:&nbsp;
                     The term ‚ÄúSoftware Bill of Materials‚Äù or ‚ÄúSBOM‚Äù means a formal record containing the
                     details and supply chain relationships of various components used in building software.&nbsp;
                     Software developers and vendors often create products by assembling existing open
                     source and commercial software components.&nbsp; The SBOM enumerates these components in
                     a product.&nbsp; It is analogous to a list of ingredients on food packaging.&nbsp; An SBOM is
                     useful to those who develop or manufacture software, those who select or purchase
                     software, and those who operate software.&nbsp; Developers often use available open source
                     and third-party software components to create a product; an SBOM allows the builder
                     to make sure those components are up to date and to respond quickly to new vulnerabilities.&nbsp;
                     Buyers can use an SBOM to perform vulnerability or license analysis, both of which
                     can be used to evaluate risk in a product.&nbsp; Those who operate software can use SBOMs
                     to quickly and easily determine whether they are at potential risk of a newly discovered
                     vulnerability. &nbsp; A widely used, machine-readable SBOM format allows for greater benefits
                     through automation and tool integration.&nbsp; The SBOMs gain greater value when collectively
                     stored in a repository that can be easily queried by other applications and systems.&nbsp;
                     Understanding the supply chain of software, obtaining an SBOM, and using it to analyze
                     known vulnerabilities are crucial in managing risk.
                     Allan and Kate spent time in their talk going into the current state of SBOMs, challenges,
                     benefits, tools available for creating and sharing SBOMs, what is a minimum SBOM,
                     standards being developed, making them fully automated, and more. Look for some future
                     LF Blog posts digging into these.&nbsp;
                     But there are things you can do now.&nbsp;
                     What can you and your organization do now?
                     Allan and Kate laid out several things you and your organization can do, starting
                     now. Starting within your organization:&nbsp;
                     
                     Next week: Understand origins of software your organization is using
                     Commercial: can you ask for an SBOM?
                     Open source: do you have an SBOM for the binary or sources you‚Äôre importing?&nbsp;
                     Three months: Understand what SBOMs your customers will require
                     Expectations: which standards, dependency depth, licensing info?
                     Six months: Prototype and deploy
                     Implement SBOM through using an OSS tool and/or starting a conversation with vendor
                     And participate in ongoing discussions to determine best practices for the ecosystem
                     and contribute to open source project any code developed to support SBOMs.&nbsp;
                     
                     But there are also steps you can take as an individual:&nbsp;
                     
                     Next week: Start playing with an open source SBOM tool and apply it to a repo
                     Three months: Have an SBOM strategy that explicitly identifies tooling needs
                     Six months:&nbsp;
                     Begin SBOM implementation through using an OSS tool or starting a conversation with
                     vendor
                     Participate in a plugfest and try to consume another‚Äôs SBOM
                     And make sure to share any open source and commercial tools you find helpful and work
                     with the tools to help harden them, test and report bugs, and push them to scale.
                     
                     How can you shape the future of SBOMs?
                     First, I want to highlight some upcoming opportunities they shared to help shape the
                     future of SBOMs. CISA is running public Tooling &amp;amp; Implementation work stream discussions
                     in July 2022. They are the same, but occur at different times to help accommodate
                     more time zones:&nbsp;
                     July 13, 2022 ‚Äì 3:00-4:30 PM ET
                     July 21, 2022 ‚Äì 9:30-11:00 AM ET&nbsp;
                     If you want to participate, please email SBOM@cisa.dhs.gov.&nbsp;
                     Additionally, there will be ‚Äúplugfests‚Äù to be announced soon, and they suggested organizations
                     already adopting SBOMs publish case studies and reference tooling workflows to help
                     others.&nbsp;
                     Conclusion
                     SBOMs are here to stay. If you aren‚Äôt already, get on the train now. It is pulling
                     out of the station, but you still have an opportunity to help shape where it is going
                     and how well the journey goes.&nbsp;
                     Allan‚Äôs and Kate‚Äôs slides are available here. If you registered to attend the RSA
                     Conference, you can now watch their full presentation on demand here. 
                     
                     
                     
                     
                     
                     
                     The Software Package Data Exchange‚ìá (SPDX‚ìá)
                     The Linux Foundation hosts SPDX, which is an open standard for communicating software
                     bill of material information, including components, licenses, copyrights, and security
                     references. SPDX reduces redundant work by providing a common format for companies
                     and communities to share important data, thereby streamlining and improving compliance.
                     The SPDX specification is an international open standard (ISO/IEC 5962:2021). Learn
                     more at spdx.dev.&nbsp;
                     
                     
                     The post SBOM ‚Äì SB Doesn‚Äôt Stand for Silver Bullet appeared first on Linux Foundation.The
                     post SBOM ‚Äì SB Doesn‚Äôt Stand for Silver Bullet appeared first on Linux.com.
                     </span></summary><time datetime="2022-06-16T02:12:07+02:00">Thu, 16 Jun 2022 00:12</time><article><img width="150" height="150" src="https://www.linux.com/wp-content/uploads/2022/06/SBOMS-and-food-labels-0BmA1f-150x150.png" class="attachment-thumbnail size-thumbnail wp-post-image" alt="" loading="lazy" srcset="https://www.linux.com/wp-content/uploads/2022/06/SBOMS-and-food-labels-0BmA1f-150x150.png 150w, https://www.linux.com/wp-content/uploads/2022/06/SBOMS-and-food-labels-0BmA1f-24x24.png 24w, https://www.linux.com/wp-content/uploads/2022/06/SBOMS-and-food-labels-0BmA1f-48x48.png 48w, https://www.linux.com/wp-content/uploads/2022/06/SBOMS-and-food-labels-0BmA1f-96x96.png 96w, https://www.linux.com/wp-content/uploads/2022/06/SBOMS-and-food-labels-0BmA1f-300x300.png 300w" sizes="(max-width: 150px) 100vw, 150px" /><div></div>
<div class="flex_column av-1yjdghc-14fe26b2bc40e6d572f4e05863be678a av_one_full  avia-builder-el-0  avia-builder-el-no-sibling  first flex_column_div ">
<div class="avia_textblock  ">
<p><span>Software Bill of Materials (SBOMs) are like ingredient labels on food.<a href="https://linuxfoundation.org/wp-content/uploads/food-label.png"></a> They are critical to keep consumers safe and healthy, they are somewhat standardized, but it is a lot more exciting to grow or make the food rather than the label.¬†</span></p>
<h2>What is an SBOM?</h2>
<p><span>What is an SBOM? In short, it is a way to tell another party all of the software that is used in the stack that makes up an application. One benefit of having a SBOM is you know what is in there when a vulnerability comes up. You can easily determine if you are vulnerable and where.¬†</span></p>
<p><span>As modern software is built utilizing a base of software already written (no sense in recreating the wheel), it is important that all of the components don‚Äôt get lost in the shuffle. It isn‚Äôt readily apparent what a particular piece of software utilizes. So, if a vulnerability for Software A arises, you need to know, do I have that piece of software somewhere in my ecosystem, and, if so, where. Then you can remediate if you need to.</span></p>
<p><span>I can‚Äôt take credit for the food label analogy used in my introduction. I heard it from </span><a href="https://www.linkedin.com/in/allanafriedman/"><span>Allan Friedman</span></a><span>, a Senior Advisor and Strategist at the </span><a href="https://www.cisa.gov/"><span>U.S. Cybersecurity and Infrastructure Security Agency</span></a><span> (CISA) and a key SBOM advocate, when he presented about SBOMs at the RSA Conference 2022 with </span><a href="https://www.linkedin.com/in/katestewartaustin/"><span>Kate Stewart</span></a><span>, the VP of Dependable Embedded Systems here at the Linux Foundation. Allan made the point that food labels only provide information. The consumer needs to read and understand them and take appropriate action. For instance, if they are allergic to peanuts, they can look at an ingredient label and determine if they can safely eat the food.¬†</span></p>
<p><span>SBOMs are similar ‚Äì they tell a person what software is used as an ‚Äúingredient‚Äù so someone can determine if they need to take action if a vulnerability arises. It isn‚Äôt a silver bullet, but it is a vital tool. Without SBOMs no one can track what component ‚Äúingredients‚Äù are in their software applications.</span></p>
<h2>SBOMs and the Software Supply Chain</h2>
<p><span>Supply chains are impacting our lives more than just restricting availability of consumer goods. Software supply chains are immensely more complicated now as software is built with pre-existing components. This makes software better, more effective, more powerful, etc. But it also introduces risk as more and more parties touch a particular piece of software. Much like our world has become so interdependent, so has our software.¬†</span></p>
<p><span>Understanding what is in the supply chain for our software helps us effectively secure it. When a new risk emerges, we know what we need to do.¬†</span></p>
<h2>SBOMs and Software Security</h2>
<p><span>SBOMs are increasingly being recognized as an important pillar in any comprehensive software security plan. A global </span><a href="https://www.linuxfoundation.org/tools/the-state-of-software-bill-of-materials-sbom-and-cybersecurity-readiness/"><span>survey conducted in 2021 Q3 by the Linux Foundation</span></a><span> found that 78% of organizations responding plan to use SBOMs in 2022. Additionally, the recently published </span><a href="https://openssf.org/oss-security-mobilization-plan/"><span>Open Source Software Security Mobilization Plan</span></a><span> recommends SBOMs be universal and the </span><a href="https://openssf.org/blog/2021/05/14/how-lf-communities-enable-security-measures-required-by-the-us-executive-order-on-cybersecurity/"><span>U.S. Executive Order on Improving the Nation‚Äôs Cybersecurity</span></a><span> requires SBOMs be provided for software purchased by the U.S. government. And, as Allan points out in his talk, ‚ÄúWe buy everything.‚Äù The E.O. actually lays out a nice summary of SBOMs and their benefits:¬†</span></p>
<div><span>The term ‚ÄúSoftware Bill of Materials‚Äù or ‚ÄúSBOM‚Äù means a formal record containing the details and supply chain relationships of various components used in building software.¬† Software developers and vendors often create products by assembling existing open source and commercial software components.¬† The SBOM enumerates these components in a product.¬† It is analogous to a list of ingredients on food packaging.¬† An SBOM is useful to those who develop or manufacture software, those who select or purchase software, and those who operate software.¬† Developers often use available open source and third-party software components to create a product; an SBOM allows the builder to make sure those components are up to date and to respond quickly to new vulnerabilities.¬† Buyers can use an SBOM to perform vulnerability or license analysis, both of which can be used to evaluate risk in a product.¬† Those who operate software can use SBOMs to quickly and easily determine whether they are at potential risk of a newly discovered vulnerability. ¬† A widely used, machine-readable SBOM format allows for greater benefits through automation and tool integration.¬† The SBOMs gain greater value when collectively stored in a repository that can be easily queried by other applications and systems.¬† Understanding the supply chain of software, obtaining an SBOM, and using it to analyze known vulnerabilities are crucial in managing risk.</span></div>
<p><span>Allan and Kate spent time in their talk going into the current state of SBOMs, challenges, benefits, tools available for creating and sharing SBOMs, what is a minimum SBOM, standards being developed, making them fully automated, and more. Look for some future LF Blog posts digging into these.¬†</span></p>
<p><span>But there are things you can do now.¬†</span></p>
<h2>What can you and your organization do now?</h2>
<p><span>Allan and Kate laid out several things you and your organization can do, starting now. Starting within your organization:¬†</span></p>
<div>
<span>Next week</span><span>: Understand origins of software your organization is using</span>
<p><span>Commercial: can you ask for an SBOM?</span><br />
<span>Open source: do you have an SBOM for the binary or sources you‚Äôre importing?¬†</span></p>
<p><span>Three months</span><span>: Understand what SBOMs your customers will require</span></p>
<p><span>Expectations: which standards, dependency depth, licensing info?</span></p>
<p><span>Six months</span><span>: Prototype and deploy</span></p>
<p><span>Implement SBOM through using an OSS tool and/or starting a conversation with vendor</span></p>
<p><span>And participate in ongoing discussions to determine best practices for the ecosystem and contribute to open source project any code developed to support SBOMs.¬†</span></p>
</div>
<h2><span>But there are also steps you can take as an individual:¬†</span></h2>
<div>
<span>Next week</span><span>: Start playing with an open source SBOM tool and apply it to a repo</span>
<p><span>Three months</span><span>: Have an SBOM strategy that explicitly identifies tooling needs</span></p>
<p><span>Six months</span><span>:¬†</span></p>
<p><span>Begin SBOM implementation through using an OSS tool or starting a conversation with vendor</span><br />
<span>Participate in a plugfest and try to consume another‚Äôs SBOM</span></p>
<p><span>And make sure to share any open source and commercial tools you find helpful and work with the tools to help harden them, test and report bugs, and push them to scale.</span></p>
</div>
<h2>How can you shape the future of SBOMs?</h2>
<p><span>First, I want to highlight some upcoming opportunities they shared to help shape the future of SBOMs. CISA is running public Tooling &amp; Implementation work stream discussions in July 2022. They are the same, but occur at different times to help accommodate more time zones:¬†</span></p>
<p><span>July 13, 2022 ‚Äì 3:00-4:30 PM ET</span><br />
<span>July 21, 2022 ‚Äì 9:30-11:00 AM ET¬†</span></p>
<p><span>If you want to participate, please email </span><a href="mailto:SBOM@cisa.dhs.gov"><span>SBOM@cisa.dhs.gov</span></a><span>.¬†</span></p>
<p><span>Additionally, there will be ‚Äú</span><a href="https://en.wikipedia.org/wiki/Plugtest"><span>plugfests</span></a><span>‚Äù to be announced soon, and they suggested organizations already adopting SBOMs publish case studies and reference tooling workflows to help others.¬†</span></p>
<h2>Conclusion</h2>
<p><span>SBOMs are here to stay. If you aren‚Äôt already, get on the train now. It is pulling out of the station, but you still have an opportunity to help shape where it is going and how well the journey goes.¬†</span></p>
<p><span>Allan‚Äôs and Kate‚Äôs slides are available </span><a href="https://www.linuxfoundation.org/wp-content/uploads/Tooling-up-Getting-SBOMs-to-Scale_slides.pdf"><span>here</span></a><span>. If you registered to attend the RSA Conference, you can now watch their full presentation on demand </span><a href="https://www.rsaconference.com/usa/agenda/session/Tooling%20up%20Getting%20SBOMs%20to%20Scale"><span>here</span></a><span>. </span></p>
</div>
<p>
</p>
<div class="hr av-1iapsfk-3146bd3c041df301aa2633a8309bee9f hr-default  avia-builder-el-2  el_after_av_textblock  el_before_av_textblock  "><span class="hr-inner "><span class="hr-inner-style"></span></span></div>
<p></p>
<div class="avia_textblock  ">
<h2>The Software Package Data Exchange‚ìá (SPDX‚ìá)</h2>
<p><span>The Linux Foundation hosts SPDX, which is an open standard for communicating software bill of material information, including components, licenses, copyrights, and security references. SPDX reduces redundant work by providing a common format for companies and communities to share important data, thereby streamlining and improving compliance. The SPDX specification is an international open standard (ISO/IEC 5962:2021). Learn more at </span><a href="https://spdx.dev/"><span>spdx.dev</span></a><span>.¬†</span></p>
</div>
</div>
<p>The post <a href="https://www.linuxfoundation.org/blog/sbom-sb-doesnt-stand-for-silver-bullet/">SBOM ‚Äì SB Doesn‚Äôt Stand for Silver Bullet</a> appeared first on <a href="https://www.linuxfoundation.org/">Linux Foundation</a>.</p><p>The post <a rel="nofollow" href="https://www.linux.com/news/sbom-sb-doesnt-stand-for-silver-bullet/">SBOM ‚Äì SB Doesn‚Äôt Stand for Silver Bullet</a> appeared first on <a rel="nofollow" href="https://www.linux.com">Linux.com</a>.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://www.linux.com/news/lightspeed-studios-joins-the-open-3d-foundation-as-a-premier-member-to-further-the-vast-potential-of-the-3d-ecosystem/">LightSpeed Studios Joins the Open 3D Foundation as a Premier Member to Further the
                     Vast Potential of the 3D Ecosystem</a> <span>
                     O3D community building a first-class, open-source 3D engine to advance development
                     across gaming, the metaverse, and a variety of other applications
                     SAN FRANCISCO ‚Äì June 15, 2022 ‚Äì The Open 3D Foundation (O3DF), the home of a vibrant
                     community focused on advancing the future of open 3D development, announces its growing
                     ecosystem with the addition of LightSpeed Studios as a Premier member alongside Adobe,
                     AWS, Huawei, Intel, Microsoft and Niantic.
                     Today‚Äôs top-quality 3D engines are as complex as operating systems, requiring significant
                     time, cost, and human capital investments to keep pace with advancements. Open source
                     has repeatedly proven to be the path to quickest innovation. The Open 3D Engine (O3DE)
                     offers a high-fidelity, fully-featured, open source alternative poised to revolutionize
                     real-time 3D development across a variety of industries‚Äîfrom game development, the
                     metaverse, AI and digital twin, to automotive, healthcare, robotics and more.
                     As a Premier member, LightSpeed Studios will bring its leadership and wealth of experience
                     in global research and development of high-quality games to help drive the development
                     of O3DE‚Äôs specifications and initiatives. Tencent Senior Project Manager, Lanye Wang,
                     will join the Open 3D Foundation‚Äôs Governing Board, helping shape the Foundation‚Äôs
                     strategic direction and its stewardship of 3D visualization and simulation projects.&nbsp;
                     ‚ÄúWe are very excited to join the Open 3D Foundation, especially for the opportunity
                     to leverage the connection with all of the other members to dive deep into the graphic
                     technologies and build a top-level open source 3D engine community,‚Äù said Lanye Wang,
                     representing LightSpeed Studios. ‚ÄúWe look forward to working with you.‚Äù
                     LightSpeed Studios is one of the world‚Äôs most innovative and successful game developers,
                     with teams around the world. Founded in 2008, LightSpeed Studios has created over
                     50 games across multiple platforms and genres for over 4 billion registered users.
                     Comprised of passionate players who advance the art and science of game development
                     through great stories, great gameplay and advanced technology, LightSpeed Studios
                     is focused on bringing next-generation experiences to gamers who want to enjoy them
                     anywhere, anytime across multiple genres and devices.
                     ‚ÄúIt has been amazing to see the rapid growth of the O3D ecosystem, and we‚Äôre elated
                     to welcome LightSpeed Studios to our community,‚Äù said Royal O‚ÄôBrien, Executive Director
                     of Open 3D Foundation and General Manager of Games and Digital Media at the Linux
                     Foundation. ‚ÄúLightSpeed Studios has achieved a strong reputation as a leading global
                     game developer, offering high-quality gaming experiences to hundreds of millions of
                     users worldwide, and we are excited to collaborate with them as we enhance O3DE‚Äôs
                     capabilities for global 3D developers.‚Äù
                     A Growing Community
                     LightSpeed Studios is one of 25 member companies since the public announcement of
                     the Open 3D Foundation in July 2021. Other premier members include Adobe, AWS, Huawei,
                     Intel, Microsoft and Niantic.
                     In May, O3DE announced its latest release, focused on performance, stability and usability
                     enhancements. With over 1,460 code merges, this new release offers several improvements
                     aimed to make it easier to build 3D simulations for AAA games and a range of other
                     applications. Significant enhancements include core stability, installer validation,
                     motion matching, user-defined property (UDP) support for the asset pipeline, and automated
                     testing advancements. The O3D Engine community is very active, averaging up to 2 million
                     line changes and 350-450 commits monthly from 60-100 authors across 41 repos.
                     Where to See the O3D Engine Next
                     On October 17-19, the Open 3D Foundation will host O3Dcon, its flagship conference,
                     bringing together technology leaders, indie and independent 3D developers, and the
                     academic community to share ideas, discuss hot topics and foster the future of 3D
                     development across a variety of industries and disciplines. For those interested in
                     sponsoring this event, please contact sponsorships@linuxfoundation.org.&nbsp;
                     Anyone interested in the O3D Engine is invited to get involved and connect with the
                     community on Discord.com/invite/o3de and GitHub.com/o3de.&nbsp;
                     About the Open 3D Engine (O3DE) project
                     O3D Engine is the flagship project managed by the Open 3D (O3D) Foundation. The open-source
                     project is a modular, cross-platform 3D engine built to power anything from AAA games
                     to cinema-quality 3D worlds to high-fidelity simulations. The code is hosted on GitHub
                     under the Apache 2.0 license. To learn more, please visit o3de.org.
                     About the Open 3D Foundation
                     Established in July 2021, the mission of the Open 3D Foundation (O3DF) is to make
                     an open-source, fully-featured, high-fidelity, real-time 3D engine for building games
                     and simulations, available to every industry. The Open 3D Foundation is home to the
                     O3D Engine project. To learn more, please visit o3d.foundation.
                     About the Linux Foundation
                     Founded in 2000, the Linux Foundation is supported by more than 1,000 members and
                     is the world‚Äôs leading home for collaboration on open source software, open standards,
                     open data, and open hardware. Linux Foundation‚Äôs projects are critical to the world‚Äôs
                     infrastructure including Linux, Kubernetes, Node.js, and more. The Linux Foundation‚Äôs
                     methodology focuses on leveraging best practices and addressing the needs of contributors,
                     users and solution providers to create sustainable models for open collaboration.
                     For more information, please visit us at linuxfoundation.org.
                     Media Inquiries:
                     pr@o3d.foundation
                     # # #
                     The Linux Foundation has registered trademarks and uses trademarks. For a list of
                     trademarks of The Linux Foundation, please see our trademark usage page: https://www.linuxfoundation.org/trademark-usage.
                     Linux is a registered trademark of Linus Torvalds.
                     The post LightSpeed Studios Joins the Open 3D Foundation as a Premier Member to Further
                     the Vast Potential of the 3D Ecosystem appeared first on Linux Foundation.The post
                     LightSpeed Studios Joins the Open 3D Foundation as a Premier Member to Further the
                     Vast Potential of the 3D Ecosystem appeared first on Linux.com.
                     </span></summary><time datetime="2022-06-16T01:49:24+02:00">Wed, 15 Jun 2022 23:49</time><article><img width="150" height="150" src="https://www.linux.com/wp-content/uploads/2022/06/o3delightspeed-aAqbtn-150x150.jpeg" class="attachment-thumbnail size-thumbnail wp-post-image" alt="" loading="lazy" srcset="https://www.linux.com/wp-content/uploads/2022/06/o3delightspeed-aAqbtn-150x150.jpeg 150w, https://www.linux.com/wp-content/uploads/2022/06/o3delightspeed-aAqbtn-356x357.jpeg 356w, https://www.linux.com/wp-content/uploads/2022/06/o3delightspeed-aAqbtn-24x24.jpeg 24w, https://www.linux.com/wp-content/uploads/2022/06/o3delightspeed-aAqbtn-48x48.jpeg 48w, https://www.linux.com/wp-content/uploads/2022/06/o3delightspeed-aAqbtn-96x96.jpeg 96w, https://www.linux.com/wp-content/uploads/2022/06/o3delightspeed-aAqbtn-300x300.jpeg 300w" sizes="(max-width: 150px) 100vw, 150px" /><div></div>
<p><strong>O3D community building a first-class, open-source 3D engine to advance development across gaming, the metaverse, and a variety of other applications</strong></p>
<p><strong>SAN FRANCISCO ‚Äì June 15, 2022</strong> ‚Äì The<a href="https://o3d.foundation/"> Open 3D Foundation (O3DF)</a>, the home of a vibrant community focused on advancing the future of open 3D development, announces its growing ecosystem with the addition of <a href="https://www.lqstudios.com/">LightSpeed Studios</a> as a Premier member alongside Adobe, AWS, Huawei, Intel, Microsoft and Niantic.</p>
<p>Today‚Äôs top-quality 3D engines are as complex as operating systems, requiring significant time, cost, and human capital investments to keep pace with advancements. Open source has repeatedly proven to be the path to quickest innovation. The<a href="https://www.o3de.org/"> Open 3D Engine (O3DE)</a> offers a high-fidelity, fully-featured, open source alternative poised to revolutionize real-time 3D development across a variety of industries‚Äîfrom game development, the metaverse, AI and digital twin, to automotive, healthcare, robotics and more.</p>
<p>As a Premier member, LightSpeed Studios will bring its leadership and wealth of experience in global research and development of high-quality games to help drive the development of O3DE‚Äôs specifications and initiatives. Tencent Senior Project Manager, Lanye Wang, will join the Open 3D Foundation‚Äôs Governing Board, helping shape the Foundation‚Äôs strategic direction and its stewardship of 3D visualization and simulation projects.¬†</p>
<p>‚ÄúWe are very excited to join the Open 3D Foundation, especially for the opportunity to leverage the connection with all of the other members to dive deep into the graphic technologies and build a top-level open source 3D engine community,‚Äù said Lanye Wang, representing LightSpeed Studios. ‚ÄúWe look forward to working with you.‚Äù</p>
<p>LightSpeed Studios is one of the world‚Äôs most innovative and successful game developers, with teams around the world. Founded in 2008, LightSpeed Studios has created over 50 games across multiple platforms and genres for over 4 billion registered users. Comprised of passionate players who advance the art and science of game development through great stories, great gameplay and advanced technology, LightSpeed Studios is focused on bringing next-generation experiences to gamers who want to enjoy them anywhere, anytime across multiple genres and devices.</p>
<p>‚ÄúIt has been amazing to see the rapid growth of the O3D ecosystem, and we‚Äôre elated to welcome LightSpeed Studios to our community,‚Äù said Royal O‚ÄôBrien, Executive Director of Open 3D Foundation and General Manager of Games and Digital Media at the Linux Foundation. ‚ÄúLightSpeed Studios has achieved a strong reputation as a leading global game developer, offering high-quality gaming experiences to hundreds of millions of users worldwide, and we are excited to collaborate with them as we enhance O3DE‚Äôs capabilities for global 3D developers.‚Äù</p>
<p><strong>A Growing Community</strong></p>
<p>LightSpeed Studios is one of 25 member companies since the public announcement of the Open 3D Foundation in July 2021. Other premier members include Adobe, AWS, Huawei, Intel, Microsoft and Niantic.</p>
<p>In May, O3DE announced its <a href="https://t.co/CuVOjb9Agx">latest release</a>, focused on performance, stability and usability enhancements. With over 1,460 code merges, this new release offers several improvements aimed to make it easier to build 3D simulations for AAA games and a range of other applications. Significant enhancements include core stability, installer validation, motion matching, user-defined property (UDP) support for the asset pipeline, and automated testing advancements. The O3D Engine community is very active, averaging up to 2 million line changes and 350-450 commits monthly from 60-100 authors across 41 repos.</p>
<p><strong>Where to See the O3D Engine Next</strong></p>
<p>On October 17-19, the Open 3D Foundation will host <a href="https://bit.ly/O3DCon">O3Dcon</a>, its flagship conference, bringing together technology leaders, indie and independent 3D developers, and the academic community to share ideas, discuss hot topics and foster the future of 3D development across a variety of industries and disciplines. For those interested in sponsoring this event, please contact <a href="mailto:sponsorships@linuxfoundation.org">sponsorships@linuxfoundation.org</a>.¬†</p>
<p>Anyone interested in the O3D Engine is invited to get involved and connect with the community on<a href="https://discord.com/invite/o3de"> Discord.com/invite/o3de</a> and<a href="https://github.com/o3de"> GitHub.com/o3de</a>.¬†</p>
<p><strong>About the Open 3D Engine (O3DE) project</strong></p>
<p>O3D Engine is the flagship project managed by the Open 3D (O3D) Foundation. The open-source project is a modular, cross-platform 3D engine built to power anything from AAA games to cinema-quality 3D worlds to high-fidelity simulations. The code is hosted on GitHub under the Apache 2.0 license. To learn more, please visit <a href="https://www.o3de.org/">o3de.org</a>.</p>
<p><strong>About the Open 3D Foundation</strong></p>
<p>Established in July 2021, the mission of the Open 3D Foundation (O3DF) is to make an open-source, fully-featured, high-fidelity, real-time 3D engine for building games and simulations, available to every industry. The Open 3D Foundation is home to the O3D Engine project. To learn more, please visit <a href="https://o3d.foundation/">o3d.foundation</a>.</p>
<p><strong>About the Linux Foundation</strong></p>
<p>Founded in 2000, the Linux Foundation is supported by more than 1,000 members and is the world‚Äôs leading home for collaboration on open source software, open standards, open data, and open hardware. Linux Foundation‚Äôs projects are critical to the world‚Äôs infrastructure including Linux, Kubernetes, Node.js, and more. The Linux Foundation‚Äôs methodology focuses on leveraging best practices and addressing the needs of contributors, users and solution providers to create sustainable models for open collaboration. For more information, please visit us at<a href="https://www.linuxfoundation.org/"> linuxfoundation.org</a>.</p>
<p><strong>Media Inquiries:</strong></p>
<p><a href="http://pr@o3d.foundation/">pr@o3d.foundation</a></p>
<p># # #</p>
<p><em>The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our trademark usage page:</em><a href="https://www.linuxfoundation.org/trademark-usage/"><em> https://www.linuxfoundation.org/trademark-usage</em></a><em>. Linux is a registered trademark of Linus Torvalds.</em></p>
<p>The post <a href="https://www.linuxfoundation.org/press-release/lightspeed-studios-joins-the-open-3d-foundation-as-a-premier-member-to-further-the-vast-potential-of-the-3d-ecosystem/">LightSpeed Studios Joins the Open 3D Foundation as a Premier Member to Further the Vast Potential of the 3D Ecosystem</a> appeared first on <a href="https://www.linuxfoundation.org/">Linux Foundation</a>.</p><p>The post <a rel="nofollow" href="https://www.linux.com/news/lightspeed-studios-joins-the-open-3d-foundation-as-a-premier-member-to-further-the-vast-potential-of-the-3d-ecosystem/">LightSpeed Studios Joins the Open 3D Foundation as a Premier Member to Further the Vast Potential of the 3D Ecosystem</a> appeared first on <a rel="nofollow" href="https://www.linux.com">Linux.com</a>.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://www.linux.com/news/tune-it-up-improving-redis-performance-for-ampere-a1-on-oracle-linux-in-oci/">Tune it Up: Improving Redis Performance for Ampere A1 on Oracle Linux in OCI</a> <span>The outcome with recommendations of an i
                     Click to Read More at Oracle Linux Kernel DevelopmentThe post Tune it Up: Improving
                     Redis Performance for Ampere A1 on Oracle Linux in OCI appeared first on Linux.com.
                     </span></summary><time datetime="2022-06-16T00:00:00+02:00">Wed, 15 Jun 2022 22:00</time><article><img width="150" height="112" src="https://www.linux.com/wp-content/uploads/2022/06/OCPC-BusinessSolution-ERP-815666838-Vh31yW.jpeg" class="attachment-thumbnail size-thumbnail wp-post-image" alt="" loading="lazy" srcset="https://www.linux.com/wp-content/uploads/2022/06/OCPC-BusinessSolution-ERP-815666838-Vh31yW.jpeg 150w, https://www.linux.com/wp-content/uploads/2022/06/OCPC-BusinessSolution-ERP-815666838-Vh31yW-80x60.jpeg 80w" sizes="(max-width: 150px) 100vw, 150px" /><p>The outcome with recommendations of an i</p>
<p>Click to <a href="https://blogs.oracle.com/linux/post/redis-a1-oracle-linux-oci" target="_blank" class="feedzy-rss-link-icon" rel="noopener">Read More</a> at Oracle Linux Kernel Development</p><p>The post <a rel="nofollow" href="https://www.linux.com/news/tune-it-up-improving-redis-performance-for-ampere-a1-on-oracle-linux-in-oci/">Tune it Up: Improving Redis Performance for Ampere A1 on Oracle Linux in OCI</a> appeared first on <a rel="nofollow" href="https://www.linux.com">Linux.com</a>.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://www.linux.com/news/how-to-configure-network-file-system-on-linux/">How to configure Network File System on Linux</a> <span>NFS is one of the easiest and most transparent ways to handle shared storage within
                     an organization. Learn how to configure it on Red Hat Enterprise Linux.
                     Read More at Enable SysadminThe post How to configure Network File System on Linux
                     appeared first on Linux.com.
                     </span></summary><time datetime="2022-06-13T05:06:22+02:00">Mon, 13 Jun 2022 03:06</time><article><p>NFS is one of the easiest and most transparent ways to handle shared storage within an organization. Learn how to configure it on Red Hat Enterprise Linux.</p>
<p><a href="https://www.redhat.com/sysadmin/configure-nfs-linux" target="_blank" class="feedzy-rss-link-icon" rel="noopener">Read More</a> at Enable Sysadmin</p><p>The post <a rel="nofollow" href="https://www.linux.com/news/how-to-configure-network-file-system-on-linux/">How to configure Network File System on Linux</a> appeared first on <a rel="nofollow" href="https://www.linux.com">Linux.com</a>.</p>
</article>
            </details>
            <details>
               <summary><a rel="noopener noreferrer" target="_blank" href="https://www.linux.com/news/lfx-mentorship-for-me/">LFX Mentorship for Me</a> <span>
                     
                     
                     
                     A brief about my experience with the Linux Foundation Mentorship.
                     
                     
                     The post originally appeared on deprov477‚Äôs blog. The author, Anubhav Choudhary, particpated
                     in the Linux Foundation‚Äôs Mentorship Program in 2022. The program is designed to help
                     developers ‚Äî many of whom are first-time open source contributors ‚Äî with necessary
                     skills and resources to learn, experiment, and contribute effectively to open source
                     communities. By participating in a mentorship program, mentees have the opportunity
                     to learn from experienced open source contributors as a segue to get internship and
                     job opportunities upon graduation. If you are interested, we invite you to learn more
                     and apply today here.
                     
                     
                     
                     
                     
                     
                     
                     
                     
                     
                     
                     
                     Hi everyone, I recently completed my LFX Mentorship project. I was a mentee for the
                     LFXM summer term of 2022 at Pixie, a CNCF sandbox project donated by The New Relic.
                     
                     
                     In this blog, I will be sharing my experience of mentorship. (TLDR; just awesome,
                     one-of-a-kind experience drop me a message. I‚Äôd be more than happy to help.
                     What is LFX Mentorship?
                     Let‚Äôs start this by knowing about The Linux Foundation. The Linux Foundation (LF)
                     is a non-profit organization, that standardizes the development of the Linux kernel
                     and also promotes open source projects such as Kubernetes, GraphQL, Hyperledger, RISC-V,
                     Xen project, etc.
                     The Linux Foundation Mentorship is a program run by LF, which helps developers with
                     the necessary skills and resources to learn and contribute to open source projects,
                     through 3 or 6 months of internship. During this period, the mentee is guided through
                     the development workflow and methodologies used by open source organizations, through
                     a project.
                     Selection procedure
                     I‚Äôve been involved in open source for some time and have been applying for the mentorship,
                     but got rejected every time.
                     This time also I was going through the projects and found a particularly interesting
                     project. It was about parsing a protocol. This took my eye as at that time I was learning
                     networking and experimenting a lot with communications. So naturally, I got interested.
                     After reading the project details, I went to the project‚Äôs slack channel to find a
                     mentor. Omid, one of Pixie‚Äôs founding engineers, was kind enough to reply to my message
                     and asked for a quick call.
                     I talked to him and told him about my interest and how I made a preliminary Mongo
                     wire protocol parser using Node.js as preparation. He seemed satisfied with this and
                     told me about further steps and time commitment.
                     Other formalities included submitting a cover letter, and my resume.
                     A few days later got this:
                     
                     Finally, after applying so many times, got selected !!!
                     Month 1
                     Started, and was introduced to my mentor Yaxiong Zhao, another founding engineer at
                     Pixie. He told me about what we were going to do in the next 3 months. He demoed me
                     the Pixie UI and explained to me the working of it, and how pixie catches packets
                     (hint:&nbsp;eBPF). And then sent me the AMQP spec sheet, and how it needs to be implemented
                     using C++.
                     Yes, the protocol changed from Mongo to AMQP, and the language from Node.js to C++.
                     But I guess a very important survival quality of industry is being flexible.
                     So, in the first month, I got a theoretical knowledge about AMQP wire spec and experimented
                     with it by deploying a local RabbitMQ server, and monitoring packets using Wireshark.
                     My mentor also tried helping me build Pixie on my local machine, but we failed, even
                     after switching distros. At last, we were able to set up my dev environment inside
                     a container.
                     ‚Ä¶quite a month
                     Month 2
                     In the first half of this month, I continued my research on AMQP (apparently implementing
                     a protocol required a lot of extensive reading) and found analogies of it with protocols
                     I was already familiar with, and kept on manually experimenting with packet translation.
                     3rd week of the month, It was finally time for me to start writing some code. Okay,
                     so this was the difficult part. Having very limited knowledge of C++, continued forward.
                     But my mentor was being an angel at this point, very patiently explaining to me, and
                     pointing me in the right direction, making me understand every lex required. I started
                     with implementing a data structure for storing and creating relations between packets.
                     After some effort, finally got my PR merged.
                     
                     
                     
                     Month 3
                     Continuing my code work, I started building a parser code. Yaxiong was very patient
                     and helpful during this time, sending me blogs, and guides and explaining to me every
                     little doubt I had. Thanks to him I was able to finally submit my preliminary code
                     for parsing the code.
                     And a final thing for this was to write tests. Learned google‚Äôs C++ testing library.
                     Wrote code, pushed.
                     Concluding the program
                     Like every good thing, this also came to an end. 12 weeks just fly by ‚Äî faster than
                     you can think ‚Äî The program opened up a new world of open source and got me introduced
                     to a lot of professional tools and etiquette. I appreciate the time and efforts my
                     mentor put into this program.
                     Completing this internship was a dream come true, dodging tonnes of problems: internet,
                     college, placement preparation, exams, everything. At many points in the internship,
                     I was very certain I won‚Äôt be able to complete the project. but:
                     At some point, everything‚Äôs gonna go south on you‚Ä¶ everything‚Äôs going to go south
                     and you‚Äôre going to say, this is it. This is how I end. Now you can either accept
                     that, or you can get to work. That‚Äôs all it is. You just begin. You do the math. You
                     solve one problem‚Ä¶ and you solve the next one‚Ä¶ and then the next. And If you solve
                     enough problems, you get to come home.
                     ‚Äî Tail ender, The Martian.
                     
                     
                     
                     The post LFX Mentorship for Me appeared first on Linux Foundation.The post LFX Mentorship
                     for Me appeared first on Linux.com.
                     </span></summary><time datetime="2022-06-10T05:49:44+02:00">Fri, 10 Jun 2022 03:49</time><article><img width="150" height="150" src="https://www.linux.com/wp-content/uploads/2022/06/LF-CNCF-Pixie-1-lny2f0-150x150.png" class="attachment-thumbnail size-thumbnail wp-post-image" alt="" loading="lazy" srcset="https://www.linux.com/wp-content/uploads/2022/06/LF-CNCF-Pixie-1-lny2f0-150x150.png 150w, https://www.linux.com/wp-content/uploads/2022/06/LF-CNCF-Pixie-1-lny2f0-24x24.png 24w, https://www.linux.com/wp-content/uploads/2022/06/LF-CNCF-Pixie-1-lny2f0-48x48.png 48w, https://www.linux.com/wp-content/uploads/2022/06/LF-CNCF-Pixie-1-lny2f0-96x96.png 96w, https://www.linux.com/wp-content/uploads/2022/06/LF-CNCF-Pixie-1-lny2f0-300x300.png 300w" sizes="(max-width: 150px) 100vw, 150px" /><div></div>
<div class="flex_column av-7wher-6058b29fb45335eb6317b74e83bda768 av_one_full  avia-builder-el-0  avia-builder-el-no-sibling  first flex_column_div ">
<div class="avia_textblock  ">
<div class="css-136cvf0">
<h2 class="css-1hesiyt">A brief about my experience with the Linux Foundation Mentorship.</h2>
</div>
<div class="css-1kjyr3x">
<div class="css-10qrllb"><em>The post originally appeared on <a href="https://blogs.deprov447.me/lfx-mentorship-for-me" target="_blank" rel="noopener">deprov477‚Äôs blog</a>. The author, Anubhav Choudhary, particpated in the <a href="https://www.linuxfoundation.org/diversity-inclusion/">Linux Foundation‚Äôs Mentorship Program</a> in 2022. The program is designed to help developers ‚Äî many of whom are first-time open source contributors ‚Äî with necessary skills and resources to learn, experiment, and contribute effectively to open source communities. By participating in a mentorship program, mentees have the opportunity to learn from experienced open source contributors as a segue to get internship and job opportunities upon graduation. If you are interested, we invite you to learn more and apply today <a href="https://www.linuxfoundation.org/diversity-inclusion/">here</a>.</em></div>
</div>
</div>
<p></p>
<div class="hr av-l47hsz5y-df72d38c91135c58cb644cae759dd60f hr-short  avia-builder-el-2  el_after_av_textblock  el_before_av_textblock  hr-center  av-small-hide av-mini-hide"><span class="hr-inner "><span class="hr-inner-style"></span></span></div>
<p></p>
<div class="avia_textblock  ">
<div class="css-136cvf0"></div>
<div class="css-1kjyr3x">
<div class="css-10qrllb"></div>
<div></div>
<div></div>
<div></div>
<div class="css-10qrllb">Hi everyone, I recently completed my LFX Mentorship project. I was a mentee for the LFXM summer term of 2022 at <a href="https://www.cncf.io/projects/pixie/">Pixie</a>, a CNCF sandbox project donated by The New Relic.</div>
</div>
<div>
<p>In this blog, I will be sharing my experience of mentorship. (TLDR; just awesome, one-of-a-kind experience drop me a message. I‚Äôd be more than happy to help.</p>
<h3>What is LFX Mentorship?</h3>
<p>Let‚Äôs start this by knowing about The Linux Foundation. The Linux Foundation (LF) is a non-profit organization, that standardizes the development of the Linux kernel and also promotes open source projects such as <a href="https://kubernetes.io/">Kubernetes</a>, <a href="https://graphql.org/">GraphQL</a>, <a href="https://www.hyperledger.org/">Hyperledger</a>, <a href="https://riscv.org/">RISC-V</a>, <a href="https://xenproject.org/">Xen project,</a> etc.</p>
<p>The Linux Foundation Mentorship is a program run by LF, which helps developers with the necessary skills and resources to learn and contribute to open source projects, through 3 or 6 months of internship. During this period, the mentee is guided through the development workflow and methodologies used by open source organizations, through a project.</p>
<h3>Selection procedure</h3>
<p>I‚Äôve been involved in open source for some time and have been applying for the mentorship, but got rejected every time.</p>
<p>This time also I was going through the projects and found a particularly interesting project. It was about parsing a protocol. This took my eye as at that time I was learning networking and experimenting a lot with communications. So naturally, I got interested. After reading the project details, I went to the project‚Äôs slack channel to find a mentor. Omid, one of Pixie‚Äôs founding engineers, was kind enough to reply to my message and asked for a quick call.</p>
<p>I talked to him and told him about my interest and how I made a preliminary Mongo wire protocol parser using Node.js as preparation. He seemed satisfied with this and told me about further steps and time commitment.</p>
<p>Other formalities included submitting a cover letter, and my resume.</p>
<p>A few days later got this:</p>
<p><a href="https://linuxfoundation.org/wp-content/uploads/LFX-mentorship-program-blog-image-1.png"></a></p>
<p>Finally, after applying so many times, got selected !!!</p>
<h3>Month 1</h3>
<p>Started, and was introduced to my mentor Yaxiong Zhao, another founding engineer at Pixie. He told me about what we were going to do in the next 3 months. He demoed me the Pixie UI and explained to me the working of it, and how pixie catches packets (hint:¬†<a href="https://docs.px.dev/about-pixie/pixie-ebpf/#title" target="_blank" rel="noopener">eBPF</a>). And then sent me the AMQP spec sheet, and how it needs to be implemented using C++.</p>
<p>Yes, the protocol changed from Mongo to AMQP, and the language from Node.js to C++. But I guess a very important survival quality of industry is being flexible.</p>
<p>So, in the first month, I got a theoretical knowledge about AMQP wire spec and experimented with it by deploying a local RabbitMQ server, and monitoring packets using Wireshark. My mentor also tried helping me build Pixie on my local machine, but we failed, even after switching distros. At last, we were able to set up my dev environment inside a container.</p>
<p>‚Ä¶quite a month</p>
<h3>Month 2</h3>
<p>In the first half of this month, I continued my research on AMQP (apparently implementing a protocol required a lot of extensive reading) and found analogies of it with protocols I was already familiar with, and kept on manually experimenting with packet translation.</p>
<p>3rd week of the month, It was finally time for me to start writing some code. Okay, so this was the difficult part. Having very limited knowledge of C++, continued forward. But my mentor was being an angel at this point, very patiently explaining to me, and pointing me in the right direction, making me understand every lex required. I started with implementing a data structure for storing and creating relations between packets. After some effort, finally got my PR merged.</p>
</div>
<div>
<p><a href="https://linuxfoundation.org/wp-content/uploads/LFX-mentorship-program-blog-image-2.png"></a></p>
<h3>Month 3</h3>
<p>Continuing my code work, I started building a parser code. Yaxiong was very patient and helpful during this time, sending me blogs, and guides and explaining to me every little doubt I had. Thanks to him I was able to finally submit my preliminary code for parsing the code.</p>
<p>And a final thing for this was to write tests. Learned google‚Äôs C++ testing library. Wrote code, pushed.</p>
<h3>Concluding the program</h3>
<p>Like every good thing, this also came to an end. 12 weeks just fly by ‚Äî faster than you can think ‚Äî The program opened up a new world of open source and got me introduced to a lot of professional tools and etiquette. I appreciate the time and efforts my mentor put into this program.</p>
<p>Completing this internship was a dream come true, dodging tonnes of problems: internet, college, placement preparation, exams, everything. At many points in the internship, I was very certain I won‚Äôt be able to complete the project. but:</p>
<p>At some point, everything‚Äôs gonna go south on you‚Ä¶ everything‚Äôs going to go south and you‚Äôre going to say, this is it. This is how I end. Now you can either accept that, or you can get to work. That‚Äôs all it is. You just begin. You do the math. You solve one problem‚Ä¶ and you solve the next one‚Ä¶ and then the next. And If you solve enough problems, you get to come home.</p>
<p>‚Äî Tail ender, The Martian.</p>
</div>
</div>
</div>
<p>The post <a href="https://www.linuxfoundation.org/blog/lfx-mentorship-for-me/">LFX Mentorship for Me</a> appeared first on <a href="https://www.linuxfoundation.org/">Linux Foundation</a>.</p><p>The post <a rel="nofollow" href="https://www.linux.com/news/lfx-mentorship-for-me/">LFX Mentorship for Me</a> appeared first on <a rel="nofollow" href="https://www.linux.com">Linux.com</a>.</p>
</article>
            </details>
            <footer>&nbsp;<q>Linux.com&nbsp;</q></footer>
         </section>
         </main>
      <footer id="x"><a href="#about">about</a> | <a href="#config">config</a> | <a href="#help">help</a></footer>
      <aside id="about">
         <div class="popup">
            <h2>101010 - my news site</h2><a class="close" href="#x">√ó</a><center class="content">
               <p>a simple news aggregator website. inspired by old sites like <a rel="noopener noreferrer" target="_blank" href="https://web.archive.org/web/*/mynewssite.org">mynewssite.org</a>.</p>
               <p><img src="./assets/image/blue-101010.png" alt="blue 101010"></p>
               <p>proudly made <em>without</em> docker, javascript, php, python, mysql or postgresql.</p>
            </center>
         </div>
      </aside>
      <aside id="config">
         <div class="popup">
            <h2>config.xml</h2><a class="close" href="#x">√ó</a><pre class="content">&lt;config max="10" icon="üõ∏" color="flat.css" theme="slash.css" beta="true"&gt;
  &lt;feed max="1"&gt;https://xkcd.com/rss.xml&lt;/feed&gt;
  &lt;feed&gt;http://rss.slashdot.org/Slashdot/slashdotMain&lt;/feed&gt;
  &lt;feed&gt;https://github.blog/feed/&lt;/feed&gt;
  &lt;feed&gt;https://netflixtechblog.com/feed&lt;/feed&gt;
  &lt;feed&gt;https://www.linux.com/feed/&lt;/feed&gt;
&lt;/config&gt;</pre>
            <form class="content"><script>function swap(name, sheet) { document.getElementById(name).setAttribute("href", "./assets/" + name + "/" + sheet) }</script><br><hr><br><label for="color">color = </label><select name="color" onchange="swap('color', this.value)">
                  <option>blue.css</option>
                  <option selected="">flat.css</option>
                  <option>green.css</option>
                  <option>solarized.css</option></select>
               				&nbsp;&nbsp;&nbsp; <label for="theme">theme = </label><select name="theme" onchange="swap('theme', this.value)">
                  <option>101010.css</option>
                  <option>planet.css</option>
                  <option>reader.css</option>
                  <option selected="">slash.css</option></select></form>
         </div>
      </aside>
      <aside id="help">
         <div class="popup">
            <h2>Check the <a rel="noopener noreferrer" target="_blank" href="https://github.com/mosterme/101010-my-news-site/wiki">Wiki</a> for information about ...</h2><a class="close" href="#x">√ó</a><ul class="content">
               <li><a rel="noopener noreferrer" target="_blank" href="https://github.com/mosterme/101010-my-news-site/wiki/Requirements">Requirements</a></li>
               <li><a rel="noopener noreferrer" target="_blank" href="https://github.com/mosterme/101010-my-news-site/wiki/Quickstart">Quickstart</a></li>
               <li><a rel="noopener noreferrer" target="_blank" href="https://github.com/mosterme/101010-my-news-site/wiki/Configuration">Configuration</a></li>
               <li><a rel="noopener noreferrer" target="_blank" href="https://github.com/mosterme/101010-my-news-site/wiki/Which-sites-are-supported%3F">Supported sites</a></li>
            </ul>
         </div>
      </aside>
   </body>
</html>